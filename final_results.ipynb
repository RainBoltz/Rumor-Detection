{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 謠言分類實作報告 Rumor Detection on Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料簡介\n",
    "PHEME資料集記錄了Twitter使用者在重大事件發生期間所發布的謠言和非謠言tweet紀錄。\n",
    "\n",
    "#### 目錄結構如下：\n",
    "\n",
    "- 事件A\n",
    "    - 謠言\n",
    "        - twitter_id_1\n",
    "        - twitter_id_2\n",
    "        - twitter_id_3\n",
    "        - ...\n",
    "    - 非謠言\n",
    "        - twitter_id_1\n",
    "        - twitter_id_2\n",
    "        - twitter_id_3\n",
    "        - ...\n",
    "- 事件B\n",
    "- 事件C\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取資料\n",
    "利用 python 原生 JSON 讀取功能讀取所有資料。\n",
    "\n",
    "我們分為：\n",
    "- dataset_original: 只有原tweet的資料集\n",
    "- dataset_with_reactions: 包含reaction的tweet資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 420/420 [00:00<00:00, 720.61it/s]\n",
      "100%|██████████| 470/470 [00:00<00:00, 726.74it/s]\n",
      "100%|██████████| 1621/1621 [00:03<00:00, 495.11it/s]\n",
      "100%|██████████| 458/458 [00:00<00:00, 629.41it/s]\n",
      "100%|██████████| 699/699 [00:01<00:00, 418.78it/s]\n",
      "100%|██████████| 522/522 [00:00<00:00, 593.22it/s]\n",
      "100%|██████████| 231/231 [00:00<00:00, 815.95it/s] \n",
      "100%|██████████| 238/238 [00:00<00:00, 845.08it/s]\n",
      "100%|██████████| 859/859 [00:01<00:00, 481.64it/s]\n",
      "100%|██████████| 284/284 [00:00<00:00, 465.82it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm, trange\n",
    "from pprint import pprint\n",
    "\n",
    "PATH_ROOT = \"./200g/dataset/training\"\n",
    "\n",
    "def parse_structure(mapper):\n",
    "    if type(mapper) == list:\n",
    "        return []\n",
    "    mk_init = list(mapper.keys())\n",
    "    q = list(zip([\"root\" for _ in range(len(mk_init))], mk_init, [mapper[k] for k in mk_init]))\n",
    "    while(len(q) != 0):\n",
    "        prev_id = q[0][0]\n",
    "        this_id = q[0][1]\n",
    "        this_mapper = q[0][2]\n",
    "        yield (prev_id, this_id)\n",
    "        if type(this_mapper) != list:\n",
    "            for qk in this_mapper.keys():\n",
    "                q.append((this_id, qk, this_mapper[qk]))\n",
    "        q = q[1:]\n",
    "        \n",
    "dataset = {\"raw_X\":[], \"Y\":[]}\n",
    "for event_name in os.listdir(PATH_ROOT):\n",
    "    if \".ipynb\" in event_name:\n",
    "        continue\n",
    "    # non-rumors\n",
    "    dir_path = PATH_ROOT + '/' + event_name + '/non-rumours'\n",
    "    for twitter_id in tqdm(os.listdir(dir_path)):\n",
    "        if \".ipynb\" in twitter_id:\n",
    "            continue\n",
    "        with open(dir_path + '/' + twitter_id + '/source-tweets/' + twitter_id + '.json', 'r') as f:\n",
    "            this_json = json.load(f)\n",
    "            root_text = this_json['text'].replace(\"|||\",\"\")\n",
    "            this_user = this_json['user']['screen_name']\n",
    "        with open(dir_path + '/' + twitter_id + '/structure.json', 'r') as f:\n",
    "            structure_json = json.load(f)\n",
    "        dataset['raw_X'].append({'main':root_text, 'sub':[], 'user':this_user})\n",
    "        dataset['Y'].append(0)\n",
    "        text_pairs = list(parse_structure(structure_json[twitter_id]))\n",
    "        if len(text_pairs) > 0:\n",
    "            for text_pair in text_pairs:\n",
    "                prev_id, curr_id = text_pair[0], text_pair[1]\n",
    "                if prev_id == 'root':\n",
    "                    this_X = root_text + \" ||| \"\n",
    "                else:\n",
    "                    with open(dir_path + '/' + twitter_id + '/reactions/' + prev_id + \".json\", 'r') as f:\n",
    "                        this_json = json.load(f)\n",
    "                        this_X = this_json['text'].replace(\"|||\",\"\") + \" ||| \"\n",
    "                with open(dir_path + '/' + twitter_id + '/reactions/' + curr_id + \".json\", 'r') as f:\n",
    "                    this_json = json.load(f)\n",
    "                    this_X += this_json['text']\n",
    "                dataset['raw_X'][-1]['sub'].append(this_X)\n",
    "        \n",
    "    # rumors\n",
    "    dir_path = PATH_ROOT + '/' + event_name + '/rumours'\n",
    "    for twitter_id in tqdm(os.listdir(dir_path)):\n",
    "        if \".ipynb\" in twitter_id:\n",
    "            continue\n",
    "        with open(dir_path + '/' + twitter_id + '/source-tweets/' + twitter_id + '.json', 'r') as f:\n",
    "            this_json = json.load(f)\n",
    "            root_text = this_json['text'].replace(\"|||\",\"\")\n",
    "            this_user = this_json['user']['screen_name']\n",
    "        with open(dir_path + '/' + twitter_id + '/structure.json', 'r') as f:\n",
    "            structure_json = json.load(f)\n",
    "        dataset['raw_X'].append({'main':root_text, 'sub':[], 'user':this_user})\n",
    "        dataset['Y'].append(1)\n",
    "        text_pairs = list(parse_structure(structure_json[twitter_id]))\n",
    "        if len(text_pairs) > 0:\n",
    "            for text_pair in text_pairs:\n",
    "                prev_id, curr_id = text_pair[0], text_pair[1]\n",
    "                if prev_id == 'root':\n",
    "                    this_X = root_text + \" ||| \"\n",
    "                else:\n",
    "                    with open(dir_path + '/' + twitter_id + '/reactions/' + prev_id + \".json\", 'r') as f:\n",
    "                        this_json = json.load(f)\n",
    "                        this_X = this_json['text'].replace(\"|||\",\"\") + \" ||| \"\n",
    "                with open(dir_path + '/' + twitter_id + '/reactions/' + curr_id + \".json\", 'r') as f:\n",
    "                    this_json = json.load(f)\n",
    "                    this_X += this_json['text']\n",
    "                dataset['raw_X'][-1]['sub'].append(this_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料清理並進行統計\n",
    "\n",
    "- 將所有文字 uncase (變成小寫)\n",
    "- 保留usertag、url和hashtag (因為hashtag裡面有文字訊息)\n",
    "- 保留其他語言文字 (不移除特殊feature)\n",
    "- 對句長進行敘述統計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5802/5802 [00:00<00:00, 67085.55it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#大小寫轉換\n",
    "for i in trange(len(dataset['raw_X'])):\n",
    "    this_x = dataset['raw_X'][i]['main']\n",
    "    this_x = this_x.lower()\n",
    "    dataset['raw_X'][i]['main'] = this_x\n",
    "    for j in range(len(dataset['raw_X'][i]['sub'])):\n",
    "        this_x = dataset['raw_X'][i]['sub'][j]\n",
    "        this_x = this_x.lower()\n",
    "        dataset['raw_X'][i]['sub'][j] = this_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[original]\n",
      "count    5802.000000\n",
      "mean       16.205963\n",
      "std         4.572858\n",
      "min         3.000000\n",
      "25%        13.000000\n",
      "50%        16.000000\n",
      "75%        19.000000\n",
      "max        31.000000\n",
      "Name: X_Length, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"[original]\")\n",
    "df_org = pd.DataFrame({'X':[d['main'] for d in dataset['raw_X']], 'Y':dataset['Y']})\n",
    "df_org['X_Length'] = df_org['X'].apply(lambda x: len(x.split()))\n",
    "plt.hist(df_org['X_Length'])\n",
    "print(df_org['X_Length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[with reactions]\n",
      "count    99253.000000\n",
      "mean        28.144318\n",
      "std          9.253973\n",
      "min          2.000000\n",
      "25%         21.000000\n",
      "50%         28.000000\n",
      "75%         35.000000\n",
      "max         58.000000\n",
      "Name: X_Length, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"[with reactions]\")\n",
    "dx = []\n",
    "dy = []\n",
    "for t in range(len(dataset['raw_X'])):\n",
    "    dx.append(dataset['raw_X'][t]['main'])\n",
    "    dy.append(dataset['Y'][t])\n",
    "    for tt in dataset['raw_X'][t]['sub']:\n",
    "        dx.append(tt)\n",
    "        dy.append(dataset['Y'][t])\n",
    "df_react = pd.DataFrame({'X':dx, 'Y':dy})\n",
    "df_react['X_Length'] = df_react['X'].apply(lambda x: len(x.replace(\" ||| \",\"\").split()))\n",
    "plt.hist(df_react['X_Length'])\n",
    "print(df_react['X_Length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先切好 train-test-set 來減少後續進行作業。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain_raw, Xtest_raw, Ytrain_org, Ytest_org = train_test_split(dataset['raw_X'], dataset['Y'], test_size=0.2, random_state=0)\n",
    "\n",
    "Xtrain_org_raw = [d['main'] for d in Xtrain_raw]\n",
    "Xtest_org_raw = [d['main'] for d in Xtest_raw]\n",
    "\n",
    "Xtrain_react_raw = []\n",
    "Ytrain_react = []\n",
    "for i in range(len(Xtrain_raw)):\n",
    "    Xtrain_react_raw.append(Xtrain_raw[i]['main'])\n",
    "    Ytrain_react.append(Ytrain_org[i])\n",
    "    for xx in Xtrain_raw[i]['sub']:\n",
    "        Xtrain_react_raw.append(xx)\n",
    "        Ytrain_react.append(Ytrain_org[i])\n",
    "Xtest_react_raw = []\n",
    "Ytest_react = Ytest_org.copy()\n",
    "for i in range(len(Xtest_raw)):\n",
    "    Xtest_react_raw.append([Xtest_raw[i]['main']])\n",
    "    for xx in Xtest_raw[i]['sub']:\n",
    "        Xtest_react_raw[-1].append(xx)\n",
    "\n",
    "assert len(Xtrain_react_raw) == len(Ytrain_react)\n",
    "assert len(Xtest_react_raw) == len(Ytest_react)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 取得 BERT 的 embedding\n",
    "\n",
    "使用 bert-as-service 來取得 BERT-Embedding (max_seq_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start original...DONE!\n",
      "start with reactions...DONE!\n"
     ]
    }
   ],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()\n",
    "\n",
    "#使用 100 作為 BERT max sequence length\n",
    "print(\"start original...\",end='')\n",
    "Xtrain_org = bc.encode(Xtrain_org_raw)\n",
    "Xtest_org = bc.encode(Xtest_org_raw)\n",
    "print(\"DONE!\")\n",
    "\n",
    "print(\"start with reactions...\",end='')\n",
    "Xtrain_react = bc.encode(Xtrain_react_raw)\n",
    "Xtest_react = []\n",
    "for x in Xtest_react_raw:\n",
    "    Xtest_react.append(bc.encode(x))\n",
    "Xtest_react = np.array(Xtest_react)\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用 joblib 輸出為 pickle 檔，並清除不必要資料 (避免佔用記憶體空間)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start dumping dataset_original...DONE!\n",
      "start dumping dataset_with_reactions...DONE!\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import joblib\n",
    "\n",
    "print(\"start dumping dataset_original...\", end='')\n",
    "df_org.to_csv('./200g/dataset/dataset_original.csv',index=False)\n",
    "joblib.dump({'X_train':Xtrain_org, 'Y_train':Ytrain_org, 'X_test':Xtest_org, 'Y_test':Ytest_org},\n",
    "            './200g/dataset/dataset_original.pkl', protocol=4)\n",
    "print(\"DONE!\")\n",
    "del Xtrain_org_raw\n",
    "del Xtest_org_raw\n",
    "del df_org\n",
    "\n",
    "print(\"start dumping dataset_with_reactions...\", end='')\n",
    "df_react.to_csv('./200g/dataset/dataset_with_reactions.csv',index=False)\n",
    "joblib.dump({'X_train':Xtrain_react, 'Y_train':Ytrain_react, 'X_test':Xtest_react, 'Y_test':Ytest_react},\n",
    "            './200g/dataset/dataset_with_reactions.pkl', protocol=4)\n",
    "print(\"DONE!\")\n",
    "del Xtrain_react_raw\n",
    "del Xtest_react_raw\n",
    "del df_react\n",
    "\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 開始訓練模型\n",
    "\n",
    "我們比較了兩種不同的模型:\n",
    "- 傳統 Logistic Regression - 使用Cross Validation找出超參數\n",
    "- 深度學習 BiDirectional LSTM - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在那之前，我們先檢查資料分布，以決定是否調整 class weight (解決 unbalanced 問題)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class#0 有 3830 個, 占比66.01%\n",
      "class#1 有 1972 個, 占比33.99%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dataset['Y'])\n",
    "\n",
    "v,c = np.unique(dataset['Y'], return_counts=True)\n",
    "total_count = len(dataset['Y'])\n",
    "for vv,cc in zip(v,c):\n",
    "    print(\"class#%d 有 %4d 個, 占比%.2f%%\"%(vv,cc,cc*100/total_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...可見是一個非常需要調整 class weight 的二元分類問題。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前置作業"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score, make_scorer, roc_curve, auc\n",
    "\n",
    "# 取得運行的CPU數量\n",
    "NJOBS = max(multiprocessing.cpu_count()-2, 1)\n",
    "\n",
    "# 定義評估函式\n",
    "myscorer = make_scorer(\n",
    "         roc_auc_score,\n",
    "         greater_is_better=True\n",
    ")\n",
    "\n",
    "# 繪製ROC曲線\n",
    "def draw_ROC(labels, y, yps, title):\n",
    "    plt.title(title)\n",
    "    for i in range(len(labels)):\n",
    "        y_true = y\n",
    "        y_prop = yps[i]\n",
    "        label = labels[i]\n",
    "        fpr, tpr, threshold = roc_curve(y_true, y_prop)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label = '%s = %0.4f' % (label, roc_auc))\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "lr = LogisticRegressionCV(Cs=20, scoring=myscorer, random_state=0, max_iter=100000, class_weight='balanced', n_jobs=NJOBS).fit(Xtrain_org, Ytrain_org)\n",
    "joblib.dump(lr, './200g/trained_models/LogisticRegressionCV_org.pkl', protocol=4)\n",
    "del lr\n",
    "\n",
    "#大資料使用多執行緒時會出錯，故取消n_jobs調用\n",
    "lr = LogisticRegressionCV(Cs=20, scoring=myscorer, random_state=0, max_iter=100000, class_weight='balanced').fit(Xtrain_react, Ytrain_react)\n",
    "joblib.dump(lr, './200g/trained_models/LogisticRegressionCV_react.pkl', protocol=4)\n",
    "del lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[original]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor       0.91      0.82      0.86       755\n",
      "       rumor       0.72      0.85      0.78       406\n",
      "\n",
      "    accuracy                           0.83      1161\n",
      "   macro avg       0.82      0.84      0.82      1161\n",
      "weighted avg       0.85      0.83      0.84      1161\n",
      "\n",
      "\n",
      "\n",
      "[with reactions]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor       0.90      0.79      0.84       755\n",
      "       rumor       0.68      0.83      0.75       406\n",
      "\n",
      "    accuracy                           0.80      1161\n",
      "   macro avg       0.79      0.81      0.79      1161\n",
      "weighted avg       0.82      0.80      0.81      1161\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def with_reaction_lr_model_predict(model, X, thresh=0.5): #return predict, prob\n",
    "    this_y_prob = []\n",
    "    this_y = []\n",
    "    for i,x in enumerate(X):\n",
    "        _y = model.predict(x)\n",
    "        if sum(_y) > len(x)*thresh:\n",
    "            this_y.append(1)\n",
    "        else:\n",
    "            this_y.append(0)\n",
    "        _y = model.predict_proba(x)[:,1]\n",
    "        _y_prob = sum(_y)/len(x)\n",
    "        this_y_prob.append(_y_prob)\n",
    "    return this_y, this_y_prob\n",
    "\n",
    "lr1 = joblib.load('./200g/trained_models/LogisticRegressionCV_org.pkl')\n",
    "lr2 = joblib.load('./200g/trained_models/LogisticRegressionCV_react.pkl')\n",
    "\n",
    "lr1_y_prob = lr1.predict_proba(Xtest_org)[:,1]\n",
    "lr1_y = lr1.predict(Xtest_org)\n",
    "lr2_y, lr2_y_prob = with_reaction_lr_model_predict(lr2, Xtest_react)\n",
    "\n",
    "draw_ROC([\"Original\",\"with Reactions\"], Ytest_org, [lr1_y_prob,lr2_y_prob], 'ROC curves - Original v.s with Reactions')\n",
    "\n",
    "print(\"[original]\")\n",
    "print(classification_report(Ytest_org, lr1_y, target_names=['non-rumor','rumor']))\n",
    "print(\"\\n\\n[with reactions]\")\n",
    "print(classification_report(Ytest_react, lr2_y, target_names=['non-rumor','rumor']))\n",
    "\n",
    "del lr1\n",
    "del lr2\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Activation\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# 建立訓練模型時的auc-roc評估函式\n",
    "def auc_roc(y_true, y_pred):\n",
    "    value, update_op = tf.contrib.metrics.streaming_auc(y_pred, y_true)\n",
    "    metric_vars = [i for i in tf.local_variables() if 'auc_roc' in i.name.split('/')[1]]\n",
    "    for v in metric_vars:\n",
    "        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n",
    "    with tf.control_dependencies([update_op]):\n",
    "        value = tf.identity(value)\n",
    "        return value\n",
    "    \n",
    "# 建立訓練模型時的callbak函式集合\n",
    "def get_callbacks(output_path):\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', patience=14, verbose=1, mode='min')\n",
    "    mcp_save = ModelCheckpoint('./200g/trained_models/%s/model-{auc_roc:03f}.h5'%output_path, save_best_only=True, monitor='val_loss', mode='min')\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
    "    return [earlyStopping, mcp_save, reduce_lr_loss]\n",
    "\n",
    "# MLP模型建立\n",
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape = (768,))) \n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile('adam', 'binary_crossentropy', metrics=['accuracy', auc_roc])\n",
    "    model.summary()\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:998: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_116 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_118 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4408 samples, validate on 233 samples\n",
      "Epoch 1/100\n",
      "4408/4408 [==============================] - 2s 369us/step - loss: 0.5587 - accuracy: 0.7053 - auc_roc: 0.6200 - val_loss: 0.4517 - val_accuracy: 0.7768 - val_auc_roc: 0.7396\n",
      "Epoch 2/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.4556 - accuracy: 0.7797 - auc_roc: 0.7737 - val_loss: 0.4520 - val_accuracy: 0.7854 - val_auc_roc: 0.8002\n",
      "Epoch 3/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.4291 - accuracy: 0.7949 - auc_roc: 0.8141 - val_loss: 0.4320 - val_accuracy: 0.8197 - val_auc_roc: 0.8249\n",
      "Epoch 4/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.4186 - accuracy: 0.8020 - auc_roc: 0.8327 - val_loss: 0.4185 - val_accuracy: 0.8240 - val_auc_roc: 0.8384\n",
      "Epoch 5/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.3996 - accuracy: 0.8240 - auc_roc: 0.8435 - val_loss: 0.4178 - val_accuracy: 0.8326 - val_auc_roc: 0.8491\n",
      "Epoch 6/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.3896 - accuracy: 0.8174 - auc_roc: 0.8531 - val_loss: 0.4034 - val_accuracy: 0.8283 - val_auc_roc: 0.8566\n",
      "Epoch 7/100\n",
      "4408/4408 [==============================] - 0s 42us/step - loss: 0.3735 - accuracy: 0.8285 - auc_roc: 0.8610 - val_loss: 0.4115 - val_accuracy: 0.8155 - val_auc_roc: 0.8635\n",
      "Epoch 8/100\n",
      "4408/4408 [==============================] - 0s 42us/step - loss: 0.3706 - accuracy: 0.8337 - auc_roc: 0.8664 - val_loss: 0.4090 - val_accuracy: 0.8283 - val_auc_roc: 0.8688\n",
      "Epoch 9/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.3618 - accuracy: 0.8317 - auc_roc: 0.8713 - val_loss: 0.4005 - val_accuracy: 0.8369 - val_auc_roc: 0.8734\n",
      "Epoch 10/100\n",
      "4408/4408 [==============================] - 0s 42us/step - loss: 0.3527 - accuracy: 0.8423 - auc_roc: 0.8756 - val_loss: 0.4088 - val_accuracy: 0.8155 - val_auc_roc: 0.8775\n",
      "Epoch 11/100\n",
      "4408/4408 [==============================] - 0s 46us/step - loss: 0.3406 - accuracy: 0.8428 - auc_roc: 0.8797 - val_loss: 0.3966 - val_accuracy: 0.8197 - val_auc_roc: 0.8815\n",
      "Epoch 12/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.3399 - accuracy: 0.8460 - auc_roc: 0.8830 - val_loss: 0.3925 - val_accuracy: 0.8326 - val_auc_roc: 0.8848\n",
      "Epoch 13/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.3299 - accuracy: 0.8557 - auc_roc: 0.8864 - val_loss: 0.3903 - val_accuracy: 0.8326 - val_auc_roc: 0.8881\n",
      "Epoch 14/100\n",
      "4408/4408 [==============================] - 0s 46us/step - loss: 0.3180 - accuracy: 0.8575 - auc_roc: 0.8899 - val_loss: 0.4097 - val_accuracy: 0.8240 - val_auc_roc: 0.8914\n",
      "Epoch 15/100\n",
      "4408/4408 [==============================] - 0s 48us/step - loss: 0.3106 - accuracy: 0.8587 - auc_roc: 0.8930 - val_loss: 0.3985 - val_accuracy: 0.8240 - val_auc_roc: 0.8943\n",
      "Epoch 16/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.3045 - accuracy: 0.8666 - auc_roc: 0.8957 - val_loss: 0.3957 - val_accuracy: 0.8283 - val_auc_roc: 0.8972\n",
      "Epoch 17/100\n",
      "4408/4408 [==============================] - 0s 42us/step - loss: 0.2973 - accuracy: 0.8677 - auc_roc: 0.8984 - val_loss: 0.3872 - val_accuracy: 0.8455 - val_auc_roc: 0.8998\n",
      "Epoch 18/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.3075 - accuracy: 0.8603 - auc_roc: 0.9007 - val_loss: 0.4334 - val_accuracy: 0.8197 - val_auc_roc: 0.9018\n",
      "Epoch 19/100\n",
      "4408/4408 [==============================] - 0s 40us/step - loss: 0.2971 - accuracy: 0.8689 - auc_roc: 0.9028 - val_loss: 0.4107 - val_accuracy: 0.8240 - val_auc_roc: 0.9039\n",
      "Epoch 20/100\n",
      "4408/4408 [==============================] - 0s 42us/step - loss: 0.2901 - accuracy: 0.8671 - auc_roc: 0.9049 - val_loss: 0.4250 - val_accuracy: 0.8069 - val_auc_roc: 0.9058\n",
      "Epoch 21/100\n",
      "4408/4408 [==============================] - 0s 45us/step - loss: 0.2881 - accuracy: 0.8732 - auc_roc: 0.9068 - val_loss: 0.3896 - val_accuracy: 0.8326 - val_auc_roc: 0.9077\n",
      "Epoch 22/100\n",
      "4408/4408 [==============================] - 0s 48us/step - loss: 0.2983 - accuracy: 0.8652 - auc_roc: 0.9084 - val_loss: 0.3935 - val_accuracy: 0.8326 - val_auc_roc: 0.9091\n",
      "Epoch 23/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.2843 - accuracy: 0.8743 - auc_roc: 0.9099 - val_loss: 0.3960 - val_accuracy: 0.8412 - val_auc_roc: 0.9107\n",
      "Epoch 24/100\n",
      "4408/4408 [==============================] - 0s 46us/step - loss: 0.2674 - accuracy: 0.8777 - auc_roc: 0.9116 - val_loss: 0.4039 - val_accuracy: 0.8541 - val_auc_roc: 0.9125\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 25/100\n",
      "4408/4408 [==============================] - 0s 46us/step - loss: 0.2553 - accuracy: 0.8956 - auc_roc: 0.9133 - val_loss: 0.3928 - val_accuracy: 0.8498 - val_auc_roc: 0.9144\n",
      "Epoch 26/100\n",
      "4408/4408 [==============================] - 0s 42us/step - loss: 0.2415 - accuracy: 0.9011 - auc_roc: 0.9154 - val_loss: 0.3884 - val_accuracy: 0.8498 - val_auc_roc: 0.9164\n",
      "Epoch 27/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.2374 - accuracy: 0.9018 - auc_roc: 0.9173 - val_loss: 0.4007 - val_accuracy: 0.8455 - val_auc_roc: 0.9183\n",
      "Epoch 28/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.2337 - accuracy: 0.9043 - auc_roc: 0.9192 - val_loss: 0.4014 - val_accuracy: 0.8369 - val_auc_roc: 0.9201\n",
      "Epoch 29/100\n",
      "4408/4408 [==============================] - 0s 41us/step - loss: 0.2226 - accuracy: 0.9093 - auc_roc: 0.9210 - val_loss: 0.4024 - val_accuracy: 0.8369 - val_auc_roc: 0.9220\n",
      "Epoch 30/100\n",
      "4408/4408 [==============================] - 0s 40us/step - loss: 0.2221 - accuracy: 0.9106 - auc_roc: 0.9228 - val_loss: 0.4032 - val_accuracy: 0.8541 - val_auc_roc: 0.9238\n",
      "Epoch 31/100\n",
      "4408/4408 [==============================] - 0s 46us/step - loss: 0.2211 - accuracy: 0.9111 - auc_roc: 0.9245 - val_loss: 0.4063 - val_accuracy: 0.8455 - val_auc_roc: 0.9253\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 00031: early stopping\n",
      "\n",
      "Fold  2\n",
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_119 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_79 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_120 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_80 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4408 samples, validate on 233 samples\n",
      "Epoch 1/100\n",
      "4408/4408 [==============================] - 2s 374us/step - loss: 0.5454 - accuracy: 0.7228 - auc_roc: 0.6402 - val_loss: 0.3730 - val_accuracy: 0.8584 - val_auc_roc: 0.7573\n",
      "Epoch 2/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.4619 - accuracy: 0.7790 - auc_roc: 0.7859 - val_loss: 0.3431 - val_accuracy: 0.8541 - val_auc_roc: 0.8075\n",
      "Epoch 3/100\n",
      "4408/4408 [==============================] - 0s 46us/step - loss: 0.4407 - accuracy: 0.7931 - auc_roc: 0.8197 - val_loss: 0.3288 - val_accuracy: 0.8755 - val_auc_roc: 0.8279\n",
      "Epoch 4/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.4156 - accuracy: 0.8088 - auc_roc: 0.8356 - val_loss: 0.3182 - val_accuracy: 0.8712 - val_auc_roc: 0.8420\n",
      "Epoch 5/100\n",
      "4408/4408 [==============================] - 0s 46us/step - loss: 0.4020 - accuracy: 0.8065 - auc_roc: 0.8478 - val_loss: 0.3171 - val_accuracy: 0.8798 - val_auc_roc: 0.8517\n",
      "Epoch 6/100\n",
      "4408/4408 [==============================] - 0s 45us/step - loss: 0.3917 - accuracy: 0.8167 - auc_roc: 0.8558 - val_loss: 0.3075 - val_accuracy: 0.8670 - val_auc_roc: 0.8591\n",
      "Epoch 7/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.3869 - accuracy: 0.8221 - auc_roc: 0.8622 - val_loss: 0.3048 - val_accuracy: 0.8712 - val_auc_roc: 0.8648\n",
      "Epoch 8/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.3732 - accuracy: 0.8276 - auc_roc: 0.8676 - val_loss: 0.2918 - val_accuracy: 0.8841 - val_auc_roc: 0.8700\n",
      "Epoch 9/100\n",
      "4408/4408 [==============================] - 0s 45us/step - loss: 0.3676 - accuracy: 0.8362 - auc_roc: 0.8726 - val_loss: 0.2950 - val_accuracy: 0.8841 - val_auc_roc: 0.8746\n",
      "Epoch 10/100\n",
      "4408/4408 [==============================] - 0s 48us/step - loss: 0.3630 - accuracy: 0.8342 - auc_roc: 0.8767 - val_loss: 0.2978 - val_accuracy: 0.8927 - val_auc_roc: 0.8782\n",
      "Epoch 11/100\n",
      "4408/4408 [==============================] - 0s 41us/step - loss: 0.3575 - accuracy: 0.8378 - auc_roc: 0.8801 - val_loss: 0.2909 - val_accuracy: 0.8884 - val_auc_roc: 0.8816\n",
      "Epoch 12/100\n",
      "4408/4408 [==============================] - 0s 45us/step - loss: 0.3433 - accuracy: 0.8453 - auc_roc: 0.8834 - val_loss: 0.2773 - val_accuracy: 0.8927 - val_auc_roc: 0.8851\n",
      "Epoch 13/100\n",
      "4408/4408 [==============================] - 0s 42us/step - loss: 0.3310 - accuracy: 0.8523 - auc_roc: 0.8868 - val_loss: 0.2797 - val_accuracy: 0.8798 - val_auc_roc: 0.8885\n",
      "Epoch 14/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.3265 - accuracy: 0.8521 - auc_roc: 0.8902 - val_loss: 0.2810 - val_accuracy: 0.8927 - val_auc_roc: 0.8917\n",
      "Epoch 15/100\n",
      "4408/4408 [==============================] - 0s 42us/step - loss: 0.3277 - accuracy: 0.8555 - auc_roc: 0.8931 - val_loss: 0.2808 - val_accuracy: 0.8884 - val_auc_roc: 0.8942\n",
      "Epoch 16/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.3117 - accuracy: 0.8600 - auc_roc: 0.8958 - val_loss: 0.2689 - val_accuracy: 0.8884 - val_auc_roc: 0.8969\n",
      "Epoch 17/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.3041 - accuracy: 0.8625 - auc_roc: 0.8983 - val_loss: 0.2627 - val_accuracy: 0.8884 - val_auc_roc: 0.8997\n",
      "Epoch 18/100\n",
      "4408/4408 [==============================] - 0s 42us/step - loss: 0.3057 - accuracy: 0.8664 - auc_roc: 0.9010 - val_loss: 0.2753 - val_accuracy: 0.8970 - val_auc_roc: 0.9020\n",
      "Epoch 19/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.2977 - accuracy: 0.8671 - auc_roc: 0.9031 - val_loss: 0.2762 - val_accuracy: 0.8927 - val_auc_roc: 0.9042\n",
      "Epoch 20/100\n",
      "4408/4408 [==============================] - 0s 47us/step - loss: 0.2886 - accuracy: 0.8820 - auc_roc: 0.9055 - val_loss: 0.2928 - val_accuracy: 0.8670 - val_auc_roc: 0.9065\n",
      "Epoch 21/100\n",
      "4408/4408 [==============================] - 0s 46us/step - loss: 0.2836 - accuracy: 0.8777 - auc_roc: 0.9075 - val_loss: 0.2900 - val_accuracy: 0.8798 - val_auc_roc: 0.9086\n",
      "Epoch 22/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.2751 - accuracy: 0.8823 - auc_roc: 0.9097 - val_loss: 0.2640 - val_accuracy: 0.9013 - val_auc_roc: 0.9107\n",
      "Epoch 23/100\n",
      "4408/4408 [==============================] - 0s 45us/step - loss: 0.2683 - accuracy: 0.8845 - auc_roc: 0.9116 - val_loss: 0.2949 - val_accuracy: 0.8927 - val_auc_roc: 0.9127\n",
      "Epoch 24/100\n",
      "4408/4408 [==============================] - 0s 45us/step - loss: 0.2643 - accuracy: 0.8873 - auc_roc: 0.9136 - val_loss: 0.2698 - val_accuracy: 0.8755 - val_auc_roc: 0.9145\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 25/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.2432 - accuracy: 0.9020 - auc_roc: 0.9156 - val_loss: 0.2743 - val_accuracy: 0.8670 - val_auc_roc: 0.9168\n",
      "Epoch 26/100\n",
      "4408/4408 [==============================] - 0s 46us/step - loss: 0.2300 - accuracy: 0.9097 - auc_roc: 0.9179 - val_loss: 0.2793 - val_accuracy: 0.8798 - val_auc_roc: 0.9190\n",
      "Epoch 27/100\n",
      "4408/4408 [==============================] - 0s 42us/step - loss: 0.2254 - accuracy: 0.9083 - auc_roc: 0.9200 - val_loss: 0.2752 - val_accuracy: 0.8927 - val_auc_roc: 0.9211\n",
      "Epoch 28/100\n",
      "4408/4408 [==============================] - 0s 46us/step - loss: 0.2242 - accuracy: 0.9086 - auc_roc: 0.9221 - val_loss: 0.2768 - val_accuracy: 0.8841 - val_auc_roc: 0.9231\n",
      "Epoch 29/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.2254 - accuracy: 0.9113 - auc_roc: 0.9240 - val_loss: 0.2760 - val_accuracy: 0.8970 - val_auc_roc: 0.9249\n",
      "Epoch 30/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.2101 - accuracy: 0.9156 - auc_roc: 0.9259 - val_loss: 0.2733 - val_accuracy: 0.8841 - val_auc_roc: 0.9268\n",
      "Epoch 31/100\n",
      "4408/4408 [==============================] - 0s 42us/step - loss: 0.2070 - accuracy: 0.9158 - auc_roc: 0.9277 - val_loss: 0.2781 - val_accuracy: 0.8927 - val_auc_roc: 0.9285\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 00031: early stopping\n",
      "\n",
      "Fold  3\n",
      "Model: \"sequential_42\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_122 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_81 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_123 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_124 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4408 samples, validate on 233 samples\n",
      "Epoch 1/100\n",
      "4408/4408 [==============================] - 2s 372us/step - loss: 0.5726 - accuracy: 0.6960 - auc_roc: 0.5999 - val_loss: 0.4505 - val_accuracy: 0.7897 - val_auc_roc: 0.7160\n",
      "Epoch 2/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.4721 - accuracy: 0.7763 - auc_roc: 0.7580 - val_loss: 0.4160 - val_accuracy: 0.8069 - val_auc_roc: 0.7847\n",
      "Epoch 3/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.4416 - accuracy: 0.7915 - auc_roc: 0.8008 - val_loss: 0.4091 - val_accuracy: 0.8112 - val_auc_roc: 0.8134\n",
      "Epoch 4/100\n",
      "4408/4408 [==============================] - 0s 45us/step - loss: 0.4216 - accuracy: 0.8031 - auc_roc: 0.8226 - val_loss: 0.4082 - val_accuracy: 0.8112 - val_auc_roc: 0.8299\n",
      "Epoch 5/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.4061 - accuracy: 0.8110 - auc_roc: 0.8359 - val_loss: 0.3727 - val_accuracy: 0.8197 - val_auc_roc: 0.8412\n",
      "Epoch 6/100\n",
      "4408/4408 [==============================] - 0s 42us/step - loss: 0.3897 - accuracy: 0.8113 - auc_roc: 0.8460 - val_loss: 0.3666 - val_accuracy: 0.8455 - val_auc_roc: 0.8505\n",
      "Epoch 7/100\n",
      "4408/4408 [==============================] - 0s 46us/step - loss: 0.3804 - accuracy: 0.8246 - auc_roc: 0.8546 - val_loss: 0.3681 - val_accuracy: 0.8197 - val_auc_roc: 0.8578\n",
      "Epoch 8/100\n",
      "4408/4408 [==============================] - 0s 42us/step - loss: 0.3752 - accuracy: 0.8296 - auc_roc: 0.8609 - val_loss: 0.3605 - val_accuracy: 0.8369 - val_auc_roc: 0.8637\n",
      "Epoch 9/100\n",
      "4408/4408 [==============================] - 0s 45us/step - loss: 0.3721 - accuracy: 0.8342 - auc_roc: 0.8662 - val_loss: 0.3656 - val_accuracy: 0.8326 - val_auc_roc: 0.8684\n",
      "Epoch 10/100\n",
      "4408/4408 [==============================] - 0s 42us/step - loss: 0.3658 - accuracy: 0.8335 - auc_roc: 0.8708 - val_loss: 0.3516 - val_accuracy: 0.8369 - val_auc_roc: 0.8725\n",
      "Epoch 11/100\n",
      "4408/4408 [==============================] - 0s 45us/step - loss: 0.3510 - accuracy: 0.8401 - auc_roc: 0.8747 - val_loss: 0.3614 - val_accuracy: 0.8455 - val_auc_roc: 0.8767\n",
      "Epoch 12/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.3460 - accuracy: 0.8469 - auc_roc: 0.8784 - val_loss: 0.3440 - val_accuracy: 0.8412 - val_auc_roc: 0.8804\n",
      "Epoch 13/100\n",
      "4408/4408 [==============================] - 0s 45us/step - loss: 0.3368 - accuracy: 0.8480 - auc_roc: 0.8821 - val_loss: 0.3402 - val_accuracy: 0.8541 - val_auc_roc: 0.8838\n",
      "Epoch 14/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.3330 - accuracy: 0.8534 - auc_roc: 0.8854 - val_loss: 0.3420 - val_accuracy: 0.8541 - val_auc_roc: 0.8869\n",
      "Epoch 15/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.3339 - accuracy: 0.8523 - auc_roc: 0.8884 - val_loss: 0.3362 - val_accuracy: 0.8670 - val_auc_roc: 0.8895\n",
      "Epoch 16/100\n",
      "4408/4408 [==============================] - 0s 41us/step - loss: 0.3191 - accuracy: 0.8598 - auc_roc: 0.8910 - val_loss: 0.3309 - val_accuracy: 0.8498 - val_auc_roc: 0.8922\n",
      "Epoch 17/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.3180 - accuracy: 0.8603 - auc_roc: 0.8935 - val_loss: 0.3511 - val_accuracy: 0.8670 - val_auc_roc: 0.8947\n",
      "Epoch 18/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.3051 - accuracy: 0.8714 - auc_roc: 0.8960 - val_loss: 0.3447 - val_accuracy: 0.8627 - val_auc_roc: 0.8973\n",
      "Epoch 19/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.3023 - accuracy: 0.8655 - auc_roc: 0.8987 - val_loss: 0.3507 - val_accuracy: 0.8584 - val_auc_roc: 0.8996\n",
      "Epoch 20/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.2977 - accuracy: 0.8664 - auc_roc: 0.9007 - val_loss: 0.3622 - val_accuracy: 0.8584 - val_auc_roc: 0.9017\n",
      "Epoch 21/100\n",
      "4408/4408 [==============================] - 0s 45us/step - loss: 0.2941 - accuracy: 0.8764 - auc_roc: 0.9029 - val_loss: 0.3586 - val_accuracy: 0.8240 - val_auc_roc: 0.9037\n",
      "Epoch 22/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.2948 - accuracy: 0.8705 - auc_roc: 0.9046 - val_loss: 0.3447 - val_accuracy: 0.8712 - val_auc_roc: 0.9056\n",
      "Epoch 23/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.2850 - accuracy: 0.8700 - auc_roc: 0.9066 - val_loss: 0.3600 - val_accuracy: 0.8584 - val_auc_roc: 0.9074\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 24/100\n",
      "4408/4408 [==============================] - 0s 46us/step - loss: 0.2745 - accuracy: 0.8843 - auc_roc: 0.9083 - val_loss: 0.3325 - val_accuracy: 0.8670 - val_auc_roc: 0.9093\n",
      "Epoch 25/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.2599 - accuracy: 0.8877 - auc_roc: 0.9104 - val_loss: 0.3371 - val_accuracy: 0.8584 - val_auc_roc: 0.9114\n",
      "Epoch 26/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.2514 - accuracy: 0.8945 - auc_roc: 0.9125 - val_loss: 0.3386 - val_accuracy: 0.8670 - val_auc_roc: 0.9135\n",
      "Epoch 27/100\n",
      "4408/4408 [==============================] - 0s 45us/step - loss: 0.2454 - accuracy: 0.8961 - auc_roc: 0.9145 - val_loss: 0.3342 - val_accuracy: 0.8712 - val_auc_roc: 0.9155\n",
      "Epoch 28/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.2423 - accuracy: 0.8968 - auc_roc: 0.9165 - val_loss: 0.3331 - val_accuracy: 0.8670 - val_auc_roc: 0.9173\n",
      "Epoch 29/100\n",
      "4408/4408 [==============================] - 0s 45us/step - loss: 0.2407 - accuracy: 0.8981 - auc_roc: 0.9183 - val_loss: 0.3370 - val_accuracy: 0.8841 - val_auc_roc: 0.9191\n",
      "Epoch 30/100\n",
      "4408/4408 [==============================] - 0s 45us/step - loss: 0.2369 - accuracy: 0.9034 - auc_roc: 0.9200 - val_loss: 0.3368 - val_accuracy: 0.8798 - val_auc_roc: 0.9208\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 00030: early stopping\n",
      "\n",
      "Fold  4\n",
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_125 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_126 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_84 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_127 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4408 samples, validate on 233 samples\n",
      "Epoch 1/100\n",
      "4408/4408 [==============================] - 2s 383us/step - loss: 0.5835 - accuracy: 0.6944 - auc_roc: 0.6000 - val_loss: 0.4441 - val_accuracy: 0.8026 - val_auc_roc: 0.7074\n",
      "Epoch 2/100\n",
      "4408/4408 [==============================] - 0s 54us/step - loss: 0.4692 - accuracy: 0.7781 - auc_roc: 0.7526 - val_loss: 0.4238 - val_accuracy: 0.7854 - val_auc_roc: 0.7806\n",
      "Epoch 3/100\n",
      "4408/4408 [==============================] - 0s 48us/step - loss: 0.4449 - accuracy: 0.7945 - auc_roc: 0.7985 - val_loss: 0.4123 - val_accuracy: 0.8112 - val_auc_roc: 0.8089\n",
      "Epoch 4/100\n",
      "4408/4408 [==============================] - 0s 46us/step - loss: 0.4226 - accuracy: 0.8047 - auc_roc: 0.8191 - val_loss: 0.4004 - val_accuracy: 0.7983 - val_auc_roc: 0.8261\n",
      "Epoch 5/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.4063 - accuracy: 0.8140 - auc_roc: 0.8324 - val_loss: 0.3984 - val_accuracy: 0.8112 - val_auc_roc: 0.8386\n",
      "Epoch 6/100\n",
      "4408/4408 [==============================] - 0s 45us/step - loss: 0.3917 - accuracy: 0.8221 - auc_roc: 0.8438 - val_loss: 0.3956 - val_accuracy: 0.8069 - val_auc_roc: 0.8481\n",
      "Epoch 7/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.3786 - accuracy: 0.8326 - auc_roc: 0.8525 - val_loss: 0.3885 - val_accuracy: 0.8069 - val_auc_roc: 0.8562\n",
      "Epoch 8/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.3727 - accuracy: 0.8301 - auc_roc: 0.8592 - val_loss: 0.3952 - val_accuracy: 0.8026 - val_auc_roc: 0.8624\n",
      "Epoch 9/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.3651 - accuracy: 0.8312 - auc_roc: 0.8650 - val_loss: 0.3981 - val_accuracy: 0.7983 - val_auc_roc: 0.8674\n",
      "Epoch 10/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.3655 - accuracy: 0.8337 - auc_roc: 0.8695 - val_loss: 0.4005 - val_accuracy: 0.7940 - val_auc_roc: 0.8715\n",
      "Epoch 11/100\n",
      "4408/4408 [==============================] - 0s 48us/step - loss: 0.3562 - accuracy: 0.8426 - auc_roc: 0.8734 - val_loss: 0.3918 - val_accuracy: 0.8112 - val_auc_roc: 0.8754\n",
      "Epoch 12/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.3440 - accuracy: 0.8423 - auc_roc: 0.8774 - val_loss: 0.3973 - val_accuracy: 0.7940 - val_auc_roc: 0.8790\n",
      "Epoch 13/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.3324 - accuracy: 0.8516 - auc_roc: 0.8808 - val_loss: 0.3990 - val_accuracy: 0.7940 - val_auc_roc: 0.8826\n",
      "Epoch 14/100\n",
      "4408/4408 [==============================] - 0s 48us/step - loss: 0.3227 - accuracy: 0.8510 - auc_roc: 0.8842 - val_loss: 0.4094 - val_accuracy: 0.7983 - val_auc_roc: 0.8860\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 15/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.3109 - accuracy: 0.8580 - auc_roc: 0.8878 - val_loss: 0.3953 - val_accuracy: 0.8112 - val_auc_roc: 0.8894\n",
      "Epoch 16/100\n",
      "4408/4408 [==============================] - 0s 48us/step - loss: 0.3036 - accuracy: 0.8734 - auc_roc: 0.8910 - val_loss: 0.3941 - val_accuracy: 0.8069 - val_auc_roc: 0.8927\n",
      "Epoch 17/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.2972 - accuracy: 0.8741 - auc_roc: 0.8942 - val_loss: 0.3930 - val_accuracy: 0.7983 - val_auc_roc: 0.8958\n",
      "Epoch 18/100\n",
      "4408/4408 [==============================] - 0s 42us/step - loss: 0.2916 - accuracy: 0.8725 - auc_roc: 0.8972 - val_loss: 0.3952 - val_accuracy: 0.8026 - val_auc_roc: 0.8985\n",
      "Epoch 19/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.2931 - accuracy: 0.8734 - auc_roc: 0.8999 - val_loss: 0.3973 - val_accuracy: 0.8026 - val_auc_roc: 0.9010\n",
      "Epoch 20/100\n",
      "4408/4408 [==============================] - 0s 47us/step - loss: 0.2920 - accuracy: 0.8752 - auc_roc: 0.9021 - val_loss: 0.3970 - val_accuracy: 0.8069 - val_auc_roc: 0.9033\n",
      "Epoch 21/100\n",
      "4408/4408 [==============================] - 0s 42us/step - loss: 0.2912 - accuracy: 0.8752 - auc_roc: 0.9043 - val_loss: 0.3948 - val_accuracy: 0.8240 - val_auc_roc: 0.9052\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 00021: early stopping\n",
      "\n",
      "Fold  5\n",
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_128 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_85 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_129 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_86 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_130 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4408 samples, validate on 233 samples\n",
      "Epoch 1/100\n",
      "4408/4408 [==============================] - 2s 387us/step - loss: 0.5964 - accuracy: 0.6881 - auc_roc: 0.5655 - val_loss: 0.4707 - val_accuracy: 0.7811 - val_auc_roc: 0.6874\n",
      "Epoch 2/100\n",
      "4408/4408 [==============================] - 0s 46us/step - loss: 0.4789 - accuracy: 0.7734 - auc_roc: 0.7359 - val_loss: 0.4271 - val_accuracy: 0.7897 - val_auc_roc: 0.7679\n",
      "Epoch 3/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.4414 - accuracy: 0.7911 - auc_roc: 0.7876 - val_loss: 0.4068 - val_accuracy: 0.8112 - val_auc_roc: 0.8024\n",
      "Epoch 4/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.4214 - accuracy: 0.7999 - auc_roc: 0.8139 - val_loss: 0.4022 - val_accuracy: 0.8112 - val_auc_roc: 0.8219\n",
      "Epoch 5/100\n",
      "4408/4408 [==============================] - 0s 46us/step - loss: 0.4075 - accuracy: 0.8131 - auc_roc: 0.8283 - val_loss: 0.3922 - val_accuracy: 0.8112 - val_auc_roc: 0.8353\n",
      "Epoch 6/100\n",
      "4408/4408 [==============================] - 0s 45us/step - loss: 0.3971 - accuracy: 0.8119 - auc_roc: 0.8404 - val_loss: 0.4003 - val_accuracy: 0.8197 - val_auc_roc: 0.8446\n",
      "Epoch 7/100\n",
      "4408/4408 [==============================] - 0s 47us/step - loss: 0.3897 - accuracy: 0.8210 - auc_roc: 0.8486 - val_loss: 0.3946 - val_accuracy: 0.8112 - val_auc_roc: 0.8521\n",
      "Epoch 8/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.3769 - accuracy: 0.8237 - auc_roc: 0.8548 - val_loss: 0.3828 - val_accuracy: 0.8155 - val_auc_roc: 0.8585\n",
      "Epoch 9/100\n",
      "4408/4408 [==============================] - 0s 47us/step - loss: 0.3692 - accuracy: 0.8335 - auc_roc: 0.8614 - val_loss: 0.3824 - val_accuracy: 0.8197 - val_auc_roc: 0.8642\n",
      "Epoch 10/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.3623 - accuracy: 0.8373 - auc_roc: 0.8665 - val_loss: 0.3784 - val_accuracy: 0.8197 - val_auc_roc: 0.8689\n",
      "Epoch 11/100\n",
      "4408/4408 [==============================] - 0s 45us/step - loss: 0.3530 - accuracy: 0.8401 - auc_roc: 0.8709 - val_loss: 0.3768 - val_accuracy: 0.8155 - val_auc_roc: 0.8731\n",
      "Epoch 12/100\n",
      "4408/4408 [==============================] - 0s 48us/step - loss: 0.3535 - accuracy: 0.8426 - auc_roc: 0.8752 - val_loss: 0.3716 - val_accuracy: 0.8326 - val_auc_roc: 0.8768\n",
      "Epoch 13/100\n",
      "4408/4408 [==============================] - 0s 45us/step - loss: 0.3441 - accuracy: 0.8480 - auc_roc: 0.8786 - val_loss: 0.4063 - val_accuracy: 0.8069 - val_auc_roc: 0.8802\n",
      "Epoch 14/100\n",
      "4408/4408 [==============================] - 0s 45us/step - loss: 0.3331 - accuracy: 0.8469 - auc_roc: 0.8818 - val_loss: 0.3711 - val_accuracy: 0.8498 - val_auc_roc: 0.8834\n",
      "Epoch 15/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.3269 - accuracy: 0.8500 - auc_roc: 0.8852 - val_loss: 0.3944 - val_accuracy: 0.8369 - val_auc_roc: 0.8866\n",
      "Epoch 16/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.3117 - accuracy: 0.8612 - auc_roc: 0.8882 - val_loss: 0.3776 - val_accuracy: 0.8326 - val_auc_roc: 0.8896\n",
      "Epoch 17/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.3072 - accuracy: 0.8616 - auc_roc: 0.8910 - val_loss: 0.3919 - val_accuracy: 0.8240 - val_auc_roc: 0.8925\n",
      "Epoch 18/100\n",
      "4408/4408 [==============================] - 0s 45us/step - loss: 0.3043 - accuracy: 0.8648 - auc_roc: 0.8940 - val_loss: 0.3806 - val_accuracy: 0.8369 - val_auc_roc: 0.8952\n",
      "Epoch 19/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.3019 - accuracy: 0.8675 - auc_roc: 0.8964 - val_loss: 0.3843 - val_accuracy: 0.8541 - val_auc_roc: 0.8975\n",
      "Epoch 20/100\n",
      "4408/4408 [==============================] - 0s 46us/step - loss: 0.3007 - accuracy: 0.8650 - auc_roc: 0.8986 - val_loss: 0.3718 - val_accuracy: 0.8541 - val_auc_roc: 0.8996\n",
      "Epoch 21/100\n",
      "4408/4408 [==============================] - 0s 47us/step - loss: 0.2868 - accuracy: 0.8707 - auc_roc: 0.9008 - val_loss: 0.3772 - val_accuracy: 0.8369 - val_auc_roc: 0.9019\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 22/100\n",
      "4408/4408 [==============================] - 0s 52us/step - loss: 0.2707 - accuracy: 0.8836 - auc_roc: 0.9031 - val_loss: 0.3736 - val_accuracy: 0.8412 - val_auc_roc: 0.9044\n",
      "Epoch 23/100\n",
      "4408/4408 [==============================] - 0s 49us/step - loss: 0.2606 - accuracy: 0.8884 - auc_roc: 0.9056 - val_loss: 0.3733 - val_accuracy: 0.8455 - val_auc_roc: 0.9069\n",
      "Epoch 24/100\n",
      "4408/4408 [==============================] - 0s 43us/step - loss: 0.2544 - accuracy: 0.8897 - auc_roc: 0.9081 - val_loss: 0.3726 - val_accuracy: 0.8455 - val_auc_roc: 0.9092\n",
      "Epoch 25/100\n",
      "4408/4408 [==============================] - 0s 48us/step - loss: 0.2555 - accuracy: 0.8972 - auc_roc: 0.9102 - val_loss: 0.3716 - val_accuracy: 0.8584 - val_auc_roc: 0.9114\n",
      "Epoch 26/100\n",
      "4408/4408 [==============================] - 0s 46us/step - loss: 0.2543 - accuracy: 0.8934 - auc_roc: 0.9123 - val_loss: 0.3760 - val_accuracy: 0.8455 - val_auc_roc: 0.9134\n",
      "Epoch 27/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.2494 - accuracy: 0.8963 - auc_roc: 0.9143 - val_loss: 0.3780 - val_accuracy: 0.8541 - val_auc_roc: 0.9153\n",
      "Epoch 28/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.2455 - accuracy: 0.8990 - auc_roc: 0.9161 - val_loss: 0.3787 - val_accuracy: 0.8455 - val_auc_roc: 0.9171\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 00028: early stopping\n",
      "\n",
      "Fold  6\n",
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_131 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_87 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_132 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_88 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_133 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4408 samples, validate on 233 samples\n",
      "Epoch 1/100\n",
      "4408/4408 [==============================] - 2s 397us/step - loss: 0.5674 - accuracy: 0.6960 - auc_roc: 0.6002 - val_loss: 0.4602 - val_accuracy: 0.7682 - val_auc_roc: 0.7277\n",
      "Epoch 2/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.4635 - accuracy: 0.7815 - auc_roc: 0.7678 - val_loss: 0.4360 - val_accuracy: 0.8197 - val_auc_roc: 0.7920\n",
      "Epoch 3/100\n",
      "4408/4408 [==============================] - 0s 48us/step - loss: 0.4255 - accuracy: 0.8010 - auc_roc: 0.8088 - val_loss: 0.4217 - val_accuracy: 0.8112 - val_auc_roc: 0.8213\n",
      "Epoch 4/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.4161 - accuracy: 0.8040 - auc_roc: 0.8288 - val_loss: 0.4170 - val_accuracy: 0.8197 - val_auc_roc: 0.8360\n",
      "Epoch 5/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.3963 - accuracy: 0.8135 - auc_roc: 0.8424 - val_loss: 0.3922 - val_accuracy: 0.8112 - val_auc_roc: 0.8471\n",
      "Epoch 6/100\n",
      "4408/4408 [==============================] - 0s 46us/step - loss: 0.3858 - accuracy: 0.8310 - auc_roc: 0.8519 - val_loss: 0.3886 - val_accuracy: 0.8326 - val_auc_roc: 0.8559\n",
      "Epoch 7/100\n",
      "4408/4408 [==============================] - 0s 42us/step - loss: 0.3744 - accuracy: 0.8303 - auc_roc: 0.8597 - val_loss: 0.3794 - val_accuracy: 0.8369 - val_auc_roc: 0.8627\n",
      "Epoch 8/100\n",
      "4408/4408 [==============================] - 0s 49us/step - loss: 0.3687 - accuracy: 0.8294 - auc_roc: 0.8658 - val_loss: 0.3771 - val_accuracy: 0.8197 - val_auc_roc: 0.8684\n",
      "Epoch 9/100\n",
      "4408/4408 [==============================] - 0s 49us/step - loss: 0.3696 - accuracy: 0.8317 - auc_roc: 0.8706 - val_loss: 0.3707 - val_accuracy: 0.8326 - val_auc_roc: 0.8726\n",
      "Epoch 10/100\n",
      "4408/4408 [==============================] - 0s 47us/step - loss: 0.3475 - accuracy: 0.8464 - auc_roc: 0.8749 - val_loss: 0.3688 - val_accuracy: 0.8240 - val_auc_roc: 0.8774\n",
      "Epoch 11/100\n",
      "4408/4408 [==============================] - 0s 44us/step - loss: 0.3460 - accuracy: 0.8437 - auc_roc: 0.8794 - val_loss: 0.3653 - val_accuracy: 0.8240 - val_auc_roc: 0.8813\n",
      "Epoch 12/100\n",
      "4408/4408 [==============================] - 0s 46us/step - loss: 0.3462 - accuracy: 0.8444 - auc_roc: 0.8828 - val_loss: 0.3800 - val_accuracy: 0.8412 - val_auc_roc: 0.8844\n",
      "Epoch 13/100\n",
      "4408/4408 [==============================] - 0s 49us/step - loss: 0.3303 - accuracy: 0.8534 - auc_roc: 0.8861 - val_loss: 0.3798 - val_accuracy: 0.8412 - val_auc_roc: 0.8878\n",
      "Epoch 14/100\n",
      "4408/4408 [==============================] - 0s 52us/step - loss: 0.3232 - accuracy: 0.8598 - auc_roc: 0.8894 - val_loss: 0.4114 - val_accuracy: 0.8369 - val_auc_roc: 0.8909\n",
      "Epoch 15/100\n",
      "4408/4408 [==============================] - 0s 49us/step - loss: 0.3185 - accuracy: 0.8578 - auc_roc: 0.8921 - val_loss: 0.3564 - val_accuracy: 0.8326 - val_auc_roc: 0.8936\n",
      "Epoch 16/100\n",
      "4408/4408 [==============================] - 0s 48us/step - loss: 0.3077 - accuracy: 0.8693 - auc_roc: 0.8952 - val_loss: 0.3470 - val_accuracy: 0.8283 - val_auc_roc: 0.8964\n",
      "Epoch 17/100\n",
      "4408/4408 [==============================] - 0s 45us/step - loss: 0.3082 - accuracy: 0.8705 - auc_roc: 0.8978 - val_loss: 0.3515 - val_accuracy: 0.8369 - val_auc_roc: 0.8990\n",
      "Epoch 18/100\n",
      "4408/4408 [==============================] - 0s 48us/step - loss: 0.3042 - accuracy: 0.8607 - auc_roc: 0.9001 - val_loss: 0.3593 - val_accuracy: 0.8412 - val_auc_roc: 0.9012\n",
      "Epoch 19/100\n",
      "4408/4408 [==============================] - 0s 47us/step - loss: 0.2994 - accuracy: 0.8739 - auc_roc: 0.9023 - val_loss: 0.3498 - val_accuracy: 0.8412 - val_auc_roc: 0.9034\n",
      "Epoch 20/100\n",
      "4408/4408 [==============================] - 0s 48us/step - loss: 0.2862 - accuracy: 0.8809 - auc_roc: 0.9045 - val_loss: 0.3547 - val_accuracy: 0.8369 - val_auc_roc: 0.9056\n",
      "Epoch 21/100\n",
      "4408/4408 [==============================] - 0s 48us/step - loss: 0.2819 - accuracy: 0.8766 - auc_roc: 0.9067 - val_loss: 0.3573 - val_accuracy: 0.8283 - val_auc_roc: 0.9077\n",
      "Epoch 22/100\n",
      "4408/4408 [==============================] - 0s 46us/step - loss: 0.2821 - accuracy: 0.8782 - auc_roc: 0.9085 - val_loss: 0.4045 - val_accuracy: 0.8455 - val_auc_roc: 0.9095\n",
      "Epoch 23/100\n",
      "4408/4408 [==============================] - 0s 49us/step - loss: 0.2663 - accuracy: 0.8791 - auc_roc: 0.9105 - val_loss: 0.4059 - val_accuracy: 0.8498 - val_auc_roc: 0.9115\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 24/100\n",
      "4408/4408 [==============================] - 0s 56us/step - loss: 0.2556 - accuracy: 0.8909 - auc_roc: 0.9124 - val_loss: 0.3628 - val_accuracy: 0.8455 - val_auc_roc: 0.9135\n",
      "Epoch 25/100\n",
      "4408/4408 [==============================] - 0s 57us/step - loss: 0.2357 - accuracy: 0.9011 - auc_roc: 0.9146 - val_loss: 0.3702 - val_accuracy: 0.8369 - val_auc_roc: 0.9158\n",
      "Epoch 26/100\n",
      "4408/4408 [==============================] - 0s 59us/step - loss: 0.2403 - accuracy: 0.9025 - auc_roc: 0.9168 - val_loss: 0.3711 - val_accuracy: 0.8369 - val_auc_roc: 0.9178\n",
      "Epoch 27/100\n",
      "4408/4408 [==============================] - 0s 52us/step - loss: 0.2354 - accuracy: 0.9020 - auc_roc: 0.9188 - val_loss: 0.3770 - val_accuracy: 0.8455 - val_auc_roc: 0.9197\n",
      "Epoch 28/100\n",
      "4408/4408 [==============================] - 0s 58us/step - loss: 0.2276 - accuracy: 0.9054 - auc_roc: 0.9207 - val_loss: 0.3754 - val_accuracy: 0.8412 - val_auc_roc: 0.9216\n",
      "Epoch 29/100\n",
      "4408/4408 [==============================] - 0s 54us/step - loss: 0.2222 - accuracy: 0.9113 - auc_roc: 0.9225 - val_loss: 0.3786 - val_accuracy: 0.8412 - val_auc_roc: 0.9234\n",
      "Epoch 30/100\n",
      "4408/4408 [==============================] - 0s 58us/step - loss: 0.2231 - accuracy: 0.9108 - auc_roc: 0.9242 - val_loss: 0.3750 - val_accuracy: 0.8412 - val_auc_roc: 0.9250\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 00030: early stopping\n",
      "\n",
      "Fold  7\n",
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_134 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_89 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_135 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_90 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_136 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4409 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "4409/4409 [==============================] - 2s 414us/step - loss: 0.5625 - accuracy: 0.7047 - auc_roc: 0.6163 - val_loss: 0.4458 - val_accuracy: 0.8017 - val_auc_roc: 0.7309\n",
      "Epoch 2/100\n",
      "4409/4409 [==============================] - 0s 48us/step - loss: 0.4642 - accuracy: 0.7780 - auc_roc: 0.7664 - val_loss: 0.4256 - val_accuracy: 0.8103 - val_auc_roc: 0.7954\n",
      "Epoch 3/100\n",
      "4409/4409 [==============================] - 0s 56us/step - loss: 0.4362 - accuracy: 0.7952 - auc_roc: 0.8084 - val_loss: 0.4176 - val_accuracy: 0.7845 - val_auc_roc: 0.8207\n",
      "Epoch 4/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.4159 - accuracy: 0.8059 - auc_roc: 0.8283 - val_loss: 0.4101 - val_accuracy: 0.8147 - val_auc_roc: 0.8362\n",
      "Epoch 5/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.4047 - accuracy: 0.8152 - auc_roc: 0.8418 - val_loss: 0.4182 - val_accuracy: 0.7888 - val_auc_roc: 0.8462\n",
      "Epoch 6/100\n",
      "4409/4409 [==============================] - 0s 50us/step - loss: 0.3862 - accuracy: 0.8186 - auc_roc: 0.8503 - val_loss: 0.4280 - val_accuracy: 0.8233 - val_auc_roc: 0.8546\n",
      "Epoch 7/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.3818 - accuracy: 0.8263 - auc_roc: 0.8574 - val_loss: 0.4052 - val_accuracy: 0.8362 - val_auc_roc: 0.8610\n",
      "Epoch 8/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.3641 - accuracy: 0.8394 - auc_roc: 0.8643 - val_loss: 0.4016 - val_accuracy: 0.8491 - val_auc_roc: 0.8673\n",
      "Epoch 9/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.3610 - accuracy: 0.8396 - auc_roc: 0.8697 - val_loss: 0.3933 - val_accuracy: 0.8405 - val_auc_roc: 0.8724\n",
      "Epoch 10/100\n",
      "4409/4409 [==============================] - 0s 49us/step - loss: 0.3533 - accuracy: 0.8419 - auc_roc: 0.8744 - val_loss: 0.4161 - val_accuracy: 0.8276 - val_auc_roc: 0.8768\n",
      "Epoch 11/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.3384 - accuracy: 0.8508 - auc_roc: 0.8790 - val_loss: 0.4028 - val_accuracy: 0.8233 - val_auc_roc: 0.8810\n",
      "Epoch 12/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.3367 - accuracy: 0.8433 - auc_roc: 0.8831 - val_loss: 0.4143 - val_accuracy: 0.8362 - val_auc_roc: 0.8845\n",
      "Epoch 13/100\n",
      "4409/4409 [==============================] - 0s 50us/step - loss: 0.3322 - accuracy: 0.8560 - auc_roc: 0.8863 - val_loss: 0.4381 - val_accuracy: 0.8190 - val_auc_roc: 0.8877\n",
      "Epoch 14/100\n",
      "4409/4409 [==============================] - 0s 55us/step - loss: 0.3219 - accuracy: 0.8582 - auc_roc: 0.8893 - val_loss: 0.4177 - val_accuracy: 0.8276 - val_auc_roc: 0.8907\n",
      "Epoch 15/100\n",
      "4409/4409 [==============================] - 0s 50us/step - loss: 0.3154 - accuracy: 0.8582 - auc_roc: 0.8922 - val_loss: 0.4282 - val_accuracy: 0.8233 - val_auc_roc: 0.8935\n",
      "Epoch 16/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.3133 - accuracy: 0.8553 - auc_roc: 0.8949 - val_loss: 0.4182 - val_accuracy: 0.8405 - val_auc_roc: 0.8960\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 17/100\n",
      "4409/4409 [==============================] - 0s 50us/step - loss: 0.2945 - accuracy: 0.8748 - auc_roc: 0.8974 - val_loss: 0.4248 - val_accuracy: 0.8362 - val_auc_roc: 0.8989\n",
      "Epoch 18/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.2860 - accuracy: 0.8780 - auc_roc: 0.9003 - val_loss: 0.4273 - val_accuracy: 0.8276 - val_auc_roc: 0.9016\n",
      "Epoch 19/100\n",
      "4409/4409 [==============================] - 0s 49us/step - loss: 0.2827 - accuracy: 0.8796 - auc_roc: 0.9029 - val_loss: 0.4169 - val_accuracy: 0.8362 - val_auc_roc: 0.9042\n",
      "Epoch 20/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.2755 - accuracy: 0.8825 - auc_roc: 0.9054 - val_loss: 0.4255 - val_accuracy: 0.8319 - val_auc_roc: 0.9066\n",
      "Epoch 21/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.2709 - accuracy: 0.8857 - auc_roc: 0.9077 - val_loss: 0.4223 - val_accuracy: 0.8405 - val_auc_roc: 0.9089\n",
      "Epoch 22/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.2716 - accuracy: 0.8850 - auc_roc: 0.9099 - val_loss: 0.4224 - val_accuracy: 0.8319 - val_auc_roc: 0.9110\n",
      "Epoch 23/100\n",
      "4409/4409 [==============================] - 0s 49us/step - loss: 0.2656 - accuracy: 0.8864 - auc_roc: 0.9119 - val_loss: 0.4247 - val_accuracy: 0.8276 - val_auc_roc: 0.9130\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 00023: early stopping\n",
      "\n",
      "Fold  8\n",
      "Model: \"sequential_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_137 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_91 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_138 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_92 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_139 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4409 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "4409/4409 [==============================] - 2s 448us/step - loss: 0.5664 - accuracy: 0.7011 - auc_roc: 0.6136 - val_loss: 0.4393 - val_accuracy: 0.7759 - val_auc_roc: 0.7268\n",
      "Epoch 2/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.4594 - accuracy: 0.7836 - auc_roc: 0.7692 - val_loss: 0.4267 - val_accuracy: 0.8060 - val_auc_roc: 0.7949\n",
      "Epoch 3/100\n",
      "4409/4409 [==============================] - 0s 56us/step - loss: 0.4340 - accuracy: 0.7966 - auc_roc: 0.8089 - val_loss: 0.4167 - val_accuracy: 0.7974 - val_auc_roc: 0.8203\n",
      "Epoch 4/100\n",
      "4409/4409 [==============================] - 0s 59us/step - loss: 0.4111 - accuracy: 0.8097 - auc_roc: 0.8295 - val_loss: 0.4116 - val_accuracy: 0.7974 - val_auc_roc: 0.8361\n",
      "Epoch 5/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.4016 - accuracy: 0.8152 - auc_roc: 0.8419 - val_loss: 0.4064 - val_accuracy: 0.7931 - val_auc_roc: 0.8468\n",
      "Epoch 6/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.3996 - accuracy: 0.8174 - auc_roc: 0.8503 - val_loss: 0.4101 - val_accuracy: 0.7888 - val_auc_roc: 0.8538\n",
      "Epoch 7/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.3796 - accuracy: 0.8224 - auc_roc: 0.8572 - val_loss: 0.4016 - val_accuracy: 0.7716 - val_auc_roc: 0.8605\n",
      "Epoch 8/100\n",
      "4409/4409 [==============================] - 0s 55us/step - loss: 0.3744 - accuracy: 0.8333 - auc_roc: 0.8631 - val_loss: 0.3921 - val_accuracy: 0.8103 - val_auc_roc: 0.8659\n",
      "Epoch 9/100\n",
      "4409/4409 [==============================] - 0s 59us/step - loss: 0.3648 - accuracy: 0.8403 - auc_roc: 0.8682 - val_loss: 0.3980 - val_accuracy: 0.8405 - val_auc_roc: 0.8707\n",
      "Epoch 10/100\n",
      "4409/4409 [==============================] - 0s 56us/step - loss: 0.3580 - accuracy: 0.8340 - auc_roc: 0.8730 - val_loss: 0.3947 - val_accuracy: 0.8319 - val_auc_roc: 0.8749\n",
      "Epoch 11/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.3513 - accuracy: 0.8424 - auc_roc: 0.8768 - val_loss: 0.3916 - val_accuracy: 0.8233 - val_auc_roc: 0.8786\n",
      "Epoch 12/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.3338 - accuracy: 0.8480 - auc_roc: 0.8807 - val_loss: 0.3886 - val_accuracy: 0.8190 - val_auc_roc: 0.8825\n",
      "Epoch 13/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.3364 - accuracy: 0.8480 - auc_roc: 0.8842 - val_loss: 0.4071 - val_accuracy: 0.8405 - val_auc_roc: 0.8857\n",
      "Epoch 14/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.3335 - accuracy: 0.8503 - auc_roc: 0.8871 - val_loss: 0.3977 - val_accuracy: 0.8233 - val_auc_roc: 0.8885\n",
      "Epoch 15/100\n",
      "4409/4409 [==============================] - 0s 50us/step - loss: 0.3226 - accuracy: 0.8630 - auc_roc: 0.8900 - val_loss: 0.3951 - val_accuracy: 0.8319 - val_auc_roc: 0.8913\n",
      "Epoch 16/100\n",
      "4409/4409 [==============================] - 0s 48us/step - loss: 0.3154 - accuracy: 0.8628 - auc_roc: 0.8926 - val_loss: 0.3935 - val_accuracy: 0.8276 - val_auc_roc: 0.8940\n",
      "Epoch 17/100\n",
      "4409/4409 [==============================] - 0s 50us/step - loss: 0.3053 - accuracy: 0.8630 - auc_roc: 0.8952 - val_loss: 0.4109 - val_accuracy: 0.8233 - val_auc_roc: 0.8966\n",
      "Epoch 18/100\n",
      "4409/4409 [==============================] - 0s 58us/step - loss: 0.3042 - accuracy: 0.8637 - auc_roc: 0.8978 - val_loss: 0.3927 - val_accuracy: 0.7974 - val_auc_roc: 0.8989\n",
      "Epoch 19/100\n",
      "4409/4409 [==============================] - 0s 56us/step - loss: 0.2935 - accuracy: 0.8714 - auc_roc: 0.9001 - val_loss: 0.3967 - val_accuracy: 0.8319 - val_auc_roc: 0.9012\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 20/100\n",
      "4409/4409 [==============================] - 0s 50us/step - loss: 0.2774 - accuracy: 0.8818 - auc_roc: 0.9025 - val_loss: 0.3951 - val_accuracy: 0.8190 - val_auc_roc: 0.9038\n",
      "Epoch 21/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.2649 - accuracy: 0.8832 - auc_roc: 0.9051 - val_loss: 0.3994 - val_accuracy: 0.8276 - val_auc_roc: 0.9064\n",
      "Epoch 22/100\n",
      "4409/4409 [==============================] - 0s 57us/step - loss: 0.2635 - accuracy: 0.8895 - auc_roc: 0.9075 - val_loss: 0.4013 - val_accuracy: 0.8276 - val_auc_roc: 0.9088\n",
      "Epoch 23/100\n",
      "4409/4409 [==============================] - 0s 56us/step - loss: 0.2617 - accuracy: 0.8891 - auc_roc: 0.9099 - val_loss: 0.4049 - val_accuracy: 0.8233 - val_auc_roc: 0.9109\n",
      "Epoch 24/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.2584 - accuracy: 0.8966 - auc_roc: 0.9119 - val_loss: 0.4017 - val_accuracy: 0.8233 - val_auc_roc: 0.9130\n",
      "Epoch 25/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.2561 - accuracy: 0.8929 - auc_roc: 0.9140 - val_loss: 0.4057 - val_accuracy: 0.8103 - val_auc_roc: 0.9149\n",
      "Epoch 26/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.2543 - accuracy: 0.8948 - auc_roc: 0.9156 - val_loss: 0.3979 - val_accuracy: 0.8233 - val_auc_roc: 0.9166\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 00026: early stopping\n",
      "\n",
      "Fold  9\n",
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_140 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_93 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_141 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_94 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_142 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4409 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "4409/4409 [==============================] - 2s 434us/step - loss: 0.5559 - accuracy: 0.7086 - auc_roc: 0.6312 - val_loss: 0.4538 - val_accuracy: 0.7716 - val_auc_roc: 0.7433\n",
      "Epoch 2/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.4601 - accuracy: 0.7795 - auc_roc: 0.7779 - val_loss: 0.4190 - val_accuracy: 0.7845 - val_auc_roc: 0.8005\n",
      "Epoch 3/100\n",
      "4409/4409 [==============================] - 0s 50us/step - loss: 0.4406 - accuracy: 0.7961 - auc_roc: 0.8127 - val_loss: 0.4067 - val_accuracy: 0.7845 - val_auc_roc: 0.8224\n",
      "Epoch 4/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.4234 - accuracy: 0.8097 - auc_roc: 0.8296 - val_loss: 0.3907 - val_accuracy: 0.7931 - val_auc_roc: 0.8364\n",
      "Epoch 5/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.4037 - accuracy: 0.8158 - auc_roc: 0.8414 - val_loss: 0.4062 - val_accuracy: 0.7845 - val_auc_roc: 0.8466\n",
      "Epoch 6/100\n",
      "4409/4409 [==============================] - 0s 57us/step - loss: 0.3927 - accuracy: 0.8229 - auc_roc: 0.8508 - val_loss: 0.3912 - val_accuracy: 0.8060 - val_auc_roc: 0.8548\n",
      "Epoch 7/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.3919 - accuracy: 0.8281 - auc_roc: 0.8577 - val_loss: 0.3787 - val_accuracy: 0.8103 - val_auc_roc: 0.8603\n",
      "Epoch 8/100\n",
      "4409/4409 [==============================] - 0s 58us/step - loss: 0.3792 - accuracy: 0.8215 - auc_roc: 0.8629 - val_loss: 0.3751 - val_accuracy: 0.8276 - val_auc_roc: 0.8653\n",
      "Epoch 9/100\n",
      "4409/4409 [==============================] - 0s 58us/step - loss: 0.3577 - accuracy: 0.8367 - auc_roc: 0.8680 - val_loss: 0.3779 - val_accuracy: 0.8190 - val_auc_roc: 0.8706\n",
      "Epoch 10/100\n",
      "4409/4409 [==============================] - 0s 56us/step - loss: 0.3557 - accuracy: 0.8433 - auc_roc: 0.8731 - val_loss: 0.3625 - val_accuracy: 0.8362 - val_auc_roc: 0.8751\n",
      "Epoch 11/100\n",
      "4409/4409 [==============================] - 0s 49us/step - loss: 0.3489 - accuracy: 0.8410 - auc_roc: 0.8772 - val_loss: 0.3693 - val_accuracy: 0.8147 - val_auc_roc: 0.8791\n",
      "Epoch 12/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.3396 - accuracy: 0.8442 - auc_roc: 0.8809 - val_loss: 0.3708 - val_accuracy: 0.8319 - val_auc_roc: 0.8827\n",
      "Epoch 13/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.3299 - accuracy: 0.8510 - auc_roc: 0.8842 - val_loss: 0.3743 - val_accuracy: 0.8190 - val_auc_roc: 0.8862\n",
      "Epoch 14/100\n",
      "4409/4409 [==============================] - 0s 56us/step - loss: 0.3290 - accuracy: 0.8519 - auc_roc: 0.8876 - val_loss: 0.3514 - val_accuracy: 0.8319 - val_auc_roc: 0.8891\n",
      "Epoch 15/100\n",
      "4409/4409 [==============================] - 0s 62us/step - loss: 0.3216 - accuracy: 0.8569 - auc_roc: 0.8905 - val_loss: 0.3588 - val_accuracy: 0.8405 - val_auc_roc: 0.8919\n",
      "Epoch 16/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.3124 - accuracy: 0.8592 - auc_roc: 0.8933 - val_loss: 0.3504 - val_accuracy: 0.8405 - val_auc_roc: 0.8947\n",
      "Epoch 17/100\n",
      "4409/4409 [==============================] - 0s 55us/step - loss: 0.2955 - accuracy: 0.8723 - auc_roc: 0.8961 - val_loss: 0.3448 - val_accuracy: 0.8448 - val_auc_roc: 0.8977\n",
      "Epoch 18/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.2986 - accuracy: 0.8660 - auc_roc: 0.8989 - val_loss: 0.3530 - val_accuracy: 0.8362 - val_auc_roc: 0.9001\n",
      "Epoch 19/100\n",
      "4409/4409 [==============================] - 0s 55us/step - loss: 0.2897 - accuracy: 0.8739 - auc_roc: 0.9013 - val_loss: 0.3668 - val_accuracy: 0.8276 - val_auc_roc: 0.9026\n",
      "Epoch 20/100\n",
      "4409/4409 [==============================] - 0s 50us/step - loss: 0.2854 - accuracy: 0.8762 - auc_roc: 0.9038 - val_loss: 0.3605 - val_accuracy: 0.8448 - val_auc_roc: 0.9049\n",
      "Epoch 21/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.2810 - accuracy: 0.8773 - auc_roc: 0.9060 - val_loss: 0.3508 - val_accuracy: 0.8147 - val_auc_roc: 0.9071\n",
      "Epoch 22/100\n",
      "4409/4409 [==============================] - 0s 50us/step - loss: 0.2733 - accuracy: 0.8830 - auc_roc: 0.9081 - val_loss: 0.3406 - val_accuracy: 0.8534 - val_auc_roc: 0.9092\n",
      "Epoch 23/100\n",
      "4409/4409 [==============================] - 0s 56us/step - loss: 0.2734 - accuracy: 0.8766 - auc_roc: 0.9101 - val_loss: 0.3784 - val_accuracy: 0.8147 - val_auc_roc: 0.9110\n",
      "Epoch 24/100\n",
      "4409/4409 [==============================] - 0s 55us/step - loss: 0.2616 - accuracy: 0.8839 - auc_roc: 0.9119 - val_loss: 0.3462 - val_accuracy: 0.8362 - val_auc_roc: 0.9129\n",
      "Epoch 25/100\n",
      "4409/4409 [==============================] - 0s 49us/step - loss: 0.2596 - accuracy: 0.8870 - auc_roc: 0.9139 - val_loss: 0.3318 - val_accuracy: 0.8534 - val_auc_roc: 0.9147\n",
      "Epoch 26/100\n",
      "4409/4409 [==============================] - 0s 58us/step - loss: 0.2520 - accuracy: 0.8875 - auc_roc: 0.9157 - val_loss: 0.3546 - val_accuracy: 0.8448 - val_auc_roc: 0.9166\n",
      "Epoch 27/100\n",
      "4409/4409 [==============================] - 0s 59us/step - loss: 0.2474 - accuracy: 0.8916 - auc_roc: 0.9173 - val_loss: 0.3482 - val_accuracy: 0.8276 - val_auc_roc: 0.9183\n",
      "Epoch 28/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.2437 - accuracy: 0.8929 - auc_roc: 0.9190 - val_loss: 0.3657 - val_accuracy: 0.8578 - val_auc_roc: 0.9199\n",
      "Epoch 29/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.2505 - accuracy: 0.8943 - auc_roc: 0.9206 - val_loss: 0.3624 - val_accuracy: 0.8448 - val_auc_roc: 0.9213\n",
      "Epoch 30/100\n",
      "4409/4409 [==============================] - 0s 48us/step - loss: 0.2444 - accuracy: 0.8943 - auc_roc: 0.9220 - val_loss: 0.3362 - val_accuracy: 0.8405 - val_auc_roc: 0.9227\n",
      "Epoch 31/100\n",
      "4409/4409 [==============================] - 0s 48us/step - loss: 0.2476 - accuracy: 0.8929 - auc_roc: 0.9233 - val_loss: 0.3604 - val_accuracy: 0.8448 - val_auc_roc: 0.9239\n",
      "Epoch 32/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.2503 - accuracy: 0.8914 - auc_roc: 0.9245 - val_loss: 0.3292 - val_accuracy: 0.8534 - val_auc_roc: 0.9250\n",
      "Epoch 33/100\n",
      "4409/4409 [==============================] - 0s 58us/step - loss: 0.2388 - accuracy: 0.8916 - auc_roc: 0.9256 - val_loss: 0.3221 - val_accuracy: 0.8664 - val_auc_roc: 0.9262\n",
      "Epoch 34/100\n",
      "4409/4409 [==============================] - 0s 50us/step - loss: 0.2210 - accuracy: 0.9022 - auc_roc: 0.9269 - val_loss: 0.3515 - val_accuracy: 0.8621 - val_auc_roc: 0.9276\n",
      "Epoch 35/100\n",
      "4409/4409 [==============================] - 0s 48us/step - loss: 0.2204 - accuracy: 0.9020 - auc_roc: 0.9282 - val_loss: 0.3659 - val_accuracy: 0.8534 - val_auc_roc: 0.9288\n",
      "Epoch 36/100\n",
      "4409/4409 [==============================] - 0s 49us/step - loss: 0.2142 - accuracy: 0.9068 - auc_roc: 0.9294 - val_loss: 0.3330 - val_accuracy: 0.8664 - val_auc_roc: 0.9301\n",
      "Epoch 37/100\n",
      "4409/4409 [==============================] - 0s 56us/step - loss: 0.2099 - accuracy: 0.9113 - auc_roc: 0.9307 - val_loss: 0.3469 - val_accuracy: 0.8319 - val_auc_roc: 0.9313\n",
      "Epoch 38/100\n",
      "4409/4409 [==============================] - 0s 48us/step - loss: 0.1992 - accuracy: 0.9134 - auc_roc: 0.9320 - val_loss: 0.3579 - val_accuracy: 0.8491 - val_auc_roc: 0.9326\n",
      "Epoch 39/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.2050 - accuracy: 0.9106 - auc_roc: 0.9332 - val_loss: 0.3741 - val_accuracy: 0.8578 - val_auc_roc: 0.9337\n",
      "Epoch 40/100\n",
      "4409/4409 [==============================] - 0s 55us/step - loss: 0.2050 - accuracy: 0.9090 - auc_roc: 0.9343 - val_loss: 0.3727 - val_accuracy: 0.8707 - val_auc_roc: 0.9348\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 41/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.1771 - accuracy: 0.9263 - auc_roc: 0.9354 - val_loss: 0.3596 - val_accuracy: 0.8664 - val_auc_roc: 0.9361\n",
      "Epoch 42/100\n",
      "4409/4409 [==============================] - 0s 50us/step - loss: 0.1686 - accuracy: 0.9274 - auc_roc: 0.9367 - val_loss: 0.3553 - val_accuracy: 0.8793 - val_auc_roc: 0.9374\n",
      "Epoch 43/100\n",
      "4409/4409 [==============================] - 0s 49us/step - loss: 0.1548 - accuracy: 0.9401 - auc_roc: 0.9380 - val_loss: 0.3594 - val_accuracy: 0.8750 - val_auc_roc: 0.9388\n",
      "Epoch 44/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.1647 - accuracy: 0.9335 - auc_roc: 0.9394 - val_loss: 0.3563 - val_accuracy: 0.8750 - val_auc_roc: 0.9400\n",
      "Epoch 45/100\n",
      "4409/4409 [==============================] - 0s 48us/step - loss: 0.1551 - accuracy: 0.9394 - auc_roc: 0.9406 - val_loss: 0.3627 - val_accuracy: 0.8707 - val_auc_roc: 0.9412\n",
      "Epoch 46/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.1513 - accuracy: 0.9381 - auc_roc: 0.9418 - val_loss: 0.3592 - val_accuracy: 0.8664 - val_auc_roc: 0.9424\n",
      "Epoch 47/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.1484 - accuracy: 0.9390 - auc_roc: 0.9430 - val_loss: 0.3570 - val_accuracy: 0.8707 - val_auc_roc: 0.9435\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 00047: early stopping\n",
      "\n",
      "Fold  10\n",
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_143 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_95 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_144 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_96 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_145 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4409 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "4409/4409 [==============================] - 2s 455us/step - loss: 0.5646 - accuracy: 0.6999 - auc_roc: 0.6247 - val_loss: 0.4282 - val_accuracy: 0.8103 - val_auc_roc: 0.7318\n",
      "Epoch 2/100\n",
      "4409/4409 [==============================] - 0s 55us/step - loss: 0.4575 - accuracy: 0.7798 - auc_roc: 0.7719 - val_loss: 0.4029 - val_accuracy: 0.8190 - val_auc_roc: 0.7972\n",
      "Epoch 3/100\n",
      "4409/4409 [==============================] - 0s 60us/step - loss: 0.4316 - accuracy: 0.7961 - auc_roc: 0.8119 - val_loss: 0.3870 - val_accuracy: 0.8233 - val_auc_roc: 0.8227\n",
      "Epoch 4/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.4216 - accuracy: 0.8009 - auc_roc: 0.8302 - val_loss: 0.3910 - val_accuracy: 0.7888 - val_auc_roc: 0.8365\n",
      "Epoch 5/100\n",
      "4409/4409 [==============================] - 0s 55us/step - loss: 0.4037 - accuracy: 0.8129 - auc_roc: 0.8424 - val_loss: 0.3888 - val_accuracy: 0.7931 - val_auc_roc: 0.8471\n",
      "Epoch 6/100\n",
      "4409/4409 [==============================] - 0s 56us/step - loss: 0.4012 - accuracy: 0.8176 - auc_roc: 0.8512 - val_loss: 0.3626 - val_accuracy: 0.8147 - val_auc_roc: 0.8542\n",
      "Epoch 7/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.3778 - accuracy: 0.8324 - auc_roc: 0.8581 - val_loss: 0.3633 - val_accuracy: 0.8276 - val_auc_roc: 0.8613\n",
      "Epoch 8/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.3652 - accuracy: 0.8378 - auc_roc: 0.8643 - val_loss: 0.3503 - val_accuracy: 0.8448 - val_auc_roc: 0.8676\n",
      "Epoch 9/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.3680 - accuracy: 0.8376 - auc_roc: 0.8701 - val_loss: 0.3635 - val_accuracy: 0.8017 - val_auc_roc: 0.8721\n",
      "Epoch 10/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.3528 - accuracy: 0.8444 - auc_roc: 0.8743 - val_loss: 0.3589 - val_accuracy: 0.8147 - val_auc_roc: 0.8765\n",
      "Epoch 11/100\n",
      "4409/4409 [==============================] - 0s 47us/step - loss: 0.3434 - accuracy: 0.8440 - auc_roc: 0.8785 - val_loss: 0.3479 - val_accuracy: 0.8276 - val_auc_roc: 0.8807\n",
      "Epoch 12/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.3361 - accuracy: 0.8467 - auc_roc: 0.8825 - val_loss: 0.3430 - val_accuracy: 0.8190 - val_auc_roc: 0.8843\n",
      "Epoch 13/100\n",
      "4409/4409 [==============================] - 0s 50us/step - loss: 0.3286 - accuracy: 0.8501 - auc_roc: 0.8862 - val_loss: 0.3501 - val_accuracy: 0.8491 - val_auc_roc: 0.8877\n",
      "Epoch 14/100\n",
      "4409/4409 [==============================] - 0s 57us/step - loss: 0.3361 - accuracy: 0.8537 - auc_roc: 0.8891 - val_loss: 0.3444 - val_accuracy: 0.8319 - val_auc_roc: 0.8904\n",
      "Epoch 15/100\n",
      "4409/4409 [==============================] - 0s 63us/step - loss: 0.3150 - accuracy: 0.8646 - auc_roc: 0.8919 - val_loss: 0.3584 - val_accuracy: 0.8147 - val_auc_roc: 0.8934\n",
      "Epoch 16/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.3163 - accuracy: 0.8601 - auc_roc: 0.8947 - val_loss: 0.3349 - val_accuracy: 0.8190 - val_auc_roc: 0.8960\n",
      "Epoch 17/100\n",
      "4409/4409 [==============================] - 0s 61us/step - loss: 0.3076 - accuracy: 0.8655 - auc_roc: 0.8972 - val_loss: 0.3473 - val_accuracy: 0.8233 - val_auc_roc: 0.8985\n",
      "Epoch 18/100\n",
      "4409/4409 [==============================] - 0s 55us/step - loss: 0.2963 - accuracy: 0.8687 - auc_roc: 0.8999 - val_loss: 0.3443 - val_accuracy: 0.8534 - val_auc_roc: 0.9010\n",
      "Epoch 19/100\n",
      "4409/4409 [==============================] - 0s 59us/step - loss: 0.2977 - accuracy: 0.8709 - auc_roc: 0.9021 - val_loss: 0.3442 - val_accuracy: 0.8319 - val_auc_roc: 0.9031\n",
      "Epoch 20/100\n",
      "4409/4409 [==============================] - 0s 55us/step - loss: 0.2836 - accuracy: 0.8753 - auc_roc: 0.9044 - val_loss: 0.3434 - val_accuracy: 0.8319 - val_auc_roc: 0.9054\n",
      "Epoch 21/100\n",
      "4409/4409 [==============================] - 0s 57us/step - loss: 0.2831 - accuracy: 0.8798 - auc_roc: 0.9065 - val_loss: 0.3591 - val_accuracy: 0.8103 - val_auc_roc: 0.9075\n",
      "Epoch 22/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.2764 - accuracy: 0.8793 - auc_roc: 0.9085 - val_loss: 0.3667 - val_accuracy: 0.8190 - val_auc_roc: 0.9095\n",
      "Epoch 23/100\n",
      "4409/4409 [==============================] - 0s 49us/step - loss: 0.2659 - accuracy: 0.8818 - auc_roc: 0.9105 - val_loss: 0.3504 - val_accuracy: 0.8534 - val_auc_roc: 0.9116\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 24/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.2567 - accuracy: 0.8918 - auc_roc: 0.9126 - val_loss: 0.3356 - val_accuracy: 0.8448 - val_auc_roc: 0.9136\n",
      "Epoch 25/100\n",
      "4409/4409 [==============================] - 0s 55us/step - loss: 0.2427 - accuracy: 0.8988 - auc_roc: 0.9147 - val_loss: 0.3315 - val_accuracy: 0.8578 - val_auc_roc: 0.9158\n",
      "Epoch 26/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.2398 - accuracy: 0.8993 - auc_roc: 0.9168 - val_loss: 0.3428 - val_accuracy: 0.8362 - val_auc_roc: 0.9178\n",
      "Epoch 27/100\n",
      "4409/4409 [==============================] - 0s 49us/step - loss: 0.2332 - accuracy: 0.9063 - auc_roc: 0.9188 - val_loss: 0.3365 - val_accuracy: 0.8491 - val_auc_roc: 0.9198\n",
      "Epoch 28/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.2310 - accuracy: 0.9022 - auc_roc: 0.9207 - val_loss: 0.3399 - val_accuracy: 0.8578 - val_auc_roc: 0.9216\n",
      "Epoch 29/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.2229 - accuracy: 0.9079 - auc_roc: 0.9226 - val_loss: 0.3418 - val_accuracy: 0.8448 - val_auc_roc: 0.9235\n",
      "Epoch 30/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.2230 - accuracy: 0.9056 - auc_roc: 0.9243 - val_loss: 0.3456 - val_accuracy: 0.8362 - val_auc_roc: 0.9251\n",
      "Epoch 31/100\n",
      "4409/4409 [==============================] - 0s 55us/step - loss: 0.2171 - accuracy: 0.9097 - auc_roc: 0.9260 - val_loss: 0.3446 - val_accuracy: 0.8405 - val_auc_roc: 0.9267\n",
      "Epoch 32/100\n",
      "4409/4409 [==============================] - 0s 50us/step - loss: 0.2234 - accuracy: 0.9075 - auc_roc: 0.9275 - val_loss: 0.3439 - val_accuracy: 0.8405 - val_auc_roc: 0.9282\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 33/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.2113 - accuracy: 0.9104 - auc_roc: 0.9289 - val_loss: 0.3422 - val_accuracy: 0.8448 - val_auc_roc: 0.9297\n",
      "Epoch 34/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.2208 - accuracy: 0.9093 - auc_roc: 0.9303 - val_loss: 0.3414 - val_accuracy: 0.8448 - val_auc_roc: 0.9309\n",
      "Epoch 35/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.2183 - accuracy: 0.9077 - auc_roc: 0.9316 - val_loss: 0.3408 - val_accuracy: 0.8448 - val_auc_roc: 0.9321\n",
      "Epoch 36/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.2158 - accuracy: 0.9115 - auc_roc: 0.9327 - val_loss: 0.3402 - val_accuracy: 0.8405 - val_auc_roc: 0.9333\n",
      "Epoch 37/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.2094 - accuracy: 0.9172 - auc_roc: 0.9339 - val_loss: 0.3401 - val_accuracy: 0.8405 - val_auc_roc: 0.9345\n",
      "Epoch 38/100\n",
      "4409/4409 [==============================] - 0s 48us/step - loss: 0.2168 - accuracy: 0.9109 - auc_roc: 0.9350 - val_loss: 0.3407 - val_accuracy: 0.8448 - val_auc_roc: 0.9355\n",
      "Epoch 39/100\n",
      "4409/4409 [==============================] - 0s 49us/step - loss: 0.2085 - accuracy: 0.9186 - auc_roc: 0.9360 - val_loss: 0.3408 - val_accuracy: 0.8448 - val_auc_roc: 0.9366\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 00039: early stopping\n",
      "\n",
      "Fold  11\n",
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_146 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_97 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_147 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_98 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_148 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4409 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "4409/4409 [==============================] - 2s 445us/step - loss: 0.5725 - accuracy: 0.6949 - auc_roc: 0.5866 - val_loss: 0.4257 - val_accuracy: 0.8190 - val_auc_roc: 0.7193\n",
      "Epoch 2/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.4719 - accuracy: 0.7709 - auc_roc: 0.7596 - val_loss: 0.3956 - val_accuracy: 0.8319 - val_auc_roc: 0.7864\n",
      "Epoch 3/100\n",
      "4409/4409 [==============================] - 0s 49us/step - loss: 0.4342 - accuracy: 0.7857 - auc_roc: 0.8035 - val_loss: 0.3661 - val_accuracy: 0.8664 - val_auc_roc: 0.8155\n",
      "Epoch 4/100\n",
      "4409/4409 [==============================] - 0s 55us/step - loss: 0.4220 - accuracy: 0.8081 - auc_roc: 0.8250 - val_loss: 0.3648 - val_accuracy: 0.8448 - val_auc_roc: 0.8320\n",
      "Epoch 5/100\n",
      "4409/4409 [==============================] - 0s 55us/step - loss: 0.4112 - accuracy: 0.8111 - auc_roc: 0.8376 - val_loss: 0.3527 - val_accuracy: 0.8448 - val_auc_roc: 0.8427\n",
      "Epoch 6/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.3815 - accuracy: 0.8249 - auc_roc: 0.8484 - val_loss: 0.3417 - val_accuracy: 0.8578 - val_auc_roc: 0.8531\n",
      "Epoch 7/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.3829 - accuracy: 0.8217 - auc_roc: 0.8567 - val_loss: 0.3333 - val_accuracy: 0.8534 - val_auc_roc: 0.8599\n",
      "Epoch 8/100\n",
      "4409/4409 [==============================] - 0s 50us/step - loss: 0.3679 - accuracy: 0.8362 - auc_roc: 0.8636 - val_loss: 0.3375 - val_accuracy: 0.8621 - val_auc_roc: 0.8661\n",
      "Epoch 9/100\n",
      "4409/4409 [==============================] - 0s 50us/step - loss: 0.3632 - accuracy: 0.8419 - auc_roc: 0.8690 - val_loss: 0.3325 - val_accuracy: 0.8448 - val_auc_roc: 0.8714\n",
      "Epoch 10/100\n",
      "4409/4409 [==============================] - 0s 55us/step - loss: 0.3609 - accuracy: 0.8392 - auc_roc: 0.8736 - val_loss: 0.3295 - val_accuracy: 0.8448 - val_auc_roc: 0.8756\n",
      "Epoch 11/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.3488 - accuracy: 0.8424 - auc_roc: 0.8777 - val_loss: 0.3313 - val_accuracy: 0.8836 - val_auc_roc: 0.8796\n",
      "Epoch 12/100\n",
      "4409/4409 [==============================] - 0s 55us/step - loss: 0.3456 - accuracy: 0.8469 - auc_roc: 0.8816 - val_loss: 0.3152 - val_accuracy: 0.8578 - val_auc_roc: 0.8831\n",
      "Epoch 13/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.3271 - accuracy: 0.8530 - auc_roc: 0.8853 - val_loss: 0.3191 - val_accuracy: 0.8362 - val_auc_roc: 0.8868\n",
      "Epoch 14/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.3226 - accuracy: 0.8601 - auc_roc: 0.8884 - val_loss: 0.3186 - val_accuracy: 0.8448 - val_auc_roc: 0.8902\n",
      "Epoch 15/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.3163 - accuracy: 0.8641 - auc_roc: 0.8918 - val_loss: 0.3216 - val_accuracy: 0.8578 - val_auc_roc: 0.8933\n",
      "Epoch 16/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.3036 - accuracy: 0.8753 - auc_roc: 0.8950 - val_loss: 0.3148 - val_accuracy: 0.8578 - val_auc_roc: 0.8964\n",
      "Epoch 17/100\n",
      "4409/4409 [==============================] - 0s 56us/step - loss: 0.3028 - accuracy: 0.8632 - auc_roc: 0.8979 - val_loss: 0.3134 - val_accuracy: 0.8664 - val_auc_roc: 0.8990\n",
      "Epoch 18/100\n",
      "4409/4409 [==============================] - 0s 55us/step - loss: 0.2959 - accuracy: 0.8696 - auc_roc: 0.9003 - val_loss: 0.3256 - val_accuracy: 0.8405 - val_auc_roc: 0.9016\n",
      "Epoch 19/100\n",
      "4409/4409 [==============================] - 0s 62us/step - loss: 0.2923 - accuracy: 0.8732 - auc_roc: 0.9028 - val_loss: 0.3306 - val_accuracy: 0.8319 - val_auc_roc: 0.9039\n",
      "Epoch 20/100\n",
      "4409/4409 [==============================] - 0s 58us/step - loss: 0.2835 - accuracy: 0.8762 - auc_roc: 0.9050 - val_loss: 0.3326 - val_accuracy: 0.8534 - val_auc_roc: 0.9061\n",
      "Epoch 21/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.2719 - accuracy: 0.8809 - auc_roc: 0.9073 - val_loss: 0.3141 - val_accuracy: 0.8534 - val_auc_roc: 0.9084\n",
      "Epoch 22/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.2697 - accuracy: 0.8852 - auc_roc: 0.9095 - val_loss: 0.3524 - val_accuracy: 0.8405 - val_auc_roc: 0.9105\n",
      "Epoch 23/100\n",
      "4409/4409 [==============================] - 0s 50us/step - loss: 0.2761 - accuracy: 0.8791 - auc_roc: 0.9114 - val_loss: 0.3426 - val_accuracy: 0.8491 - val_auc_roc: 0.9123\n",
      "Epoch 24/100\n",
      "4409/4409 [==============================] - 0s 49us/step - loss: 0.2685 - accuracy: 0.8836 - auc_roc: 0.9132 - val_loss: 0.3773 - val_accuracy: 0.8362 - val_auc_roc: 0.9140\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 25/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.2482 - accuracy: 0.8943 - auc_roc: 0.9150 - val_loss: 0.3406 - val_accuracy: 0.8578 - val_auc_roc: 0.9160\n",
      "Epoch 26/100\n",
      "4409/4409 [==============================] - 0s 49us/step - loss: 0.2365 - accuracy: 0.8982 - auc_roc: 0.9170 - val_loss: 0.3329 - val_accuracy: 0.8534 - val_auc_roc: 0.9181\n",
      "Epoch 27/100\n",
      "4409/4409 [==============================] - 0s 48us/step - loss: 0.2308 - accuracy: 0.9061 - auc_roc: 0.9191 - val_loss: 0.3323 - val_accuracy: 0.8621 - val_auc_roc: 0.9201\n",
      "Epoch 28/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.2198 - accuracy: 0.9125 - auc_roc: 0.9211 - val_loss: 0.3343 - val_accuracy: 0.8578 - val_auc_roc: 0.9221\n",
      "Epoch 29/100\n",
      "4409/4409 [==============================] - 0s 55us/step - loss: 0.2186 - accuracy: 0.9143 - auc_roc: 0.9231 - val_loss: 0.3350 - val_accuracy: 0.8534 - val_auc_roc: 0.9240\n",
      "Epoch 30/100\n",
      "4409/4409 [==============================] - 0s 48us/step - loss: 0.2309 - accuracy: 0.9034 - auc_roc: 0.9249 - val_loss: 0.3397 - val_accuracy: 0.8534 - val_auc_roc: 0.9256\n",
      "Epoch 31/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.2225 - accuracy: 0.9127 - auc_roc: 0.9263 - val_loss: 0.3348 - val_accuracy: 0.8448 - val_auc_roc: 0.9272\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 00031: early stopping\n",
      "\n",
      "Fold  12\n",
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_149 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_99 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_150 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_100 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_151 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4409 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "4409/4409 [==============================] - 2s 459us/step - loss: 0.5485 - accuracy: 0.7165 - auc_roc: 0.6559 - val_loss: 0.4822 - val_accuracy: 0.7629 - val_auc_roc: 0.7521\n",
      "Epoch 2/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.4618 - accuracy: 0.7768 - auc_roc: 0.7839 - val_loss: 0.4775 - val_accuracy: 0.7759 - val_auc_roc: 0.8025\n",
      "Epoch 3/100\n",
      "4409/4409 [==============================] - 0s 55us/step - loss: 0.4367 - accuracy: 0.7961 - auc_roc: 0.8143 - val_loss: 0.4855 - val_accuracy: 0.7371 - val_auc_roc: 0.8240\n",
      "Epoch 4/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.4197 - accuracy: 0.8020 - auc_roc: 0.8303 - val_loss: 0.4652 - val_accuracy: 0.7802 - val_auc_roc: 0.8368\n",
      "Epoch 5/100\n",
      "4409/4409 [==============================] - 0s 56us/step - loss: 0.4034 - accuracy: 0.8104 - auc_roc: 0.8418 - val_loss: 0.4626 - val_accuracy: 0.7716 - val_auc_roc: 0.8462\n",
      "Epoch 6/100\n",
      "4409/4409 [==============================] - 0s 49us/step - loss: 0.3760 - accuracy: 0.8319 - auc_roc: 0.8513 - val_loss: 0.4606 - val_accuracy: 0.7759 - val_auc_roc: 0.8558\n",
      "Epoch 7/100\n",
      "4409/4409 [==============================] - 0s 49us/step - loss: 0.3726 - accuracy: 0.8288 - auc_roc: 0.8595 - val_loss: 0.4628 - val_accuracy: 0.7845 - val_auc_roc: 0.8625\n",
      "Epoch 8/100\n",
      "4409/4409 [==============================] - 0s 48us/step - loss: 0.3713 - accuracy: 0.8308 - auc_roc: 0.8652 - val_loss: 0.4481 - val_accuracy: 0.8060 - val_auc_roc: 0.8676\n",
      "Epoch 9/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.3567 - accuracy: 0.8367 - auc_roc: 0.8703 - val_loss: 0.4587 - val_accuracy: 0.8103 - val_auc_roc: 0.8726\n",
      "Epoch 10/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.3471 - accuracy: 0.8449 - auc_roc: 0.8747 - val_loss: 0.4731 - val_accuracy: 0.7888 - val_auc_roc: 0.8770\n",
      "Epoch 11/100\n",
      "4409/4409 [==============================] - 0s 59us/step - loss: 0.3423 - accuracy: 0.8433 - auc_roc: 0.8789 - val_loss: 0.4666 - val_accuracy: 0.8060 - val_auc_roc: 0.8808\n",
      "Epoch 12/100\n",
      "4409/4409 [==============================] - 0s 58us/step - loss: 0.3350 - accuracy: 0.8451 - auc_roc: 0.8826 - val_loss: 0.4662 - val_accuracy: 0.8103 - val_auc_roc: 0.8842\n",
      "Epoch 13/100\n",
      "4409/4409 [==============================] - 0s 61us/step - loss: 0.3297 - accuracy: 0.8505 - auc_roc: 0.8857 - val_loss: 0.4536 - val_accuracy: 0.8060 - val_auc_roc: 0.8874\n",
      "Epoch 14/100\n",
      "4409/4409 [==============================] - 0s 63us/step - loss: 0.3216 - accuracy: 0.8544 - auc_roc: 0.8889 - val_loss: 0.4597 - val_accuracy: 0.8060 - val_auc_roc: 0.8904\n",
      "Epoch 15/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.3093 - accuracy: 0.8607 - auc_roc: 0.8919 - val_loss: 0.4666 - val_accuracy: 0.8060 - val_auc_roc: 0.8934\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 16/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.2887 - accuracy: 0.8730 - auc_roc: 0.8950 - val_loss: 0.4615 - val_accuracy: 0.8190 - val_auc_roc: 0.8967\n",
      "Epoch 17/100\n",
      "4409/4409 [==============================] - 0s 50us/step - loss: 0.2832 - accuracy: 0.8784 - auc_roc: 0.8982 - val_loss: 0.4649 - val_accuracy: 0.8190 - val_auc_roc: 0.8998\n",
      "Epoch 18/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.2791 - accuracy: 0.8832 - auc_roc: 0.9012 - val_loss: 0.4647 - val_accuracy: 0.8190 - val_auc_roc: 0.9027\n",
      "Epoch 19/100\n",
      "4409/4409 [==============================] - 0s 60us/step - loss: 0.2757 - accuracy: 0.8832 - auc_roc: 0.9041 - val_loss: 0.4698 - val_accuracy: 0.8060 - val_auc_roc: 0.9053\n",
      "Epoch 20/100\n",
      "4409/4409 [==============================] - 0s 55us/step - loss: 0.2756 - accuracy: 0.8886 - auc_roc: 0.9065 - val_loss: 0.4704 - val_accuracy: 0.8190 - val_auc_roc: 0.9077\n",
      "Epoch 21/100\n",
      "4409/4409 [==============================] - 0s 58us/step - loss: 0.2690 - accuracy: 0.8846 - auc_roc: 0.9087 - val_loss: 0.4710 - val_accuracy: 0.8147 - val_auc_roc: 0.9099\n",
      "Epoch 22/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.2747 - accuracy: 0.8843 - auc_roc: 0.9108 - val_loss: 0.4695 - val_accuracy: 0.8190 - val_auc_roc: 0.9118\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 00022: early stopping\n",
      "\n",
      "Fold  13\n",
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_152 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_101 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_153 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_102 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_154 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_153 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4409 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "4409/4409 [==============================] - 2s 464us/step - loss: 0.5614 - accuracy: 0.7076 - auc_roc: 0.6107 - val_loss: 0.5218 - val_accuracy: 0.7112 - val_auc_roc: 0.7299\n",
      "Epoch 2/100\n",
      "4409/4409 [==============================] - 0s 55us/step - loss: 0.4652 - accuracy: 0.7829 - auc_roc: 0.7705 - val_loss: 0.5044 - val_accuracy: 0.7198 - val_auc_roc: 0.7926\n",
      "Epoch 3/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.4266 - accuracy: 0.8013 - auc_roc: 0.8076 - val_loss: 0.4869 - val_accuracy: 0.7543 - val_auc_roc: 0.8198\n",
      "Epoch 4/100\n",
      "4409/4409 [==============================] - 0s 55us/step - loss: 0.4134 - accuracy: 0.8081 - auc_roc: 0.8279 - val_loss: 0.4883 - val_accuracy: 0.7629 - val_auc_roc: 0.8352\n",
      "Epoch 5/100\n",
      "4409/4409 [==============================] - 0s 50us/step - loss: 0.4017 - accuracy: 0.8186 - auc_roc: 0.8405 - val_loss: 0.4789 - val_accuracy: 0.7457 - val_auc_roc: 0.8455\n",
      "Epoch 6/100\n",
      "4409/4409 [==============================] - 0s 56us/step - loss: 0.3891 - accuracy: 0.8263 - auc_roc: 0.8499 - val_loss: 0.4645 - val_accuracy: 0.7586 - val_auc_roc: 0.8536\n",
      "Epoch 7/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.3807 - accuracy: 0.8245 - auc_roc: 0.8570 - val_loss: 0.4638 - val_accuracy: 0.7672 - val_auc_roc: 0.8598\n",
      "Epoch 8/100\n",
      "4409/4409 [==============================] - 0s 58us/step - loss: 0.3668 - accuracy: 0.8308 - auc_roc: 0.8632 - val_loss: 0.4493 - val_accuracy: 0.7629 - val_auc_roc: 0.8655\n",
      "Epoch 9/100\n",
      "4409/4409 [==============================] - 0s 62us/step - loss: 0.3587 - accuracy: 0.8421 - auc_roc: 0.8683 - val_loss: 0.4602 - val_accuracy: 0.7759 - val_auc_roc: 0.8707\n",
      "Epoch 10/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.3566 - accuracy: 0.8433 - auc_roc: 0.8728 - val_loss: 0.4489 - val_accuracy: 0.7672 - val_auc_roc: 0.8749\n",
      "Epoch 11/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.3422 - accuracy: 0.8492 - auc_roc: 0.8771 - val_loss: 0.4362 - val_accuracy: 0.7759 - val_auc_roc: 0.8790\n",
      "Epoch 12/100\n",
      "4409/4409 [==============================] - 0s 58us/step - loss: 0.3402 - accuracy: 0.8453 - auc_roc: 0.8808 - val_loss: 0.4245 - val_accuracy: 0.8017 - val_auc_roc: 0.8825\n",
      "Epoch 13/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.3355 - accuracy: 0.8526 - auc_roc: 0.8842 - val_loss: 0.4321 - val_accuracy: 0.7931 - val_auc_roc: 0.8857\n",
      "Epoch 14/100\n",
      "4409/4409 [==============================] - 0s 49us/step - loss: 0.3331 - accuracy: 0.8535 - auc_roc: 0.8870 - val_loss: 0.4401 - val_accuracy: 0.7931 - val_auc_roc: 0.8884\n",
      "Epoch 15/100\n",
      "4409/4409 [==============================] - 0s 49us/step - loss: 0.3308 - accuracy: 0.8560 - auc_roc: 0.8896 - val_loss: 0.4282 - val_accuracy: 0.8060 - val_auc_roc: 0.8908\n",
      "Epoch 16/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.3045 - accuracy: 0.8619 - auc_roc: 0.8923 - val_loss: 0.4541 - val_accuracy: 0.7845 - val_auc_roc: 0.8938\n",
      "Epoch 17/100\n",
      "4409/4409 [==============================] - 0s 57us/step - loss: 0.3067 - accuracy: 0.8678 - auc_roc: 0.8951 - val_loss: 0.4354 - val_accuracy: 0.7802 - val_auc_roc: 0.8964\n",
      "Epoch 18/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.3070 - accuracy: 0.8687 - auc_roc: 0.8975 - val_loss: 0.4519 - val_accuracy: 0.7759 - val_auc_roc: 0.8986\n",
      "Epoch 19/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.2958 - accuracy: 0.8719 - auc_roc: 0.8996 - val_loss: 0.4315 - val_accuracy: 0.7931 - val_auc_roc: 0.9008\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 20/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.2714 - accuracy: 0.8846 - auc_roc: 0.9022 - val_loss: 0.4168 - val_accuracy: 0.8060 - val_auc_roc: 0.9035\n",
      "Epoch 21/100\n",
      "4409/4409 [==============================] - 0s 58us/step - loss: 0.2633 - accuracy: 0.8850 - auc_roc: 0.9047 - val_loss: 0.4242 - val_accuracy: 0.8017 - val_auc_roc: 0.9061\n",
      "Epoch 22/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.2629 - accuracy: 0.8884 - auc_roc: 0.9072 - val_loss: 0.4284 - val_accuracy: 0.8060 - val_auc_roc: 0.9085\n",
      "Epoch 23/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.2558 - accuracy: 0.8943 - auc_roc: 0.9096 - val_loss: 0.4299 - val_accuracy: 0.8103 - val_auc_roc: 0.9107\n",
      "Epoch 24/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.2543 - accuracy: 0.8895 - auc_roc: 0.9118 - val_loss: 0.4274 - val_accuracy: 0.8060 - val_auc_roc: 0.9128\n",
      "Epoch 25/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.2481 - accuracy: 0.8979 - auc_roc: 0.9138 - val_loss: 0.4337 - val_accuracy: 0.8103 - val_auc_roc: 0.9148\n",
      "Epoch 26/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.2530 - accuracy: 0.8970 - auc_roc: 0.9157 - val_loss: 0.4327 - val_accuracy: 0.8060 - val_auc_roc: 0.9166\n",
      "Epoch 27/100\n",
      "4409/4409 [==============================] - 0s 57us/step - loss: 0.2510 - accuracy: 0.8957 - auc_roc: 0.9174 - val_loss: 0.4335 - val_accuracy: 0.8017 - val_auc_roc: 0.9182\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 28/100\n",
      "4409/4409 [==============================] - 0s 58us/step - loss: 0.2478 - accuracy: 0.9000 - auc_roc: 0.9190 - val_loss: 0.4329 - val_accuracy: 0.8060 - val_auc_roc: 0.9198\n",
      "Epoch 29/100\n",
      "4409/4409 [==============================] - 0s 59us/step - loss: 0.2522 - accuracy: 0.9025 - auc_roc: 0.9205 - val_loss: 0.4325 - val_accuracy: 0.8060 - val_auc_roc: 0.9212\n",
      "Epoch 30/100\n",
      "4409/4409 [==============================] - 0s 60us/step - loss: 0.2462 - accuracy: 0.8979 - auc_roc: 0.9219 - val_loss: 0.4334 - val_accuracy: 0.8060 - val_auc_roc: 0.9225\n",
      "Epoch 31/100\n",
      "4409/4409 [==============================] - 0s 56us/step - loss: 0.2450 - accuracy: 0.9036 - auc_roc: 0.9232 - val_loss: 0.4327 - val_accuracy: 0.8060 - val_auc_roc: 0.9238\n",
      "Epoch 32/100\n",
      "4409/4409 [==============================] - 0s 49us/step - loss: 0.2462 - accuracy: 0.9027 - auc_roc: 0.9244 - val_loss: 0.4324 - val_accuracy: 0.8060 - val_auc_roc: 0.9250\n",
      "Epoch 33/100\n",
      "4409/4409 [==============================] - 0s 50us/step - loss: 0.2458 - accuracy: 0.9038 - auc_roc: 0.9255 - val_loss: 0.4326 - val_accuracy: 0.8103 - val_auc_roc: 0.9261\n",
      "Epoch 34/100\n",
      "4409/4409 [==============================] - 0s 48us/step - loss: 0.2445 - accuracy: 0.9070 - auc_roc: 0.9266 - val_loss: 0.4338 - val_accuracy: 0.8103 - val_auc_roc: 0.9272\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 00034: early stopping\n",
      "\n",
      "Fold  14\n",
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_155 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_154 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_103 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_156 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_155 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_104 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_157 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4409 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "4409/4409 [==============================] - 2s 470us/step - loss: 0.6356 - accuracy: 0.6525 - auc_roc: 0.5539 - val_loss: 0.4735 - val_accuracy: 0.7500 - val_auc_roc: 0.6424\n",
      "Epoch 2/100\n",
      "4409/4409 [==============================] - 0s 55us/step - loss: 0.4880 - accuracy: 0.7578 - auc_roc: 0.7010 - val_loss: 0.3981 - val_accuracy: 0.8319 - val_auc_roc: 0.7431\n",
      "Epoch 3/100\n",
      "4409/4409 [==============================] - 0s 58us/step - loss: 0.4571 - accuracy: 0.7886 - auc_roc: 0.7663 - val_loss: 0.3733 - val_accuracy: 0.8578 - val_auc_roc: 0.7840\n",
      "Epoch 4/100\n",
      "4409/4409 [==============================] - 0s 61us/step - loss: 0.4415 - accuracy: 0.7943 - auc_roc: 0.7963 - val_loss: 0.3935 - val_accuracy: 0.8319 - val_auc_roc: 0.8056\n",
      "Epoch 5/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.4193 - accuracy: 0.8063 - auc_roc: 0.8141 - val_loss: 0.3678 - val_accuracy: 0.8750 - val_auc_roc: 0.8214\n",
      "Epoch 6/100\n",
      "4409/4409 [==============================] - 0s 56us/step - loss: 0.4041 - accuracy: 0.8188 - auc_roc: 0.8269 - val_loss: 0.3513 - val_accuracy: 0.8233 - val_auc_roc: 0.8333\n",
      "Epoch 7/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.3884 - accuracy: 0.8285 - auc_roc: 0.8382 - val_loss: 0.3388 - val_accuracy: 0.8621 - val_auc_roc: 0.8430\n",
      "Epoch 8/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.3775 - accuracy: 0.8279 - auc_roc: 0.8474 - val_loss: 0.3208 - val_accuracy: 0.8707 - val_auc_roc: 0.8511\n",
      "Epoch 9/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.3685 - accuracy: 0.8347 - auc_roc: 0.8544 - val_loss: 0.3105 - val_accuracy: 0.8664 - val_auc_roc: 0.8580\n",
      "Epoch 10/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.3674 - accuracy: 0.8303 - auc_roc: 0.8608 - val_loss: 0.3191 - val_accuracy: 0.8491 - val_auc_roc: 0.8635\n",
      "Epoch 11/100\n",
      "4409/4409 [==============================] - 0s 50us/step - loss: 0.3645 - accuracy: 0.8347 - auc_roc: 0.8659 - val_loss: 0.3364 - val_accuracy: 0.8707 - val_auc_roc: 0.8678\n",
      "Epoch 12/100\n",
      "4409/4409 [==============================] - 0s 55us/step - loss: 0.3563 - accuracy: 0.8437 - auc_roc: 0.8700 - val_loss: 0.3082 - val_accuracy: 0.8621 - val_auc_roc: 0.8720\n",
      "Epoch 13/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.3431 - accuracy: 0.8440 - auc_roc: 0.8742 - val_loss: 0.3124 - val_accuracy: 0.8793 - val_auc_roc: 0.8761\n",
      "Epoch 14/100\n",
      "4409/4409 [==============================] - 0s 57us/step - loss: 0.3299 - accuracy: 0.8521 - auc_roc: 0.8781 - val_loss: 0.2997 - val_accuracy: 0.8707 - val_auc_roc: 0.8801\n",
      "Epoch 15/100\n",
      "4409/4409 [==============================] - 0s 50us/step - loss: 0.3274 - accuracy: 0.8517 - auc_roc: 0.8820 - val_loss: 0.3238 - val_accuracy: 0.8405 - val_auc_roc: 0.8835\n",
      "Epoch 16/100\n",
      "4409/4409 [==============================] - 0s 55us/step - loss: 0.3236 - accuracy: 0.8571 - auc_roc: 0.8852 - val_loss: 0.3140 - val_accuracy: 0.8836 - val_auc_roc: 0.8867\n",
      "Epoch 17/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.3256 - accuracy: 0.8539 - auc_roc: 0.8882 - val_loss: 0.3146 - val_accuracy: 0.8750 - val_auc_roc: 0.8894\n",
      "Epoch 18/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.3170 - accuracy: 0.8616 - auc_roc: 0.8907 - val_loss: 0.3347 - val_accuracy: 0.8664 - val_auc_roc: 0.8919\n",
      "Epoch 19/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.3034 - accuracy: 0.8673 - auc_roc: 0.8934 - val_loss: 0.3108 - val_accuracy: 0.8793 - val_auc_roc: 0.8946\n",
      "Epoch 20/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.3035 - accuracy: 0.8680 - auc_roc: 0.8958 - val_loss: 0.3020 - val_accuracy: 0.8750 - val_auc_roc: 0.8970\n",
      "Epoch 21/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.3030 - accuracy: 0.8669 - auc_roc: 0.8981 - val_loss: 0.3046 - val_accuracy: 0.8707 - val_auc_roc: 0.8991\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 22/100\n",
      "4409/4409 [==============================] - 0s 50us/step - loss: 0.2835 - accuracy: 0.8787 - auc_roc: 0.9003 - val_loss: 0.3016 - val_accuracy: 0.8750 - val_auc_roc: 0.9015\n",
      "Epoch 23/100\n",
      "4409/4409 [==============================] - 0s 49us/step - loss: 0.2722 - accuracy: 0.8882 - auc_roc: 0.9028 - val_loss: 0.3003 - val_accuracy: 0.8793 - val_auc_roc: 0.9040\n",
      "Epoch 24/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.2636 - accuracy: 0.8905 - auc_roc: 0.9053 - val_loss: 0.2957 - val_accuracy: 0.8707 - val_auc_roc: 0.9065\n",
      "Epoch 25/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.2608 - accuracy: 0.8941 - auc_roc: 0.9076 - val_loss: 0.2976 - val_accuracy: 0.8664 - val_auc_roc: 0.9087\n",
      "Epoch 26/100\n",
      "4409/4409 [==============================] - 0s 57us/step - loss: 0.2563 - accuracy: 0.8916 - auc_roc: 0.9098 - val_loss: 0.2964 - val_accuracy: 0.8879 - val_auc_roc: 0.9109\n",
      "Epoch 27/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.2538 - accuracy: 0.8939 - auc_roc: 0.9119 - val_loss: 0.2927 - val_accuracy: 0.8793 - val_auc_roc: 0.9129\n",
      "Epoch 28/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.2533 - accuracy: 0.8957 - auc_roc: 0.9138 - val_loss: 0.2979 - val_accuracy: 0.8879 - val_auc_roc: 0.9148\n",
      "Epoch 29/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.2531 - accuracy: 0.8961 - auc_roc: 0.9156 - val_loss: 0.2905 - val_accuracy: 0.8750 - val_auc_roc: 0.9165\n",
      "Epoch 30/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.2500 - accuracy: 0.9004 - auc_roc: 0.9174 - val_loss: 0.2959 - val_accuracy: 0.8879 - val_auc_roc: 0.9181\n",
      "Epoch 31/100\n",
      "4409/4409 [==============================] - 0s 57us/step - loss: 0.2430 - accuracy: 0.8982 - auc_roc: 0.9189 - val_loss: 0.2934 - val_accuracy: 0.8664 - val_auc_roc: 0.9197\n",
      "Epoch 32/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.2464 - accuracy: 0.9022 - auc_roc: 0.9205 - val_loss: 0.2954 - val_accuracy: 0.8793 - val_auc_roc: 0.9212\n",
      "Epoch 33/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.2410 - accuracy: 0.9034 - auc_roc: 0.9219 - val_loss: 0.2939 - val_accuracy: 0.8922 - val_auc_roc: 0.9226\n",
      "Epoch 34/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.2480 - accuracy: 0.8986 - auc_roc: 0.9232 - val_loss: 0.2912 - val_accuracy: 0.8793 - val_auc_roc: 0.9239\n",
      "Epoch 35/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.2469 - accuracy: 0.9002 - auc_roc: 0.9244 - val_loss: 0.2946 - val_accuracy: 0.8836 - val_auc_roc: 0.9250\n",
      "Epoch 36/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.2425 - accuracy: 0.9045 - auc_roc: 0.9256 - val_loss: 0.2951 - val_accuracy: 0.8836 - val_auc_roc: 0.9262\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 37/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.2438 - accuracy: 0.9025 - auc_roc: 0.9267 - val_loss: 0.2938 - val_accuracy: 0.8836 - val_auc_roc: 0.9273\n",
      "Epoch 38/100\n",
      "4409/4409 [==============================] - 0s 60us/step - loss: 0.2358 - accuracy: 0.9056 - auc_roc: 0.9278 - val_loss: 0.2925 - val_accuracy: 0.8836 - val_auc_roc: 0.9284\n",
      "Epoch 39/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.2369 - accuracy: 0.9079 - auc_roc: 0.9289 - val_loss: 0.2921 - val_accuracy: 0.8836 - val_auc_roc: 0.9294\n",
      "Epoch 40/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.2327 - accuracy: 0.9084 - auc_roc: 0.9299 - val_loss: 0.2910 - val_accuracy: 0.8836 - val_auc_roc: 0.9304\n",
      "Epoch 41/100\n",
      "4409/4409 [==============================] - 0s 50us/step - loss: 0.2360 - accuracy: 0.9056 - auc_roc: 0.9309 - val_loss: 0.2906 - val_accuracy: 0.8793 - val_auc_roc: 0.9314\n",
      "Epoch 42/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.2302 - accuracy: 0.9106 - auc_roc: 0.9318 - val_loss: 0.2901 - val_accuracy: 0.8793 - val_auc_roc: 0.9323\n",
      "Epoch 43/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.2373 - accuracy: 0.9066 - auc_roc: 0.9327 - val_loss: 0.2906 - val_accuracy: 0.8836 - val_auc_roc: 0.9331\n",
      "Epoch 44/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.2303 - accuracy: 0.9084 - auc_roc: 0.9336 - val_loss: 0.2903 - val_accuracy: 0.8836 - val_auc_roc: 0.9339\n",
      "Epoch 45/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.2336 - accuracy: 0.9052 - auc_roc: 0.9343 - val_loss: 0.2896 - val_accuracy: 0.8836 - val_auc_roc: 0.9347\n",
      "Epoch 46/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.2305 - accuracy: 0.9095 - auc_roc: 0.9351 - val_loss: 0.2902 - val_accuracy: 0.8836 - val_auc_roc: 0.9354\n",
      "Epoch 47/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.2368 - accuracy: 0.9061 - auc_roc: 0.9358 - val_loss: 0.2907 - val_accuracy: 0.8836 - val_auc_roc: 0.9361\n",
      "Epoch 48/100\n",
      "4409/4409 [==============================] - 0s 50us/step - loss: 0.2363 - accuracy: 0.9061 - auc_roc: 0.9364 - val_loss: 0.2904 - val_accuracy: 0.8836 - val_auc_roc: 0.9367\n",
      "Epoch 49/100\n",
      "4409/4409 [==============================] - 0s 50us/step - loss: 0.2328 - accuracy: 0.9079 - auc_roc: 0.9370 - val_loss: 0.2902 - val_accuracy: 0.8836 - val_auc_roc: 0.9374\n",
      "Epoch 50/100\n",
      "4409/4409 [==============================] - 0s 58us/step - loss: 0.2376 - accuracy: 0.9059 - auc_roc: 0.9377 - val_loss: 0.2905 - val_accuracy: 0.8836 - val_auc_roc: 0.9379\n",
      "Epoch 51/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.2308 - accuracy: 0.9120 - auc_roc: 0.9382 - val_loss: 0.2905 - val_accuracy: 0.8836 - val_auc_roc: 0.9385\n",
      "Epoch 52/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.2298 - accuracy: 0.9079 - auc_roc: 0.9388 - val_loss: 0.2899 - val_accuracy: 0.8836 - val_auc_roc: 0.9391\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 53/100\n",
      "4409/4409 [==============================] - 0s 49us/step - loss: 0.2266 - accuracy: 0.9115 - auc_roc: 0.9394 - val_loss: 0.2898 - val_accuracy: 0.8836 - val_auc_roc: 0.9397\n",
      "Epoch 54/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.2264 - accuracy: 0.9068 - auc_roc: 0.9400 - val_loss: 0.2898 - val_accuracy: 0.8836 - val_auc_roc: 0.9402\n",
      "Epoch 55/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.2309 - accuracy: 0.9097 - auc_roc: 0.9405 - val_loss: 0.2898 - val_accuracy: 0.8836 - val_auc_roc: 0.9407\n",
      "Epoch 56/100\n",
      "4409/4409 [==============================] - 0s 49us/step - loss: 0.2334 - accuracy: 0.9059 - auc_roc: 0.9410 - val_loss: 0.2898 - val_accuracy: 0.8836 - val_auc_roc: 0.9412\n",
      "Epoch 57/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.2320 - accuracy: 0.9061 - auc_roc: 0.9414 - val_loss: 0.2898 - val_accuracy: 0.8836 - val_auc_roc: 0.9417\n",
      "Epoch 58/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.2405 - accuracy: 0.9063 - auc_roc: 0.9419 - val_loss: 0.2899 - val_accuracy: 0.8836 - val_auc_roc: 0.9420\n",
      "Epoch 59/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.2345 - accuracy: 0.9034 - auc_roc: 0.9422 - val_loss: 0.2898 - val_accuracy: 0.8836 - val_auc_roc: 0.9424\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 00059: early stopping\n",
      "\n",
      "Fold  15\n",
      "Model: \"sequential_54\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_158 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_157 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_105 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_159 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_158 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_106 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_160 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_159 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4409 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "4409/4409 [==============================] - 2s 477us/step - loss: 0.5457 - accuracy: 0.7158 - auc_roc: 0.6465 - val_loss: 0.4466 - val_accuracy: 0.7845 - val_auc_roc: 0.7596\n",
      "Epoch 2/100\n",
      "4409/4409 [==============================] - 0s 58us/step - loss: 0.4642 - accuracy: 0.7707 - auc_roc: 0.7852 - val_loss: 0.4293 - val_accuracy: 0.7845 - val_auc_roc: 0.8049\n",
      "Epoch 3/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.4345 - accuracy: 0.7954 - auc_roc: 0.8154 - val_loss: 0.4099 - val_accuracy: 0.7974 - val_auc_roc: 0.8263\n",
      "Epoch 4/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.4202 - accuracy: 0.7997 - auc_roc: 0.8324 - val_loss: 0.4038 - val_accuracy: 0.7845 - val_auc_roc: 0.8389\n",
      "Epoch 5/100\n",
      "4409/4409 [==============================] - 0s 55us/step - loss: 0.4000 - accuracy: 0.8145 - auc_roc: 0.8439 - val_loss: 0.3894 - val_accuracy: 0.7974 - val_auc_roc: 0.8491\n",
      "Epoch 6/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.3924 - accuracy: 0.8186 - auc_roc: 0.8533 - val_loss: 0.3790 - val_accuracy: 0.8362 - val_auc_roc: 0.8565\n",
      "Epoch 7/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.3808 - accuracy: 0.8235 - auc_roc: 0.8597 - val_loss: 0.3916 - val_accuracy: 0.8147 - val_auc_roc: 0.8628\n",
      "Epoch 8/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.3718 - accuracy: 0.8299 - auc_roc: 0.8657 - val_loss: 0.3722 - val_accuracy: 0.8190 - val_auc_roc: 0.8681\n",
      "Epoch 9/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.3542 - accuracy: 0.8406 - auc_roc: 0.8707 - val_loss: 0.3693 - val_accuracy: 0.8319 - val_auc_roc: 0.8733\n",
      "Epoch 10/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.3618 - accuracy: 0.8369 - auc_roc: 0.8752 - val_loss: 0.3622 - val_accuracy: 0.8405 - val_auc_roc: 0.8770\n",
      "Epoch 11/100\n",
      "4409/4409 [==============================] - 0s 56us/step - loss: 0.3444 - accuracy: 0.8412 - auc_roc: 0.8790 - val_loss: 0.3649 - val_accuracy: 0.8233 - val_auc_roc: 0.8810\n",
      "Epoch 12/100\n",
      "4409/4409 [==============================] - 0s 56us/step - loss: 0.3349 - accuracy: 0.8437 - auc_roc: 0.8830 - val_loss: 0.3835 - val_accuracy: 0.8103 - val_auc_roc: 0.8846\n",
      "Epoch 13/100\n",
      "4409/4409 [==============================] - 0s 57us/step - loss: 0.3440 - accuracy: 0.8444 - auc_roc: 0.8857 - val_loss: 0.3627 - val_accuracy: 0.8190 - val_auc_roc: 0.8873\n",
      "Epoch 14/100\n",
      "4409/4409 [==============================] - 0s 59us/step - loss: 0.3234 - accuracy: 0.8528 - auc_roc: 0.8887 - val_loss: 0.3672 - val_accuracy: 0.8190 - val_auc_roc: 0.8903\n",
      "Epoch 15/100\n",
      "4409/4409 [==============================] - 0s 57us/step - loss: 0.3205 - accuracy: 0.8537 - auc_roc: 0.8916 - val_loss: 0.3796 - val_accuracy: 0.8319 - val_auc_roc: 0.8930\n",
      "Epoch 16/100\n",
      "4409/4409 [==============================] - 0s 59us/step - loss: 0.3111 - accuracy: 0.8671 - auc_roc: 0.8943 - val_loss: 0.3656 - val_accuracy: 0.8448 - val_auc_roc: 0.8958\n",
      "Epoch 17/100\n",
      "4409/4409 [==============================] - 0s 56us/step - loss: 0.2991 - accuracy: 0.8705 - auc_roc: 0.8971 - val_loss: 0.3447 - val_accuracy: 0.8405 - val_auc_roc: 0.8985\n",
      "Epoch 18/100\n",
      "4409/4409 [==============================] - 0s 48us/step - loss: 0.2984 - accuracy: 0.8707 - auc_roc: 0.8998 - val_loss: 0.3687 - val_accuracy: 0.8060 - val_auc_roc: 0.9010\n",
      "Epoch 19/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.2948 - accuracy: 0.8734 - auc_roc: 0.9021 - val_loss: 0.3642 - val_accuracy: 0.8362 - val_auc_roc: 0.9033\n",
      "Epoch 20/100\n",
      "4409/4409 [==============================] - 0s 62us/step - loss: 0.2864 - accuracy: 0.8709 - auc_roc: 0.9044 - val_loss: 0.3388 - val_accuracy: 0.8319 - val_auc_roc: 0.9056\n",
      "Epoch 21/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.2785 - accuracy: 0.8768 - auc_roc: 0.9066 - val_loss: 0.3637 - val_accuracy: 0.8190 - val_auc_roc: 0.9077\n",
      "Epoch 22/100\n",
      "4409/4409 [==============================] - 0s 57us/step - loss: 0.2742 - accuracy: 0.8796 - auc_roc: 0.9087 - val_loss: 0.3454 - val_accuracy: 0.8405 - val_auc_roc: 0.9098\n",
      "Epoch 23/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.2739 - accuracy: 0.8805 - auc_roc: 0.9107 - val_loss: 0.3422 - val_accuracy: 0.8319 - val_auc_roc: 0.9116\n",
      "Epoch 24/100\n",
      "4409/4409 [==============================] - 0s 53us/step - loss: 0.2597 - accuracy: 0.8870 - auc_roc: 0.9127 - val_loss: 0.3516 - val_accuracy: 0.8491 - val_auc_roc: 0.9136\n",
      "Epoch 25/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.2675 - accuracy: 0.8809 - auc_roc: 0.9144 - val_loss: 0.3692 - val_accuracy: 0.8190 - val_auc_roc: 0.9153\n",
      "Epoch 26/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.2510 - accuracy: 0.8932 - auc_roc: 0.9161 - val_loss: 0.3497 - val_accuracy: 0.8319 - val_auc_roc: 0.9170\n",
      "Epoch 27/100\n",
      "4409/4409 [==============================] - 0s 51us/step - loss: 0.2453 - accuracy: 0.8918 - auc_roc: 0.9179 - val_loss: 0.3686 - val_accuracy: 0.8276 - val_auc_roc: 0.9188\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 28/100\n",
      "4409/4409 [==============================] - 0s 56us/step - loss: 0.2343 - accuracy: 0.8970 - auc_roc: 0.9196 - val_loss: 0.3511 - val_accuracy: 0.8578 - val_auc_roc: 0.9206\n",
      "Epoch 29/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.2095 - accuracy: 0.9097 - auc_roc: 0.9216 - val_loss: 0.3544 - val_accuracy: 0.8578 - val_auc_roc: 0.9226\n",
      "Epoch 30/100\n",
      "4409/4409 [==============================] - 0s 54us/step - loss: 0.2052 - accuracy: 0.9186 - auc_roc: 0.9236 - val_loss: 0.3535 - val_accuracy: 0.8534 - val_auc_roc: 0.9246\n",
      "Epoch 31/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.2063 - accuracy: 0.9174 - auc_roc: 0.9254 - val_loss: 0.3570 - val_accuracy: 0.8405 - val_auc_roc: 0.9264\n",
      "Epoch 32/100\n",
      "4409/4409 [==============================] - 0s 52us/step - loss: 0.2058 - accuracy: 0.9145 - auc_roc: 0.9272 - val_loss: 0.3536 - val_accuracy: 0.8534 - val_auc_roc: 0.9281\n",
      "Epoch 33/100\n",
      "4409/4409 [==============================] - 0s 59us/step - loss: 0.1997 - accuracy: 0.9211 - auc_roc: 0.9289 - val_loss: 0.3586 - val_accuracy: 0.8405 - val_auc_roc: 0.9297\n",
      "Epoch 34/100\n",
      "4409/4409 [==============================] - 0s 57us/step - loss: 0.1958 - accuracy: 0.9204 - auc_roc: 0.9305 - val_loss: 0.3534 - val_accuracy: 0.8448 - val_auc_roc: 0.9313\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 00034: early stopping\n",
      "\n",
      "Fold  16\n",
      "Model: \"sequential_55\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_161 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_160 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_107 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_162 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_108 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_163 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_162 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4410 samples, validate on 231 samples\n",
      "Epoch 1/100\n",
      "4410/4410 [==============================] - 2s 484us/step - loss: 0.5764 - accuracy: 0.6893 - auc_roc: 0.5919 - val_loss: 0.4297 - val_accuracy: 0.8095 - val_auc_roc: 0.7125\n",
      "Epoch 2/100\n",
      "4410/4410 [==============================] - 0s 56us/step - loss: 0.4765 - accuracy: 0.7694 - auc_roc: 0.7566 - val_loss: 0.3856 - val_accuracy: 0.8139 - val_auc_roc: 0.7818\n",
      "Epoch 3/100\n",
      "4410/4410 [==============================] - 0s 60us/step - loss: 0.4475 - accuracy: 0.7857 - auc_roc: 0.7981 - val_loss: 0.3681 - val_accuracy: 0.8139 - val_auc_roc: 0.8097\n",
      "Epoch 4/100\n",
      "4410/4410 [==============================] - 0s 60us/step - loss: 0.4299 - accuracy: 0.7893 - auc_roc: 0.8194 - val_loss: 0.3460 - val_accuracy: 0.8485 - val_auc_roc: 0.8259\n",
      "Epoch 5/100\n",
      "4410/4410 [==============================] - 0s 60us/step - loss: 0.4085 - accuracy: 0.8073 - auc_roc: 0.8322 - val_loss: 0.3439 - val_accuracy: 0.8528 - val_auc_roc: 0.8384\n",
      "Epoch 6/100\n",
      "4410/4410 [==============================] - 0s 49us/step - loss: 0.3912 - accuracy: 0.8195 - auc_roc: 0.8441 - val_loss: 0.3361 - val_accuracy: 0.8398 - val_auc_roc: 0.8488\n",
      "Epoch 7/100\n",
      "4410/4410 [==============================] - 0s 59us/step - loss: 0.3840 - accuracy: 0.8281 - auc_roc: 0.8526 - val_loss: 0.3327 - val_accuracy: 0.8398 - val_auc_roc: 0.8565\n",
      "Epoch 8/100\n",
      "4410/4410 [==============================] - 0s 52us/step - loss: 0.3830 - accuracy: 0.8238 - auc_roc: 0.8597 - val_loss: 0.3424 - val_accuracy: 0.8312 - val_auc_roc: 0.8621\n",
      "Epoch 9/100\n",
      "4410/4410 [==============================] - 0s 56us/step - loss: 0.3711 - accuracy: 0.8286 - auc_roc: 0.8646 - val_loss: 0.3297 - val_accuracy: 0.8398 - val_auc_roc: 0.8673\n",
      "Epoch 10/100\n",
      "4410/4410 [==============================] - 0s 54us/step - loss: 0.3569 - accuracy: 0.8415 - auc_roc: 0.8699 - val_loss: 0.3207 - val_accuracy: 0.8528 - val_auc_roc: 0.8723\n",
      "Epoch 11/100\n",
      "4410/4410 [==============================] - 0s 47us/step - loss: 0.3563 - accuracy: 0.8415 - auc_roc: 0.8747 - val_loss: 0.3186 - val_accuracy: 0.8528 - val_auc_roc: 0.8762\n",
      "Epoch 12/100\n",
      "4410/4410 [==============================] - 0s 53us/step - loss: 0.3532 - accuracy: 0.8426 - auc_roc: 0.8781 - val_loss: 0.3230 - val_accuracy: 0.8658 - val_auc_roc: 0.8797\n",
      "Epoch 13/100\n",
      "4410/4410 [==============================] - 0s 49us/step - loss: 0.3411 - accuracy: 0.8458 - auc_roc: 0.8814 - val_loss: 0.3210 - val_accuracy: 0.8528 - val_auc_roc: 0.8832\n",
      "Epoch 14/100\n",
      "4410/4410 [==============================] - 0s 49us/step - loss: 0.3310 - accuracy: 0.8551 - auc_roc: 0.8849 - val_loss: 0.3292 - val_accuracy: 0.8398 - val_auc_roc: 0.8864\n",
      "Epoch 15/100\n",
      "4410/4410 [==============================] - 0s 52us/step - loss: 0.3201 - accuracy: 0.8599 - auc_roc: 0.8882 - val_loss: 0.3372 - val_accuracy: 0.8571 - val_auc_roc: 0.8897\n",
      "Epoch 16/100\n",
      "4410/4410 [==============================] - 0s 52us/step - loss: 0.3279 - accuracy: 0.8537 - auc_roc: 0.8909 - val_loss: 0.3213 - val_accuracy: 0.8658 - val_auc_roc: 0.8921\n",
      "Epoch 17/100\n",
      "4410/4410 [==============================] - 0s 53us/step - loss: 0.3208 - accuracy: 0.8583 - auc_roc: 0.8933 - val_loss: 0.3264 - val_accuracy: 0.8442 - val_auc_roc: 0.8945\n",
      "Epoch 18/100\n",
      "4410/4410 [==============================] - 0s 56us/step - loss: 0.3079 - accuracy: 0.8694 - auc_roc: 0.8959 - val_loss: 0.3129 - val_accuracy: 0.8788 - val_auc_roc: 0.8970\n",
      "Epoch 19/100\n",
      "4410/4410 [==============================] - 0s 50us/step - loss: 0.3079 - accuracy: 0.8671 - auc_roc: 0.8982 - val_loss: 0.3332 - val_accuracy: 0.8442 - val_auc_roc: 0.8993\n",
      "Epoch 20/100\n",
      "4410/4410 [==============================] - 0s 50us/step - loss: 0.3011 - accuracy: 0.8651 - auc_roc: 0.9004 - val_loss: 0.3260 - val_accuracy: 0.8571 - val_auc_roc: 0.9014\n",
      "Epoch 21/100\n",
      "4410/4410 [==============================] - 0s 52us/step - loss: 0.2899 - accuracy: 0.8696 - auc_roc: 0.9024 - val_loss: 0.3167 - val_accuracy: 0.8788 - val_auc_roc: 0.9036\n",
      "Epoch 22/100\n",
      "4410/4410 [==============================] - 0s 58us/step - loss: 0.2731 - accuracy: 0.8859 - auc_roc: 0.9048 - val_loss: 0.3336 - val_accuracy: 0.8485 - val_auc_roc: 0.9060\n",
      "Epoch 23/100\n",
      "4410/4410 [==============================] - 0s 52us/step - loss: 0.2703 - accuracy: 0.8850 - auc_roc: 0.9071 - val_loss: 0.3260 - val_accuracy: 0.8658 - val_auc_roc: 0.9081\n",
      "Epoch 24/100\n",
      "4410/4410 [==============================] - 0s 54us/step - loss: 0.2736 - accuracy: 0.8785 - auc_roc: 0.9092 - val_loss: 0.3469 - val_accuracy: 0.8485 - val_auc_roc: 0.9100\n",
      "Epoch 25/100\n",
      "4410/4410 [==============================] - 0s 51us/step - loss: 0.2909 - accuracy: 0.8714 - auc_roc: 0.9107 - val_loss: 0.3300 - val_accuracy: 0.8528 - val_auc_roc: 0.9114\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 26/100\n",
      "4410/4410 [==============================] - 0s 66us/step - loss: 0.2535 - accuracy: 0.8912 - auc_roc: 0.9124 - val_loss: 0.3297 - val_accuracy: 0.8485 - val_auc_roc: 0.9135\n",
      "Epoch 27/100\n",
      "4410/4410 [==============================] - 0s 68us/step - loss: 0.2491 - accuracy: 0.8934 - auc_roc: 0.9144 - val_loss: 0.3297 - val_accuracy: 0.8398 - val_auc_roc: 0.9154\n",
      "Epoch 28/100\n",
      "4410/4410 [==============================] - 0s 60us/step - loss: 0.2362 - accuracy: 0.8982 - auc_roc: 0.9164 - val_loss: 0.3316 - val_accuracy: 0.8485 - val_auc_roc: 0.9174\n",
      "Epoch 29/100\n",
      "4410/4410 [==============================] - 0s 56us/step - loss: 0.2374 - accuracy: 0.8991 - auc_roc: 0.9183 - val_loss: 0.3328 - val_accuracy: 0.8571 - val_auc_roc: 0.9192\n",
      "Epoch 30/100\n",
      "4410/4410 [==============================] - 0s 58us/step - loss: 0.2309 - accuracy: 0.9027 - auc_roc: 0.9201 - val_loss: 0.3301 - val_accuracy: 0.8571 - val_auc_roc: 0.9210\n",
      "Epoch 31/100\n",
      "4410/4410 [==============================] - 0s 48us/step - loss: 0.2285 - accuracy: 0.9059 - auc_roc: 0.9218 - val_loss: 0.3354 - val_accuracy: 0.8528 - val_auc_roc: 0.9226\n",
      "Epoch 32/100\n",
      "4410/4410 [==============================] - 0s 54us/step - loss: 0.2350 - accuracy: 0.9048 - auc_roc: 0.9234 - val_loss: 0.3281 - val_accuracy: 0.8485 - val_auc_roc: 0.9241\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 00032: early stopping\n",
      "\n",
      "Fold  17\n",
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_164 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_163 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_109 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_165 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_164 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_110 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_166 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_165 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4410 samples, validate on 231 samples\n",
      "Epoch 1/100\n",
      "4410/4410 [==============================] - 2s 486us/step - loss: 0.5628 - accuracy: 0.7007 - auc_roc: 0.6174 - val_loss: 0.4321 - val_accuracy: 0.8095 - val_auc_roc: 0.7376\n",
      "Epoch 2/100\n",
      "4410/4410 [==============================] - 0s 57us/step - loss: 0.4547 - accuracy: 0.7764 - auc_roc: 0.7761 - val_loss: 0.4218 - val_accuracy: 0.8225 - val_auc_roc: 0.7993\n",
      "Epoch 3/100\n",
      "4410/4410 [==============================] - 0s 59us/step - loss: 0.4263 - accuracy: 0.7980 - auc_roc: 0.8134 - val_loss: 0.4218 - val_accuracy: 0.8182 - val_auc_roc: 0.8250\n",
      "Epoch 4/100\n",
      "4410/4410 [==============================] - 0s 60us/step - loss: 0.4117 - accuracy: 0.8122 - auc_roc: 0.8332 - val_loss: 0.4030 - val_accuracy: 0.8052 - val_auc_roc: 0.8400\n",
      "Epoch 5/100\n",
      "4410/4410 [==============================] - 0s 59us/step - loss: 0.4007 - accuracy: 0.8195 - auc_roc: 0.8462 - val_loss: 0.4091 - val_accuracy: 0.8052 - val_auc_roc: 0.8501\n",
      "Epoch 6/100\n",
      "4410/4410 [==============================] - 0s 62us/step - loss: 0.3868 - accuracy: 0.8254 - auc_roc: 0.8541 - val_loss: 0.4059 - val_accuracy: 0.8182 - val_auc_roc: 0.8579\n",
      "Epoch 7/100\n",
      "4410/4410 [==============================] - 0s 52us/step - loss: 0.3739 - accuracy: 0.8302 - auc_roc: 0.8614 - val_loss: 0.3947 - val_accuracy: 0.7879 - val_auc_roc: 0.8646\n",
      "Epoch 8/100\n",
      "4410/4410 [==============================] - 0s 54us/step - loss: 0.3667 - accuracy: 0.8379 - auc_roc: 0.8676 - val_loss: 0.4026 - val_accuracy: 0.8268 - val_auc_roc: 0.8702\n",
      "Epoch 9/100\n",
      "4410/4410 [==============================] - 0s 55us/step - loss: 0.3551 - accuracy: 0.8379 - auc_roc: 0.8729 - val_loss: 0.4070 - val_accuracy: 0.8052 - val_auc_roc: 0.8751\n",
      "Epoch 10/100\n",
      "4410/4410 [==============================] - 0s 54us/step - loss: 0.3460 - accuracy: 0.8406 - auc_roc: 0.8773 - val_loss: 0.4122 - val_accuracy: 0.8182 - val_auc_roc: 0.8794\n",
      "Epoch 11/100\n",
      "4410/4410 [==============================] - 0s 56us/step - loss: 0.3374 - accuracy: 0.8410 - auc_roc: 0.8815 - val_loss: 0.4035 - val_accuracy: 0.8268 - val_auc_roc: 0.8834\n",
      "Epoch 12/100\n",
      "4410/4410 [==============================] - 0s 51us/step - loss: 0.3341 - accuracy: 0.8522 - auc_roc: 0.8852 - val_loss: 0.4044 - val_accuracy: 0.8225 - val_auc_roc: 0.8868\n",
      "Epoch 13/100\n",
      "4410/4410 [==============================] - 0s 55us/step - loss: 0.3191 - accuracy: 0.8601 - auc_roc: 0.8887 - val_loss: 0.4139 - val_accuracy: 0.8225 - val_auc_roc: 0.8903\n",
      "Epoch 14/100\n",
      "4410/4410 [==============================] - 0s 58us/step - loss: 0.3155 - accuracy: 0.8630 - auc_roc: 0.8919 - val_loss: 0.3978 - val_accuracy: 0.8442 - val_auc_roc: 0.8934\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 15/100\n",
      "4410/4410 [==============================] - 0s 56us/step - loss: 0.2992 - accuracy: 0.8626 - auc_roc: 0.8949 - val_loss: 0.3955 - val_accuracy: 0.8442 - val_auc_roc: 0.8967\n",
      "Epoch 16/100\n",
      "4410/4410 [==============================] - 0s 55us/step - loss: 0.2894 - accuracy: 0.8723 - auc_roc: 0.8984 - val_loss: 0.3971 - val_accuracy: 0.8528 - val_auc_roc: 0.8999\n",
      "Epoch 17/100\n",
      "4410/4410 [==============================] - 0s 55us/step - loss: 0.2820 - accuracy: 0.8791 - auc_roc: 0.9014 - val_loss: 0.3962 - val_accuracy: 0.8571 - val_auc_roc: 0.9029\n",
      "Epoch 18/100\n",
      "4410/4410 [==============================] - 0s 63us/step - loss: 0.2848 - accuracy: 0.8791 - auc_roc: 0.9042 - val_loss: 0.3972 - val_accuracy: 0.8615 - val_auc_roc: 0.9055\n",
      "Epoch 19/100\n",
      "4410/4410 [==============================] - 0s 62us/step - loss: 0.2797 - accuracy: 0.8803 - auc_roc: 0.9068 - val_loss: 0.3987 - val_accuracy: 0.8528 - val_auc_roc: 0.9079\n",
      "Epoch 20/100\n",
      "4410/4410 [==============================] - 0s 52us/step - loss: 0.2806 - accuracy: 0.8819 - auc_roc: 0.9091 - val_loss: 0.3989 - val_accuracy: 0.8571 - val_auc_roc: 0.9100\n",
      "Epoch 21/100\n",
      "4410/4410 [==============================] - 0s 53us/step - loss: 0.2773 - accuracy: 0.8812 - auc_roc: 0.9110 - val_loss: 0.4018 - val_accuracy: 0.8442 - val_auc_roc: 0.9120\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 00021: early stopping\n",
      "\n",
      "Fold  18\n",
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_167 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_166 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_111 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_168 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_167 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_112 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_169 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_168 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4410 samples, validate on 231 samples\n",
      "Epoch 1/100\n",
      "4410/4410 [==============================] - 2s 503us/step - loss: 0.5639 - accuracy: 0.7091 - auc_roc: 0.6201 - val_loss: 0.4583 - val_accuracy: 0.7706 - val_auc_roc: 0.7299\n",
      "Epoch 2/100\n",
      "4410/4410 [==============================] - 0s 54us/step - loss: 0.4665 - accuracy: 0.7685 - auc_roc: 0.7663 - val_loss: 0.4317 - val_accuracy: 0.7749 - val_auc_roc: 0.7910\n",
      "Epoch 3/100\n",
      "4410/4410 [==============================] - 0s 57us/step - loss: 0.4318 - accuracy: 0.7991 - auc_roc: 0.8072 - val_loss: 0.4061 - val_accuracy: 0.8052 - val_auc_roc: 0.8191\n",
      "Epoch 4/100\n",
      "4410/4410 [==============================] - 0s 60us/step - loss: 0.4166 - accuracy: 0.8023 - auc_roc: 0.8275 - val_loss: 0.3953 - val_accuracy: 0.8268 - val_auc_roc: 0.8348\n",
      "Epoch 5/100\n",
      "4410/4410 [==============================] - 0s 51us/step - loss: 0.4029 - accuracy: 0.8127 - auc_roc: 0.8399 - val_loss: 0.4043 - val_accuracy: 0.8095 - val_auc_roc: 0.8455\n",
      "Epoch 6/100\n",
      "4410/4410 [==============================] - 0s 51us/step - loss: 0.3876 - accuracy: 0.8281 - auc_roc: 0.8504 - val_loss: 0.3828 - val_accuracy: 0.8312 - val_auc_roc: 0.8545\n",
      "Epoch 7/100\n",
      "4410/4410 [==============================] - 0s 59us/step - loss: 0.3850 - accuracy: 0.8290 - auc_roc: 0.8580 - val_loss: 0.3934 - val_accuracy: 0.7879 - val_auc_roc: 0.8609\n",
      "Epoch 8/100\n",
      "4410/4410 [==============================] - 0s 56us/step - loss: 0.3704 - accuracy: 0.8320 - auc_roc: 0.8637 - val_loss: 0.3788 - val_accuracy: 0.8268 - val_auc_roc: 0.8665\n",
      "Epoch 9/100\n",
      "4410/4410 [==============================] - 0s 50us/step - loss: 0.3614 - accuracy: 0.8317 - auc_roc: 0.8694 - val_loss: 0.3720 - val_accuracy: 0.8442 - val_auc_roc: 0.8715\n",
      "Epoch 10/100\n",
      "4410/4410 [==============================] - 0s 57us/step - loss: 0.3532 - accuracy: 0.8395 - auc_roc: 0.8741 - val_loss: 0.3844 - val_accuracy: 0.8268 - val_auc_roc: 0.8759\n",
      "Epoch 11/100\n",
      "4410/4410 [==============================] - 0s 60us/step - loss: 0.3433 - accuracy: 0.8442 - auc_roc: 0.8781 - val_loss: 0.3693 - val_accuracy: 0.8355 - val_auc_roc: 0.8800\n",
      "Epoch 12/100\n",
      "4410/4410 [==============================] - 0s 56us/step - loss: 0.3341 - accuracy: 0.8535 - auc_roc: 0.8820 - val_loss: 0.3623 - val_accuracy: 0.8225 - val_auc_roc: 0.8839\n",
      "Epoch 13/100\n",
      "4410/4410 [==============================] - 0s 54us/step - loss: 0.3351 - accuracy: 0.8483 - auc_roc: 0.8855 - val_loss: 0.3658 - val_accuracy: 0.8442 - val_auc_roc: 0.8871\n",
      "Epoch 14/100\n",
      "4410/4410 [==============================] - 0s 54us/step - loss: 0.3197 - accuracy: 0.8578 - auc_roc: 0.8888 - val_loss: 0.3709 - val_accuracy: 0.8355 - val_auc_roc: 0.8904\n",
      "Epoch 15/100\n",
      "4410/4410 [==============================] - 0s 52us/step - loss: 0.3165 - accuracy: 0.8580 - auc_roc: 0.8919 - val_loss: 0.3892 - val_accuracy: 0.8139 - val_auc_roc: 0.8933\n",
      "Epoch 16/100\n",
      "4410/4410 [==============================] - 0s 59us/step - loss: 0.3140 - accuracy: 0.8644 - auc_roc: 0.8945 - val_loss: 0.3683 - val_accuracy: 0.8225 - val_auc_roc: 0.8959\n",
      "Epoch 17/100\n",
      "4410/4410 [==============================] - 0s 51us/step - loss: 0.3038 - accuracy: 0.8642 - auc_roc: 0.8972 - val_loss: 0.3633 - val_accuracy: 0.8355 - val_auc_roc: 0.8985\n",
      "Epoch 18/100\n",
      "4410/4410 [==============================] - 0s 53us/step - loss: 0.3037 - accuracy: 0.8673 - auc_roc: 0.8997 - val_loss: 0.3760 - val_accuracy: 0.8398 - val_auc_roc: 0.9008\n",
      "Epoch 19/100\n",
      "4410/4410 [==============================] - 0s 59us/step - loss: 0.2937 - accuracy: 0.8717 - auc_roc: 0.9019 - val_loss: 0.3673 - val_accuracy: 0.8442 - val_auc_roc: 0.9031\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 20/100\n",
      "4410/4410 [==============================] - 0s 52us/step - loss: 0.2787 - accuracy: 0.8778 - auc_roc: 0.9043 - val_loss: 0.3506 - val_accuracy: 0.8398 - val_auc_roc: 0.9055\n",
      "Epoch 21/100\n",
      "4410/4410 [==============================] - 0s 55us/step - loss: 0.2623 - accuracy: 0.8900 - auc_roc: 0.9068 - val_loss: 0.3511 - val_accuracy: 0.8528 - val_auc_roc: 0.9081\n",
      "Epoch 22/100\n",
      "4410/4410 [==============================] - 0s 54us/step - loss: 0.2623 - accuracy: 0.8884 - auc_roc: 0.9094 - val_loss: 0.3556 - val_accuracy: 0.8485 - val_auc_roc: 0.9105\n",
      "Epoch 23/100\n",
      "4410/4410 [==============================] - 0s 55us/step - loss: 0.2561 - accuracy: 0.8927 - auc_roc: 0.9116 - val_loss: 0.3538 - val_accuracy: 0.8528 - val_auc_roc: 0.9127\n",
      "Epoch 24/100\n",
      "4410/4410 [==============================] - 0s 60us/step - loss: 0.2550 - accuracy: 0.8943 - auc_roc: 0.9138 - val_loss: 0.3537 - val_accuracy: 0.8528 - val_auc_roc: 0.9148\n",
      "Epoch 25/100\n",
      "4410/4410 [==============================] - 0s 55us/step - loss: 0.2538 - accuracy: 0.8995 - auc_roc: 0.9158 - val_loss: 0.3537 - val_accuracy: 0.8398 - val_auc_roc: 0.9167\n",
      "Epoch 26/100\n",
      "4410/4410 [==============================] - 0s 51us/step - loss: 0.2505 - accuracy: 0.9011 - auc_roc: 0.9177 - val_loss: 0.3556 - val_accuracy: 0.8528 - val_auc_roc: 0.9186\n",
      "Epoch 27/100\n",
      "4410/4410 [==============================] - 0s 49us/step - loss: 0.2556 - accuracy: 0.8964 - auc_roc: 0.9193 - val_loss: 0.3508 - val_accuracy: 0.8528 - val_auc_roc: 0.9201\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 28/100\n",
      "4410/4410 [==============================] - 0s 54us/step - loss: 0.2420 - accuracy: 0.9025 - auc_roc: 0.9210 - val_loss: 0.3508 - val_accuracy: 0.8485 - val_auc_roc: 0.9218\n",
      "Epoch 29/100\n",
      "4410/4410 [==============================] - 0s 58us/step - loss: 0.2386 - accuracy: 0.9045 - auc_roc: 0.9226 - val_loss: 0.3513 - val_accuracy: 0.8485 - val_auc_roc: 0.9234\n",
      "Epoch 30/100\n",
      "4410/4410 [==============================] - 0s 56us/step - loss: 0.2484 - accuracy: 0.9002 - auc_roc: 0.9241 - val_loss: 0.3512 - val_accuracy: 0.8485 - val_auc_roc: 0.9247\n",
      "Epoch 31/100\n",
      "4410/4410 [==============================] - 0s 60us/step - loss: 0.2513 - accuracy: 0.8973 - auc_roc: 0.9254 - val_loss: 0.3515 - val_accuracy: 0.8485 - val_auc_roc: 0.9259\n",
      "Epoch 32/100\n",
      "4410/4410 [==============================] - 0s 58us/step - loss: 0.2351 - accuracy: 0.9025 - auc_roc: 0.9266 - val_loss: 0.3517 - val_accuracy: 0.8528 - val_auc_roc: 0.9273\n",
      "Epoch 33/100\n",
      "4410/4410 [==============================] - 0s 54us/step - loss: 0.2403 - accuracy: 0.9029 - auc_roc: 0.9278 - val_loss: 0.3525 - val_accuracy: 0.8528 - val_auc_roc: 0.9284\n",
      "Epoch 34/100\n",
      "4410/4410 [==============================] - 0s 54us/step - loss: 0.2395 - accuracy: 0.9032 - auc_roc: 0.9290 - val_loss: 0.3531 - val_accuracy: 0.8528 - val_auc_roc: 0.9295\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 00034: early stopping\n",
      "\n",
      "Fold  19\n",
      "Model: \"sequential_58\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_170 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_169 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_113 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_171 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_170 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_114 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_172 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_171 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4410 samples, validate on 231 samples\n",
      "Epoch 1/100\n",
      "4410/4410 [==============================] - 2s 505us/step - loss: 0.5716 - accuracy: 0.6993 - auc_roc: 0.6125 - val_loss: 0.4609 - val_accuracy: 0.8052 - val_auc_roc: 0.7213\n",
      "Epoch 2/100\n",
      "4410/4410 [==============================] - 0s 62us/step - loss: 0.4623 - accuracy: 0.7873 - auc_roc: 0.7655 - val_loss: 0.4223 - val_accuracy: 0.8139 - val_auc_roc: 0.7914\n",
      "Epoch 3/100\n",
      "4410/4410 [==============================] - 0s 51us/step - loss: 0.4436 - accuracy: 0.7882 - auc_roc: 0.8039 - val_loss: 0.4009 - val_accuracy: 0.8095 - val_auc_roc: 0.8159\n",
      "Epoch 4/100\n",
      "4410/4410 [==============================] - 0s 58us/step - loss: 0.4191 - accuracy: 0.8036 - auc_roc: 0.8247 - val_loss: 0.3985 - val_accuracy: 0.7922 - val_auc_roc: 0.8323\n",
      "Epoch 5/100\n",
      "4410/4410 [==============================] - 0s 60us/step - loss: 0.4050 - accuracy: 0.8132 - auc_roc: 0.8377 - val_loss: 0.3796 - val_accuracy: 0.8355 - val_auc_roc: 0.8433\n",
      "Epoch 6/100\n",
      "4410/4410 [==============================] - 0s 59us/step - loss: 0.3880 - accuracy: 0.8220 - auc_roc: 0.8474 - val_loss: 0.3674 - val_accuracy: 0.8268 - val_auc_roc: 0.8524\n",
      "Epoch 7/100\n",
      "4410/4410 [==============================] - 0s 56us/step - loss: 0.3888 - accuracy: 0.8145 - auc_roc: 0.8554 - val_loss: 0.3584 - val_accuracy: 0.8398 - val_auc_roc: 0.8586\n",
      "Epoch 8/100\n",
      "4410/4410 [==============================] - 0s 63us/step - loss: 0.3787 - accuracy: 0.8283 - auc_roc: 0.8617 - val_loss: 0.3581 - val_accuracy: 0.8225 - val_auc_roc: 0.8641\n",
      "Epoch 9/100\n",
      "4410/4410 [==============================] - 0s 59us/step - loss: 0.3701 - accuracy: 0.8367 - auc_roc: 0.8670 - val_loss: 0.3491 - val_accuracy: 0.8398 - val_auc_roc: 0.8691\n",
      "Epoch 10/100\n",
      "4410/4410 [==============================] - 0s 54us/step - loss: 0.3642 - accuracy: 0.8401 - auc_roc: 0.8713 - val_loss: 0.3493 - val_accuracy: 0.8528 - val_auc_roc: 0.8734\n",
      "Epoch 11/100\n",
      "4410/4410 [==============================] - 0s 60us/step - loss: 0.3513 - accuracy: 0.8404 - auc_roc: 0.8755 - val_loss: 0.3524 - val_accuracy: 0.8355 - val_auc_roc: 0.8774\n",
      "Epoch 12/100\n",
      "4410/4410 [==============================] - 0s 62us/step - loss: 0.3428 - accuracy: 0.8410 - auc_roc: 0.8792 - val_loss: 0.3385 - val_accuracy: 0.8571 - val_auc_roc: 0.8810\n",
      "Epoch 13/100\n",
      "4410/4410 [==============================] - 0s 54us/step - loss: 0.3369 - accuracy: 0.8494 - auc_roc: 0.8828 - val_loss: 0.3338 - val_accuracy: 0.8701 - val_auc_roc: 0.8845\n",
      "Epoch 14/100\n",
      "4410/4410 [==============================] - 0s 53us/step - loss: 0.3283 - accuracy: 0.8549 - auc_roc: 0.8863 - val_loss: 0.3396 - val_accuracy: 0.8528 - val_auc_roc: 0.8878\n",
      "Epoch 15/100\n",
      "4410/4410 [==============================] - 0s 53us/step - loss: 0.3204 - accuracy: 0.8621 - auc_roc: 0.8894 - val_loss: 0.3267 - val_accuracy: 0.8615 - val_auc_roc: 0.8909\n",
      "Epoch 16/100\n",
      "4410/4410 [==============================] - 0s 57us/step - loss: 0.3176 - accuracy: 0.8596 - auc_roc: 0.8924 - val_loss: 0.3277 - val_accuracy: 0.8571 - val_auc_roc: 0.8936\n",
      "Epoch 17/100\n",
      "4410/4410 [==============================] - 0s 63us/step - loss: 0.3113 - accuracy: 0.8671 - auc_roc: 0.8950 - val_loss: 0.3092 - val_accuracy: 0.8571 - val_auc_roc: 0.8963\n",
      "Epoch 18/100\n",
      "4410/4410 [==============================] - 0s 53us/step - loss: 0.3052 - accuracy: 0.8628 - auc_roc: 0.8977 - val_loss: 0.3330 - val_accuracy: 0.8398 - val_auc_roc: 0.8987\n",
      "Epoch 19/100\n",
      "4410/4410 [==============================] - 0s 54us/step - loss: 0.2969 - accuracy: 0.8719 - auc_roc: 0.9000 - val_loss: 0.3177 - val_accuracy: 0.8658 - val_auc_roc: 0.9011\n",
      "Epoch 20/100\n",
      "4410/4410 [==============================] - 0s 53us/step - loss: 0.2950 - accuracy: 0.8707 - auc_roc: 0.9023 - val_loss: 0.3147 - val_accuracy: 0.8528 - val_auc_roc: 0.9033\n",
      "Epoch 21/100\n",
      "4410/4410 [==============================] - 0s 53us/step - loss: 0.2861 - accuracy: 0.8714 - auc_roc: 0.9044 - val_loss: 0.3295 - val_accuracy: 0.8571 - val_auc_roc: 0.9054\n",
      "Epoch 22/100\n",
      "4410/4410 [==============================] - 0s 55us/step - loss: 0.2873 - accuracy: 0.8730 - auc_roc: 0.9065 - val_loss: 0.3253 - val_accuracy: 0.8485 - val_auc_roc: 0.9073\n",
      "Epoch 23/100\n",
      "4410/4410 [==============================] - 0s 55us/step - loss: 0.2800 - accuracy: 0.8771 - auc_roc: 0.9083 - val_loss: 0.3210 - val_accuracy: 0.8528 - val_auc_roc: 0.9092\n",
      "Epoch 24/100\n",
      "4410/4410 [==============================] - 0s 56us/step - loss: 0.2614 - accuracy: 0.8871 - auc_roc: 0.9103 - val_loss: 0.3267 - val_accuracy: 0.8571 - val_auc_roc: 0.9113\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 25/100\n",
      "4410/4410 [==============================] - 0s 60us/step - loss: 0.2492 - accuracy: 0.8968 - auc_roc: 0.9124 - val_loss: 0.3210 - val_accuracy: 0.8528 - val_auc_roc: 0.9135\n",
      "Epoch 26/100\n",
      "4410/4410 [==============================] - 0s 51us/step - loss: 0.2433 - accuracy: 0.8984 - auc_roc: 0.9146 - val_loss: 0.3213 - val_accuracy: 0.8485 - val_auc_roc: 0.9156\n",
      "Epoch 27/100\n",
      "4410/4410 [==============================] - 0s 52us/step - loss: 0.2333 - accuracy: 0.9068 - auc_roc: 0.9166 - val_loss: 0.3223 - val_accuracy: 0.8485 - val_auc_roc: 0.9177\n",
      "Epoch 28/100\n",
      "4410/4410 [==============================] - 0s 49us/step - loss: 0.2368 - accuracy: 0.9016 - auc_roc: 0.9187 - val_loss: 0.3227 - val_accuracy: 0.8571 - val_auc_roc: 0.9196\n",
      "Epoch 29/100\n",
      "4410/4410 [==============================] - 0s 54us/step - loss: 0.2230 - accuracy: 0.9093 - auc_roc: 0.9206 - val_loss: 0.3205 - val_accuracy: 0.8615 - val_auc_roc: 0.9216\n",
      "Epoch 30/100\n",
      "4410/4410 [==============================] - 0s 51us/step - loss: 0.2246 - accuracy: 0.9093 - auc_roc: 0.9225 - val_loss: 0.3206 - val_accuracy: 0.8701 - val_auc_roc: 0.9233\n",
      "Epoch 31/100\n",
      "4410/4410 [==============================] - 0s 53us/step - loss: 0.2261 - accuracy: 0.9102 - auc_roc: 0.9242 - val_loss: 0.3205 - val_accuracy: 0.8615 - val_auc_roc: 0.9250\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 00031: early stopping\n",
      "\n",
      "Fold  20\n",
      "Model: \"sequential_59\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_173 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_172 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_115 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_174 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_173 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_116 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_175 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_174 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4410 samples, validate on 231 samples\n",
      "Epoch 1/100\n",
      "4410/4410 [==============================] - 2s 520us/step - loss: 0.5768 - accuracy: 0.6907 - auc_roc: 0.5848 - val_loss: 0.4554 - val_accuracy: 0.7879 - val_auc_roc: 0.7071\n",
      "Epoch 2/100\n",
      "4410/4410 [==============================] - 0s 63us/step - loss: 0.4645 - accuracy: 0.7757 - auc_roc: 0.7549 - val_loss: 0.4323 - val_accuracy: 0.8052 - val_auc_roc: 0.7842\n",
      "Epoch 3/100\n",
      "4410/4410 [==============================] - 0s 63us/step - loss: 0.4416 - accuracy: 0.7973 - auc_roc: 0.8020 - val_loss: 0.4173 - val_accuracy: 0.8095 - val_auc_roc: 0.8131\n",
      "Epoch 4/100\n",
      "4410/4410 [==============================] - 0s 57us/step - loss: 0.4277 - accuracy: 0.8014 - auc_roc: 0.8220 - val_loss: 0.3959 - val_accuracy: 0.8398 - val_auc_roc: 0.8286\n",
      "Epoch 5/100\n",
      "4410/4410 [==============================] - 0s 55us/step - loss: 0.4086 - accuracy: 0.8147 - auc_roc: 0.8352 - val_loss: 0.3833 - val_accuracy: 0.8442 - val_auc_roc: 0.8405\n",
      "Epoch 6/100\n",
      "4410/4410 [==============================] - 0s 60us/step - loss: 0.4012 - accuracy: 0.8204 - auc_roc: 0.8447 - val_loss: 0.3888 - val_accuracy: 0.8312 - val_auc_roc: 0.8489\n",
      "Epoch 7/100\n",
      "4410/4410 [==============================] - 0s 59us/step - loss: 0.3854 - accuracy: 0.8206 - auc_roc: 0.8526 - val_loss: 0.3707 - val_accuracy: 0.8398 - val_auc_roc: 0.8561\n",
      "Epoch 8/100\n",
      "4410/4410 [==============================] - 0s 54us/step - loss: 0.3741 - accuracy: 0.8313 - auc_roc: 0.8593 - val_loss: 0.3650 - val_accuracy: 0.8485 - val_auc_roc: 0.8624\n",
      "Epoch 9/100\n",
      "4410/4410 [==============================] - 0s 58us/step - loss: 0.3748 - accuracy: 0.8215 - auc_roc: 0.8647 - val_loss: 0.3706 - val_accuracy: 0.8442 - val_auc_roc: 0.8669\n",
      "Epoch 10/100\n",
      "4410/4410 [==============================] - 0s 56us/step - loss: 0.3645 - accuracy: 0.8322 - auc_roc: 0.8693 - val_loss: 0.3645 - val_accuracy: 0.8268 - val_auc_roc: 0.8713\n",
      "Epoch 11/100\n",
      "4410/4410 [==============================] - 0s 50us/step - loss: 0.3509 - accuracy: 0.8431 - auc_roc: 0.8733 - val_loss: 0.3803 - val_accuracy: 0.8312 - val_auc_roc: 0.8754\n",
      "Epoch 12/100\n",
      "4410/4410 [==============================] - 0s 51us/step - loss: 0.3480 - accuracy: 0.8465 - auc_roc: 0.8772 - val_loss: 0.3498 - val_accuracy: 0.8398 - val_auc_roc: 0.8791\n",
      "Epoch 13/100\n",
      "4410/4410 [==============================] - 0s 56us/step - loss: 0.3380 - accuracy: 0.8460 - auc_roc: 0.8808 - val_loss: 0.3246 - val_accuracy: 0.8485 - val_auc_roc: 0.8826\n",
      "Epoch 14/100\n",
      "4410/4410 [==============================] - 0s 55us/step - loss: 0.3364 - accuracy: 0.8531 - auc_roc: 0.8842 - val_loss: 0.3460 - val_accuracy: 0.8442 - val_auc_roc: 0.8857\n",
      "Epoch 15/100\n",
      "4410/4410 [==============================] - 0s 53us/step - loss: 0.3177 - accuracy: 0.8619 - auc_roc: 0.8872 - val_loss: 0.3293 - val_accuracy: 0.8312 - val_auc_roc: 0.8889\n",
      "Epoch 16/100\n",
      "4410/4410 [==============================] - 0s 55us/step - loss: 0.3215 - accuracy: 0.8649 - auc_roc: 0.8906 - val_loss: 0.3275 - val_accuracy: 0.8485 - val_auc_roc: 0.8918\n",
      "Epoch 17/100\n",
      "4410/4410 [==============================] - 0s 56us/step - loss: 0.3130 - accuracy: 0.8630 - auc_roc: 0.8932 - val_loss: 0.3232 - val_accuracy: 0.8355 - val_auc_roc: 0.8945\n",
      "Epoch 18/100\n",
      "4410/4410 [==============================] - 0s 60us/step - loss: 0.3034 - accuracy: 0.8637 - auc_roc: 0.8957 - val_loss: 0.3149 - val_accuracy: 0.8571 - val_auc_roc: 0.8971\n",
      "Epoch 19/100\n",
      "4410/4410 [==============================] - 0s 57us/step - loss: 0.2874 - accuracy: 0.8764 - auc_roc: 0.8984 - val_loss: 0.3233 - val_accuracy: 0.8528 - val_auc_roc: 0.8998\n",
      "Epoch 20/100\n",
      "4410/4410 [==============================] - 0s 57us/step - loss: 0.2891 - accuracy: 0.8755 - auc_roc: 0.9010 - val_loss: 0.3274 - val_accuracy: 0.8571 - val_auc_roc: 0.9023\n",
      "Epoch 21/100\n",
      "4410/4410 [==============================] - 0s 55us/step - loss: 0.2908 - accuracy: 0.8753 - auc_roc: 0.9032 - val_loss: 0.3308 - val_accuracy: 0.8312 - val_auc_roc: 0.9044\n",
      "Epoch 22/100\n",
      "4410/4410 [==============================] - 0s 55us/step - loss: 0.2764 - accuracy: 0.8819 - auc_roc: 0.9055 - val_loss: 0.3175 - val_accuracy: 0.8312 - val_auc_roc: 0.9067\n",
      "Epoch 23/100\n",
      "4410/4410 [==============================] - 0s 55us/step - loss: 0.2802 - accuracy: 0.8837 - auc_roc: 0.9077 - val_loss: 0.3061 - val_accuracy: 0.8528 - val_auc_roc: 0.9086\n",
      "Epoch 24/100\n",
      "4410/4410 [==============================] - 0s 56us/step - loss: 0.2650 - accuracy: 0.8830 - auc_roc: 0.9096 - val_loss: 0.3124 - val_accuracy: 0.8615 - val_auc_roc: 0.9106\n",
      "Epoch 25/100\n",
      "4410/4410 [==============================] - 0s 54us/step - loss: 0.2595 - accuracy: 0.8864 - auc_roc: 0.9116 - val_loss: 0.3187 - val_accuracy: 0.8701 - val_auc_roc: 0.9126\n",
      "Epoch 26/100\n",
      "4410/4410 [==============================] - 0s 51us/step - loss: 0.2594 - accuracy: 0.8927 - auc_roc: 0.9136 - val_loss: 0.3417 - val_accuracy: 0.8571 - val_auc_roc: 0.9144\n",
      "Epoch 27/100\n",
      "4410/4410 [==============================] - 0s 54us/step - loss: 0.2492 - accuracy: 0.8927 - auc_roc: 0.9153 - val_loss: 0.3216 - val_accuracy: 0.8571 - val_auc_roc: 0.9162\n",
      "Epoch 28/100\n",
      "4410/4410 [==============================] - 0s 55us/step - loss: 0.2550 - accuracy: 0.8905 - auc_roc: 0.9170 - val_loss: 0.3145 - val_accuracy: 0.8615 - val_auc_roc: 0.9178\n",
      "Epoch 29/100\n",
      "4410/4410 [==============================] - 0s 55us/step - loss: 0.2493 - accuracy: 0.8909 - auc_roc: 0.9186 - val_loss: 0.3278 - val_accuracy: 0.8528 - val_auc_roc: 0.9193\n",
      "Epoch 30/100\n",
      "4410/4410 [==============================] - 0s 58us/step - loss: 0.2479 - accuracy: 0.8909 - auc_roc: 0.9201 - val_loss: 0.3150 - val_accuracy: 0.8745 - val_auc_roc: 0.9207\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 31/100\n",
      "4410/4410 [==============================] - 0s 58us/step - loss: 0.2295 - accuracy: 0.9045 - auc_roc: 0.9215 - val_loss: 0.3108 - val_accuracy: 0.8745 - val_auc_roc: 0.9224\n",
      "Epoch 32/100\n",
      "4410/4410 [==============================] - 0s 59us/step - loss: 0.2135 - accuracy: 0.9145 - auc_roc: 0.9233 - val_loss: 0.3076 - val_accuracy: 0.8701 - val_auc_roc: 0.9242\n",
      "Epoch 33/100\n",
      "4410/4410 [==============================] - 0s 64us/step - loss: 0.2061 - accuracy: 0.9143 - auc_roc: 0.9251 - val_loss: 0.3119 - val_accuracy: 0.8615 - val_auc_roc: 0.9259\n",
      "Epoch 34/100\n",
      "4410/4410 [==============================] - 0s 64us/step - loss: 0.2052 - accuracy: 0.9181 - auc_roc: 0.9267 - val_loss: 0.3068 - val_accuracy: 0.8701 - val_auc_roc: 0.9276\n",
      "Epoch 35/100\n",
      "4410/4410 [==============================] - 0s 50us/step - loss: 0.2060 - accuracy: 0.9206 - auc_roc: 0.9283 - val_loss: 0.3041 - val_accuracy: 0.8701 - val_auc_roc: 0.9291\n",
      "Epoch 36/100\n",
      "4410/4410 [==============================] - 0s 54us/step - loss: 0.1953 - accuracy: 0.9236 - auc_roc: 0.9298 - val_loss: 0.3060 - val_accuracy: 0.8701 - val_auc_roc: 0.9306\n",
      "Epoch 37/100\n",
      "4410/4410 [==============================] - 0s 50us/step - loss: 0.1944 - accuracy: 0.9261 - auc_roc: 0.9313 - val_loss: 0.3044 - val_accuracy: 0.8701 - val_auc_roc: 0.9321\n",
      "Epoch 38/100\n",
      "4410/4410 [==============================] - 0s 56us/step - loss: 0.1964 - accuracy: 0.9200 - auc_roc: 0.9328 - val_loss: 0.3003 - val_accuracy: 0.8745 - val_auc_roc: 0.9334\n",
      "Epoch 39/100\n",
      "4410/4410 [==============================] - 0s 56us/step - loss: 0.1840 - accuracy: 0.9277 - auc_roc: 0.9341 - val_loss: 0.3103 - val_accuracy: 0.8701 - val_auc_roc: 0.9348\n",
      "Epoch 40/100\n",
      "4410/4410 [==============================] - 0s 55us/step - loss: 0.1876 - accuracy: 0.9218 - auc_roc: 0.9354 - val_loss: 0.3001 - val_accuracy: 0.8701 - val_auc_roc: 0.9361\n",
      "Epoch 41/100\n",
      "4410/4410 [==============================] - 0s 54us/step - loss: 0.1866 - accuracy: 0.9245 - auc_roc: 0.9367 - val_loss: 0.3031 - val_accuracy: 0.8701 - val_auc_roc: 0.9373\n",
      "Epoch 42/100\n",
      "4410/4410 [==============================] - 0s 54us/step - loss: 0.1885 - accuracy: 0.9229 - auc_roc: 0.9378 - val_loss: 0.3018 - val_accuracy: 0.8745 - val_auc_roc: 0.9384\n",
      "Epoch 43/100\n",
      "4410/4410 [==============================] - 0s 58us/step - loss: 0.1871 - accuracy: 0.9222 - auc_roc: 0.9389 - val_loss: 0.3100 - val_accuracy: 0.8658 - val_auc_roc: 0.9395\n",
      "Epoch 44/100\n",
      "4410/4410 [==============================] - 0s 55us/step - loss: 0.1795 - accuracy: 0.9331 - auc_roc: 0.9400 - val_loss: 0.3073 - val_accuracy: 0.8831 - val_auc_roc: 0.9405\n",
      "Epoch 45/100\n",
      "4410/4410 [==============================] - 0s 55us/step - loss: 0.1794 - accuracy: 0.9306 - auc_roc: 0.9410 - val_loss: 0.3108 - val_accuracy: 0.8701 - val_auc_roc: 0.9416\n",
      "Epoch 46/100\n",
      "4410/4410 [==============================] - 0s 50us/step - loss: 0.1761 - accuracy: 0.9320 - auc_roc: 0.9420 - val_loss: 0.3095 - val_accuracy: 0.8701 - val_auc_roc: 0.9426\n",
      "Epoch 47/100\n",
      "4410/4410 [==============================] - 0s 54us/step - loss: 0.1744 - accuracy: 0.9342 - auc_roc: 0.9430 - val_loss: 0.3085 - val_accuracy: 0.8788 - val_auc_roc: 0.9435\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 48/100\n",
      "4410/4410 [==============================] - 0s 54us/step - loss: 0.1757 - accuracy: 0.9315 - auc_roc: 0.9439 - val_loss: 0.3097 - val_accuracy: 0.8745 - val_auc_roc: 0.9444\n",
      "Epoch 49/100\n",
      "4410/4410 [==============================] - 0s 62us/step - loss: 0.1695 - accuracy: 0.9354 - auc_roc: 0.9449 - val_loss: 0.3105 - val_accuracy: 0.8788 - val_auc_roc: 0.9453\n",
      "Epoch 50/100\n",
      "4410/4410 [==============================] - 0s 58us/step - loss: 0.1712 - accuracy: 0.9374 - auc_roc: 0.9457 - val_loss: 0.3110 - val_accuracy: 0.8745 - val_auc_roc: 0.9462\n",
      "Epoch 51/100\n",
      "4410/4410 [==============================] - 0s 51us/step - loss: 0.1763 - accuracy: 0.9306 - auc_roc: 0.9466 - val_loss: 0.3118 - val_accuracy: 0.8745 - val_auc_roc: 0.9469\n",
      "Epoch 52/100\n",
      "4410/4410 [==============================] - 0s 55us/step - loss: 0.1756 - accuracy: 0.9311 - auc_roc: 0.9473 - val_loss: 0.3113 - val_accuracy: 0.8788 - val_auc_roc: 0.9477\n",
      "Epoch 53/100\n",
      "4410/4410 [==============================] - 0s 62us/step - loss: 0.1762 - accuracy: 0.9329 - auc_roc: 0.9480 - val_loss: 0.3114 - val_accuracy: 0.8788 - val_auc_roc: 0.9484\n",
      "Epoch 54/100\n",
      "4410/4410 [==============================] - 0s 51us/step - loss: 0.1741 - accuracy: 0.9293 - auc_roc: 0.9487 - val_loss: 0.3131 - val_accuracy: 0.8701 - val_auc_roc: 0.9491\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 00054: early stopping\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "folds = list(StratifiedKFold(n_splits=20, shuffle=True, random_state=1).split(Xtrain_org, Ytrain_org))\n",
    "\n",
    "for j, (train_idx, val_idx) in enumerate(folds):\n",
    "    print('\\nFold ',j+1)\n",
    "    X_train_cv = np.array(Xtrain_org)[train_idx]\n",
    "    y_train_cv = np.array(Ytrain_org)[train_idx]\n",
    "    X_valid_cv = np.array(Xtrain_org)[val_idx]\n",
    "    y_valid_cv= np.array(Ytrain_org)[val_idx]\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train_cv), y_train_cv)\n",
    "    my_callbacks = get_callbacks('MLP_org')\n",
    "    model = get_model()\n",
    "    model.fit(X_train_cv, y_train_cv,\n",
    "              validation_data=(X_valid_cv, y_valid_cv),\n",
    "              shuffle=True,\n",
    "              batch_size=128,\n",
    "              epochs=100,\n",
    "              class_weight=class_weights,\n",
    "              callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:998: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_60\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_176 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_175 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_117 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_177 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_176 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_118 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_178 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_177 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 74879 samples, validate on 3942 samples\n",
      "Epoch 1/100\n",
      "74879/74879 [==============================] - 6s 85us/step - loss: 0.4880 - accuracy: 0.7720 - auc_roc: 0.7446 - val_loss: 0.4282 - val_accuracy: 0.8062 - val_auc_roc: 0.7948\n",
      "Epoch 2/100\n",
      "74879/74879 [==============================] - 4s 54us/step - loss: 0.4415 - accuracy: 0.7987 - auc_roc: 0.8079 - val_loss: 0.3984 - val_accuracy: 0.8184 - val_auc_roc: 0.8177\n",
      "Epoch 3/100\n",
      "74879/74879 [==============================] - 4s 55us/step - loss: 0.4218 - accuracy: 0.8077 - auc_roc: 0.8246 - val_loss: 0.3833 - val_accuracy: 0.8267 - val_auc_roc: 0.8302\n",
      "Epoch 4/100\n",
      "74879/74879 [==============================] - 4s 55us/step - loss: 0.4061 - accuracy: 0.8167 - auc_roc: 0.8352 - val_loss: 0.3708 - val_accuracy: 0.8359 - val_auc_roc: 0.8396\n",
      "Epoch 5/100\n",
      "74879/74879 [==============================] - 4s 54us/step - loss: 0.3924 - accuracy: 0.8226 - auc_roc: 0.8437 - val_loss: 0.3665 - val_accuracy: 0.8382 - val_auc_roc: 0.8470\n",
      "Epoch 6/100\n",
      "74879/74879 [==============================] - 4s 55us/step - loss: 0.3821 - accuracy: 0.8295 - auc_roc: 0.8502 - val_loss: 0.3558 - val_accuracy: 0.8415 - val_auc_roc: 0.8531\n",
      "Epoch 7/100\n",
      "74879/74879 [==============================] - 4s 55us/step - loss: 0.3735 - accuracy: 0.8319 - auc_roc: 0.8558 - val_loss: 0.3485 - val_accuracy: 0.8445 - val_auc_roc: 0.8583\n",
      "Epoch 8/100\n",
      "74879/74879 [==============================] - 4s 53us/step - loss: 0.3674 - accuracy: 0.8359 - auc_roc: 0.8605 - val_loss: 0.3295 - val_accuracy: 0.8579 - val_auc_roc: 0.8627\n",
      "Epoch 9/100\n",
      "74879/74879 [==============================] - 4s 53us/step - loss: 0.3558 - accuracy: 0.8408 - auc_roc: 0.8650 - val_loss: 0.3191 - val_accuracy: 0.8551 - val_auc_roc: 0.8671\n",
      "Epoch 10/100\n",
      "74879/74879 [==============================] - 4s 52us/step - loss: 0.3476 - accuracy: 0.8448 - auc_roc: 0.8691 - val_loss: 0.3138 - val_accuracy: 0.8605 - val_auc_roc: 0.8710\n",
      "Epoch 11/100\n",
      "74879/74879 [==============================] - 4s 57us/step - loss: 0.3409 - accuracy: 0.8485 - auc_roc: 0.8728 - val_loss: 0.3088 - val_accuracy: 0.8645 - val_auc_roc: 0.8746\n",
      "Epoch 12/100\n",
      "74879/74879 [==============================] - 4s 55us/step - loss: 0.3363 - accuracy: 0.8503 - auc_roc: 0.8763 - val_loss: 0.3045 - val_accuracy: 0.8663 - val_auc_roc: 0.8777\n",
      "Epoch 13/100\n",
      "74879/74879 [==============================] - 4s 56us/step - loss: 0.3363 - accuracy: 0.8492 - auc_roc: 0.8792 - val_loss: 0.3040 - val_accuracy: 0.8656 - val_auc_roc: 0.8805\n",
      "Epoch 14/100\n",
      "74879/74879 [==============================] - 4s 55us/step - loss: 0.3285 - accuracy: 0.8532 - auc_roc: 0.8818 - val_loss: 0.3000 - val_accuracy: 0.8701 - val_auc_roc: 0.8831\n",
      "Epoch 15/100\n",
      "74879/74879 [==============================] - 4s 52us/step - loss: 0.3254 - accuracy: 0.8550 - auc_roc: 0.8843 - val_loss: 0.2937 - val_accuracy: 0.8706 - val_auc_roc: 0.8855\n",
      "Epoch 16/100\n",
      "74879/74879 [==============================] - 4s 54us/step - loss: 0.3188 - accuracy: 0.8583 - auc_roc: 0.8867 - val_loss: 0.2862 - val_accuracy: 0.8742 - val_auc_roc: 0.8879\n",
      "Epoch 17/100\n",
      "74879/74879 [==============================] - 4s 51us/step - loss: 0.3148 - accuracy: 0.8593 - auc_roc: 0.8890 - val_loss: 0.2881 - val_accuracy: 0.8732 - val_auc_roc: 0.8901\n",
      "Epoch 18/100\n",
      "74879/74879 [==============================] - 4s 51us/step - loss: 0.3068 - accuracy: 0.8632 - auc_roc: 0.8912 - val_loss: 0.2795 - val_accuracy: 0.8775 - val_auc_roc: 0.8923\n",
      "Epoch 19/100\n",
      "74879/74879 [==============================] - 4s 54us/step - loss: 0.3086 - accuracy: 0.8624 - auc_roc: 0.8932 - val_loss: 0.2783 - val_accuracy: 0.8792 - val_auc_roc: 0.8942\n",
      "Epoch 20/100\n",
      "74879/74879 [==============================] - 4s 52us/step - loss: 0.3060 - accuracy: 0.8618 - auc_roc: 0.8950 - val_loss: 0.2763 - val_accuracy: 0.8772 - val_auc_roc: 0.8959\n",
      "Epoch 21/100\n",
      "74879/74879 [==============================] - 4s 54us/step - loss: 0.2976 - accuracy: 0.8663 - auc_roc: 0.8969 - val_loss: 0.2703 - val_accuracy: 0.8805 - val_auc_roc: 0.8978\n",
      "Epoch 22/100\n",
      "74879/74879 [==============================] - 4s 48us/step - loss: 0.2961 - accuracy: 0.8685 - auc_roc: 0.8986 - val_loss: 0.2664 - val_accuracy: 0.8831 - val_auc_roc: 0.8994\n",
      "Epoch 23/100\n",
      "74879/74879 [==============================] - 4s 48us/step - loss: 0.2938 - accuracy: 0.8687 - auc_roc: 0.9002 - val_loss: 0.2725 - val_accuracy: 0.8757 - val_auc_roc: 0.9010\n",
      "Epoch 24/100\n",
      "74879/74879 [==============================] - 4s 49us/step - loss: 0.2918 - accuracy: 0.8681 - auc_roc: 0.9018 - val_loss: 0.2582 - val_accuracy: 0.8891 - val_auc_roc: 0.9025\n",
      "Epoch 25/100\n",
      "74879/74879 [==============================] - 4s 51us/step - loss: 0.2878 - accuracy: 0.8711 - auc_roc: 0.9033 - val_loss: 0.2679 - val_accuracy: 0.8871 - val_auc_roc: 0.9040\n",
      "Epoch 26/100\n",
      "74879/74879 [==============================] - 4s 49us/step - loss: 0.2860 - accuracy: 0.8709 - auc_roc: 0.9046 - val_loss: 0.2602 - val_accuracy: 0.8907 - val_auc_roc: 0.9053\n",
      "Epoch 27/100\n",
      "74879/74879 [==============================] - 4s 47us/step - loss: 0.2845 - accuracy: 0.8707 - auc_roc: 0.9060 - val_loss: 0.2565 - val_accuracy: 0.8894 - val_auc_roc: 0.9066\n",
      "Epoch 28/100\n",
      "74879/74879 [==============================] - 4s 49us/step - loss: 0.2805 - accuracy: 0.8713 - auc_roc: 0.9072 - val_loss: 0.2568 - val_accuracy: 0.8846 - val_auc_roc: 0.9079\n",
      "Epoch 29/100\n",
      "74879/74879 [==============================] - 4s 48us/step - loss: 0.2751 - accuracy: 0.8742 - auc_roc: 0.9085 - val_loss: 0.2531 - val_accuracy: 0.8904 - val_auc_roc: 0.9092\n",
      "Epoch 30/100\n",
      "74879/74879 [==============================] - 4s 51us/step - loss: 0.2751 - accuracy: 0.8761 - auc_roc: 0.9097 - val_loss: 0.2596 - val_accuracy: 0.8851 - val_auc_roc: 0.9103\n",
      "Epoch 31/100\n",
      "74879/74879 [==============================] - 4s 50us/step - loss: 0.2720 - accuracy: 0.8759 - auc_roc: 0.9109 - val_loss: 0.2463 - val_accuracy: 0.8957 - val_auc_roc: 0.9115\n",
      "Epoch 32/100\n",
      "74879/74879 [==============================] - 4s 49us/step - loss: 0.2714 - accuracy: 0.8777 - auc_roc: 0.9120 - val_loss: 0.2481 - val_accuracy: 0.8919 - val_auc_roc: 0.9126\n",
      "Epoch 33/100\n",
      "74879/74879 [==============================] - 4s 48us/step - loss: 0.2691 - accuracy: 0.8776 - auc_roc: 0.9131 - val_loss: 0.2437 - val_accuracy: 0.8945 - val_auc_roc: 0.9136\n",
      "Epoch 34/100\n",
      "74879/74879 [==============================] - 4s 49us/step - loss: 0.2669 - accuracy: 0.8789 - auc_roc: 0.9141 - val_loss: 0.2528 - val_accuracy: 0.8871 - val_auc_roc: 0.9146\n",
      "Epoch 35/100\n",
      "74879/74879 [==============================] - 4s 49us/step - loss: 0.2680 - accuracy: 0.8773 - auc_roc: 0.9151 - val_loss: 0.2447 - val_accuracy: 0.8960 - val_auc_roc: 0.9155\n",
      "Epoch 36/100\n",
      "74879/74879 [==============================] - 4s 48us/step - loss: 0.2661 - accuracy: 0.8787 - auc_roc: 0.9160 - val_loss: 0.2437 - val_accuracy: 0.8962 - val_auc_roc: 0.9164\n",
      "Epoch 37/100\n",
      "74879/74879 [==============================] - 4s 49us/step - loss: 0.2632 - accuracy: 0.8808 - auc_roc: 0.9169 - val_loss: 0.2395 - val_accuracy: 0.8975 - val_auc_roc: 0.9173\n",
      "Epoch 38/100\n",
      "74879/74879 [==============================] - 4s 50us/step - loss: 0.2597 - accuracy: 0.8808 - auc_roc: 0.9178 - val_loss: 0.2518 - val_accuracy: 0.8937 - val_auc_roc: 0.9182\n",
      "Epoch 39/100\n",
      "74879/74879 [==============================] - 4s 54us/step - loss: 0.2594 - accuracy: 0.8804 - auc_roc: 0.9186 - val_loss: 0.2447 - val_accuracy: 0.8914 - val_auc_roc: 0.9191\n",
      "Epoch 40/100\n",
      "74879/74879 [==============================] - 4s 52us/step - loss: 0.2547 - accuracy: 0.8823 - auc_roc: 0.9195 - val_loss: 0.2387 - val_accuracy: 0.8978 - val_auc_roc: 0.9199\n",
      "Epoch 41/100\n",
      "74879/74879 [==============================] - 4s 53us/step - loss: 0.2578 - accuracy: 0.8803 - auc_roc: 0.9203 - val_loss: 0.2451 - val_accuracy: 0.8973 - val_auc_roc: 0.9207\n",
      "Epoch 42/100\n",
      "74879/74879 [==============================] - 4s 50us/step - loss: 0.2519 - accuracy: 0.8849 - auc_roc: 0.9211 - val_loss: 0.2402 - val_accuracy: 0.8965 - val_auc_roc: 0.9215\n",
      "Epoch 43/100\n",
      "74879/74879 [==============================] - 4s 51us/step - loss: 0.2521 - accuracy: 0.8838 - auc_roc: 0.9219 - val_loss: 0.2359 - val_accuracy: 0.8978 - val_auc_roc: 0.9223\n",
      "Epoch 44/100\n",
      "74879/74879 [==============================] - 4s 55us/step - loss: 0.2496 - accuracy: 0.8857 - auc_roc: 0.9226 - val_loss: 0.2319 - val_accuracy: 0.9023 - val_auc_roc: 0.9230\n",
      "Epoch 45/100\n",
      "74879/74879 [==============================] - 4s 54us/step - loss: 0.2487 - accuracy: 0.8856 - auc_roc: 0.9234 - val_loss: 0.2338 - val_accuracy: 0.9021 - val_auc_roc: 0.9237\n",
      "Epoch 46/100\n",
      "74879/74879 [==============================] - 4s 51us/step - loss: 0.2472 - accuracy: 0.8865 - auc_roc: 0.9241 - val_loss: 0.2324 - val_accuracy: 0.9021 - val_auc_roc: 0.9244\n",
      "Epoch 47/100\n",
      "74879/74879 [==============================] - 4s 50us/step - loss: 0.2456 - accuracy: 0.8865 - auc_roc: 0.9248 - val_loss: 0.2467 - val_accuracy: 0.8980 - val_auc_roc: 0.9251\n",
      "Epoch 48/100\n",
      "74879/74879 [==============================] - 4s 49us/step - loss: 0.2453 - accuracy: 0.8862 - auc_roc: 0.9255 - val_loss: 0.2392 - val_accuracy: 0.9023 - val_auc_roc: 0.9258\n",
      "Epoch 49/100\n",
      "74879/74879 [==============================] - 4s 53us/step - loss: 0.2424 - accuracy: 0.8868 - auc_roc: 0.9261 - val_loss: 0.2273 - val_accuracy: 0.9046 - val_auc_roc: 0.9264\n",
      "Epoch 50/100\n",
      "74879/74879 [==============================] - 4s 53us/step - loss: 0.2426 - accuracy: 0.8861 - auc_roc: 0.9267 - val_loss: 0.2325 - val_accuracy: 0.8993 - val_auc_roc: 0.9271\n",
      "Epoch 51/100\n",
      "74879/74879 [==============================] - 4s 55us/step - loss: 0.2400 - accuracy: 0.8879 - auc_roc: 0.9274 - val_loss: 0.2270 - val_accuracy: 0.9028 - val_auc_roc: 0.9277\n",
      "Epoch 52/100\n",
      "74879/74879 [==============================] - 4s 54us/step - loss: 0.2396 - accuracy: 0.8898 - auc_roc: 0.9280 - val_loss: 0.2227 - val_accuracy: 0.9061 - val_auc_roc: 0.9283\n",
      "Epoch 53/100\n",
      "74879/74879 [==============================] - 4s 52us/step - loss: 0.2386 - accuracy: 0.8876 - auc_roc: 0.9286 - val_loss: 0.2520 - val_accuracy: 0.8922 - val_auc_roc: 0.9289\n",
      "Epoch 54/100\n",
      "74879/74879 [==============================] - 4s 50us/step - loss: 0.2380 - accuracy: 0.8903 - auc_roc: 0.9292 - val_loss: 0.2300 - val_accuracy: 0.8993 - val_auc_roc: 0.9294\n",
      "Epoch 55/100\n",
      "74879/74879 [==============================] - 4s 50us/step - loss: 0.2368 - accuracy: 0.8908 - auc_roc: 0.9297 - val_loss: 0.2233 - val_accuracy: 0.9044 - val_auc_roc: 0.9300\n",
      "Epoch 56/100\n",
      "74879/74879 [==============================] - 4s 48us/step - loss: 0.2366 - accuracy: 0.8892 - auc_roc: 0.9303 - val_loss: 0.2249 - val_accuracy: 0.9056 - val_auc_roc: 0.9305\n",
      "Epoch 57/100\n",
      "74879/74879 [==============================] - 4s 49us/step - loss: 0.2333 - accuracy: 0.8913 - auc_roc: 0.9308 - val_loss: 0.2266 - val_accuracy: 0.9039 - val_auc_roc: 0.9311\n",
      "Epoch 58/100\n",
      "74879/74879 [==============================] - 4s 48us/step - loss: 0.2334 - accuracy: 0.8915 - auc_roc: 0.9313 - val_loss: 0.2295 - val_accuracy: 0.9021 - val_auc_roc: 0.9316\n",
      "Epoch 59/100\n",
      "74879/74879 [==============================] - 4s 50us/step - loss: 0.2305 - accuracy: 0.8930 - auc_roc: 0.9318 - val_loss: 0.2217 - val_accuracy: 0.9099 - val_auc_roc: 0.9321\n",
      "Epoch 60/100\n",
      "74879/74879 [==============================] - 4s 52us/step - loss: 0.2307 - accuracy: 0.8903 - auc_roc: 0.9323 - val_loss: 0.2196 - val_accuracy: 0.9094 - val_auc_roc: 0.9326\n",
      "Epoch 61/100\n",
      "74879/74879 [==============================] - 4s 51us/step - loss: 0.2322 - accuracy: 0.8900 - auc_roc: 0.9328 - val_loss: 0.2163 - val_accuracy: 0.9092 - val_auc_roc: 0.9331\n",
      "Epoch 62/100\n",
      "74879/74879 [==============================] - 4s 52us/step - loss: 0.2299 - accuracy: 0.8937 - auc_roc: 0.9333 - val_loss: 0.2225 - val_accuracy: 0.9056 - val_auc_roc: 0.9335\n",
      "Epoch 63/100\n",
      "74879/74879 [==============================] - 4s 51us/step - loss: 0.2274 - accuracy: 0.8935 - auc_roc: 0.9338 - val_loss: 0.2229 - val_accuracy: 0.9051 - val_auc_roc: 0.9340\n",
      "Epoch 64/100\n",
      "74879/74879 [==============================] - 4s 50us/step - loss: 0.2255 - accuracy: 0.8943 - auc_roc: 0.9343 - val_loss: 0.2191 - val_accuracy: 0.9120 - val_auc_roc: 0.9345\n",
      "Epoch 65/100\n",
      "74879/74879 [==============================] - 4s 48us/step - loss: 0.2238 - accuracy: 0.8945 - auc_roc: 0.9347 - val_loss: 0.2291 - val_accuracy: 0.9054 - val_auc_roc: 0.9350\n",
      "Epoch 66/100\n",
      "74879/74879 [==============================] - 4s 50us/step - loss: 0.2264 - accuracy: 0.8931 - auc_roc: 0.9352 - val_loss: 0.2233 - val_accuracy: 0.9044 - val_auc_roc: 0.9354\n",
      "Epoch 67/100\n",
      "74879/74879 [==============================] - 4s 50us/step - loss: 0.2226 - accuracy: 0.8953 - auc_roc: 0.9356 - val_loss: 0.2263 - val_accuracy: 0.9105 - val_auc_roc: 0.9358\n",
      "Epoch 68/100\n",
      "74879/74879 [==============================] - 4s 55us/step - loss: 0.2205 - accuracy: 0.8954 - auc_roc: 0.9360 - val_loss: 0.2202 - val_accuracy: 0.9077 - val_auc_roc: 0.9363\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 69/100\n",
      "74879/74879 [==============================] - 4s 51us/step - loss: 0.1964 - accuracy: 0.9063 - auc_roc: 0.9365 - val_loss: 0.2088 - val_accuracy: 0.9112 - val_auc_roc: 0.9368\n",
      "Epoch 70/100\n",
      "74879/74879 [==============================] - 4s 57us/step - loss: 0.1922 - accuracy: 0.9073 - auc_roc: 0.9371 - val_loss: 0.2109 - val_accuracy: 0.9150 - val_auc_roc: 0.9374\n",
      "Epoch 71/100\n",
      "74879/74879 [==============================] - 4s 53us/step - loss: 0.1883 - accuracy: 0.9087 - auc_roc: 0.9377 - val_loss: 0.2080 - val_accuracy: 0.9145 - val_auc_roc: 0.9380\n",
      "Epoch 72/100\n",
      "74879/74879 [==============================] - 4s 53us/step - loss: 0.1862 - accuracy: 0.9107 - auc_roc: 0.9383 - val_loss: 0.2080 - val_accuracy: 0.9155 - val_auc_roc: 0.9386\n",
      "Epoch 73/100\n",
      "74879/74879 [==============================] - 4s 52us/step - loss: 0.1864 - accuracy: 0.9091 - auc_roc: 0.9389 - val_loss: 0.2075 - val_accuracy: 0.9170 - val_auc_roc: 0.9392\n",
      "Epoch 74/100\n",
      "74879/74879 [==============================] - 4s 53us/step - loss: 0.1865 - accuracy: 0.9078 - auc_roc: 0.9394 - val_loss: 0.2076 - val_accuracy: 0.9150 - val_auc_roc: 0.9397\n",
      "Epoch 75/100\n",
      "74879/74879 [==============================] - 4s 50us/step - loss: 0.1843 - accuracy: 0.9100 - auc_roc: 0.9400 - val_loss: 0.2111 - val_accuracy: 0.9191 - val_auc_roc: 0.9403\n",
      "Epoch 76/100\n",
      "74879/74879 [==============================] - 4s 50us/step - loss: 0.1841 - accuracy: 0.9115 - auc_roc: 0.9405 - val_loss: 0.2111 - val_accuracy: 0.9143 - val_auc_roc: 0.9408\n",
      "Epoch 77/100\n",
      "74879/74879 [==============================] - 4s 51us/step - loss: 0.1843 - accuracy: 0.9099 - auc_roc: 0.9410 - val_loss: 0.2102 - val_accuracy: 0.9160 - val_auc_roc: 0.9413\n",
      "Epoch 78/100\n",
      "74879/74879 [==============================] - 4s 51us/step - loss: 0.1821 - accuracy: 0.9104 - auc_roc: 0.9415 - val_loss: 0.2101 - val_accuracy: 0.9150 - val_auc_roc: 0.9418\n",
      "Epoch 79/100\n",
      "74879/74879 [==============================] - 4s 49us/step - loss: 0.1827 - accuracy: 0.9107 - auc_roc: 0.9420 - val_loss: 0.2095 - val_accuracy: 0.9168 - val_auc_roc: 0.9423\n",
      "Epoch 80/100\n",
      "74879/74879 [==============================] - 4s 52us/step - loss: 0.1813 - accuracy: 0.9121 - auc_roc: 0.9425 - val_loss: 0.2112 - val_accuracy: 0.9150 - val_auc_roc: 0.9428\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 81/100\n",
      "74879/74879 [==============================] - 4s 52us/step - loss: 0.1782 - accuracy: 0.9140 - auc_roc: 0.9430 - val_loss: 0.2114 - val_accuracy: 0.9158 - val_auc_roc: 0.9432\n",
      "Epoch 82/100\n",
      "74879/74879 [==============================] - 4s 52us/step - loss: 0.1777 - accuracy: 0.9137 - auc_roc: 0.9435 - val_loss: 0.2106 - val_accuracy: 0.9155 - val_auc_roc: 0.9437\n",
      "Epoch 83/100\n",
      "74879/74879 [==============================] - 4s 55us/step - loss: 0.1771 - accuracy: 0.9139 - auc_roc: 0.9439 - val_loss: 0.2108 - val_accuracy: 0.9158 - val_auc_roc: 0.9442\n",
      "Epoch 84/100\n",
      "74879/74879 [==============================] - 4s 54us/step - loss: 0.1761 - accuracy: 0.9136 - auc_roc: 0.9444 - val_loss: 0.2101 - val_accuracy: 0.9160 - val_auc_roc: 0.9446\n",
      "Epoch 85/100\n",
      "74879/74879 [==============================] - 4s 55us/step - loss: 0.1780 - accuracy: 0.9138 - auc_roc: 0.9448 - val_loss: 0.2102 - val_accuracy: 0.9160 - val_auc_roc: 0.9451\n",
      "Epoch 86/100\n",
      "74879/74879 [==============================] - 4s 52us/step - loss: 0.1790 - accuracy: 0.9132 - auc_roc: 0.9453 - val_loss: 0.2104 - val_accuracy: 0.9165 - val_auc_roc: 0.9455\n",
      "Epoch 87/100\n",
      "74879/74879 [==============================] - 4s 51us/step - loss: 0.1786 - accuracy: 0.9134 - auc_roc: 0.9457 - val_loss: 0.2106 - val_accuracy: 0.9173 - val_auc_roc: 0.9459\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 00087: early stopping\n",
      "\n",
      "Fold  2\n",
      "Model: \"sequential_61\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_179 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_178 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_119 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_180 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_179 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_120 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_181 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_180 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 74880 samples, validate on 3941 samples\n",
      "Epoch 1/100\n",
      "74880/74880 [==============================] - 6s 79us/step - loss: 0.4887 - accuracy: 0.7702 - auc_roc: 0.7339 - val_loss: 0.4329 - val_accuracy: 0.8016 - val_auc_roc: 0.7932\n",
      "Epoch 2/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.4412 - accuracy: 0.7973 - auc_roc: 0.8068 - val_loss: 0.4155 - val_accuracy: 0.8084 - val_auc_roc: 0.8167\n",
      "Epoch 3/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.4214 - accuracy: 0.8081 - auc_roc: 0.8239 - val_loss: 0.3911 - val_accuracy: 0.8209 - val_auc_roc: 0.8296\n",
      "Epoch 4/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.4058 - accuracy: 0.8165 - auc_roc: 0.8347 - val_loss: 0.3740 - val_accuracy: 0.8371 - val_auc_roc: 0.8390\n",
      "Epoch 5/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.3932 - accuracy: 0.8234 - auc_roc: 0.8431 - val_loss: 0.3599 - val_accuracy: 0.8386 - val_auc_roc: 0.8466\n",
      "Epoch 6/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.3831 - accuracy: 0.8275 - auc_roc: 0.8500 - val_loss: 0.3503 - val_accuracy: 0.8437 - val_auc_roc: 0.8528\n",
      "Epoch 7/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.3702 - accuracy: 0.8346 - auc_roc: 0.8557 - val_loss: 0.3432 - val_accuracy: 0.8472 - val_auc_roc: 0.8584\n",
      "Epoch 8/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.3627 - accuracy: 0.8390 - auc_roc: 0.8610 - val_loss: 0.3346 - val_accuracy: 0.8500 - val_auc_roc: 0.8632\n",
      "Epoch 9/100\n",
      "74880/74880 [==============================] - 4s 48us/step - loss: 0.3556 - accuracy: 0.8420 - auc_roc: 0.8653 - val_loss: 0.3243 - val_accuracy: 0.8536 - val_auc_roc: 0.8675\n",
      "Epoch 10/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.3502 - accuracy: 0.8427 - auc_roc: 0.8694 - val_loss: 0.3208 - val_accuracy: 0.8617 - val_auc_roc: 0.8712\n",
      "Epoch 11/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.3422 - accuracy: 0.8486 - auc_roc: 0.8730 - val_loss: 0.3116 - val_accuracy: 0.8637 - val_auc_roc: 0.8747\n",
      "Epoch 12/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.3360 - accuracy: 0.8513 - auc_roc: 0.8764 - val_loss: 0.3082 - val_accuracy: 0.8645 - val_auc_roc: 0.8779\n",
      "Epoch 13/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.3315 - accuracy: 0.8531 - auc_roc: 0.8795 - val_loss: 0.3029 - val_accuracy: 0.8645 - val_auc_roc: 0.8809\n",
      "Epoch 14/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.3257 - accuracy: 0.8552 - auc_roc: 0.8822 - val_loss: 0.3074 - val_accuracy: 0.8642 - val_auc_roc: 0.8836\n",
      "Epoch 15/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.3236 - accuracy: 0.8567 - auc_roc: 0.8849 - val_loss: 0.3016 - val_accuracy: 0.8630 - val_auc_roc: 0.8861\n",
      "Epoch 16/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.3173 - accuracy: 0.8592 - auc_roc: 0.8873 - val_loss: 0.2927 - val_accuracy: 0.8721 - val_auc_roc: 0.8884\n",
      "Epoch 17/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.3128 - accuracy: 0.8605 - auc_roc: 0.8896 - val_loss: 0.2915 - val_accuracy: 0.8729 - val_auc_roc: 0.8907\n",
      "Epoch 18/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.3101 - accuracy: 0.8609 - auc_roc: 0.8917 - val_loss: 0.2871 - val_accuracy: 0.8752 - val_auc_roc: 0.8927\n",
      "Epoch 19/100\n",
      "74880/74880 [==============================] - 4s 57us/step - loss: 0.3059 - accuracy: 0.8636 - auc_roc: 0.8937 - val_loss: 0.2818 - val_accuracy: 0.8764 - val_auc_roc: 0.8947\n",
      "Epoch 20/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.3033 - accuracy: 0.8638 - auc_roc: 0.8956 - val_loss: 0.2863 - val_accuracy: 0.8734 - val_auc_roc: 0.8965\n",
      "Epoch 21/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.2973 - accuracy: 0.8676 - auc_roc: 0.8974 - val_loss: 0.2822 - val_accuracy: 0.8736 - val_auc_roc: 0.8983\n",
      "Epoch 22/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.2953 - accuracy: 0.8677 - auc_roc: 0.8991 - val_loss: 0.2711 - val_accuracy: 0.8810 - val_auc_roc: 0.8999\n",
      "Epoch 23/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.2930 - accuracy: 0.8700 - auc_roc: 0.9007 - val_loss: 0.2730 - val_accuracy: 0.8777 - val_auc_roc: 0.9015\n",
      "Epoch 24/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.2923 - accuracy: 0.8696 - auc_roc: 0.9022 - val_loss: 0.2791 - val_accuracy: 0.8744 - val_auc_roc: 0.9029\n",
      "Epoch 25/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.2893 - accuracy: 0.8702 - auc_roc: 0.9037 - val_loss: 0.2756 - val_accuracy: 0.8759 - val_auc_roc: 0.9043\n",
      "Epoch 26/100\n",
      "74880/74880 [==============================] - 5s 61us/step - loss: 0.2851 - accuracy: 0.8727 - auc_roc: 0.9050 - val_loss: 0.2746 - val_accuracy: 0.8754 - val_auc_roc: 0.9057\n",
      "Epoch 27/100\n",
      "74880/74880 [==============================] - 5s 61us/step - loss: 0.2826 - accuracy: 0.8719 - auc_roc: 0.9063 - val_loss: 0.2659 - val_accuracy: 0.8815 - val_auc_roc: 0.9070\n",
      "Epoch 28/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.2815 - accuracy: 0.8730 - auc_roc: 0.9076 - val_loss: 0.2727 - val_accuracy: 0.8762 - val_auc_roc: 0.9082\n",
      "Epoch 29/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.2755 - accuracy: 0.8755 - auc_roc: 0.9088 - val_loss: 0.2633 - val_accuracy: 0.8843 - val_auc_roc: 0.9094\n",
      "Epoch 30/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.2763 - accuracy: 0.8751 - auc_roc: 0.9100 - val_loss: 0.2628 - val_accuracy: 0.8891 - val_auc_roc: 0.9106\n",
      "Epoch 31/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.2738 - accuracy: 0.8763 - auc_roc: 0.9111 - val_loss: 0.2777 - val_accuracy: 0.8749 - val_auc_roc: 0.9117\n",
      "Epoch 32/100\n",
      "74880/74880 [==============================] - 4s 60us/step - loss: 0.2739 - accuracy: 0.8749 - auc_roc: 0.9122 - val_loss: 0.2576 - val_accuracy: 0.8922 - val_auc_roc: 0.9127\n",
      "Epoch 33/100\n",
      "74880/74880 [==============================] - 5s 62us/step - loss: 0.2703 - accuracy: 0.8772 - auc_roc: 0.9132 - val_loss: 0.2541 - val_accuracy: 0.8896 - val_auc_roc: 0.9137\n",
      "Epoch 34/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.2664 - accuracy: 0.8790 - auc_roc: 0.9142 - val_loss: 0.2492 - val_accuracy: 0.8932 - val_auc_roc: 0.9147\n",
      "Epoch 35/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.2670 - accuracy: 0.8783 - auc_roc: 0.9152 - val_loss: 0.2577 - val_accuracy: 0.8906 - val_auc_roc: 0.9157\n",
      "Epoch 36/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.2645 - accuracy: 0.8808 - auc_roc: 0.9161 - val_loss: 0.2615 - val_accuracy: 0.8820 - val_auc_roc: 0.9166\n",
      "Epoch 37/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.2632 - accuracy: 0.8805 - auc_roc: 0.9170 - val_loss: 0.2555 - val_accuracy: 0.8884 - val_auc_roc: 0.9175\n",
      "Epoch 38/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.2595 - accuracy: 0.8812 - auc_roc: 0.9179 - val_loss: 0.2559 - val_accuracy: 0.8914 - val_auc_roc: 0.9183\n",
      "Epoch 39/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.2596 - accuracy: 0.8817 - auc_roc: 0.9188 - val_loss: 0.2525 - val_accuracy: 0.8922 - val_auc_roc: 0.9192\n",
      "Epoch 40/100\n",
      "74880/74880 [==============================] - 4s 57us/step - loss: 0.2547 - accuracy: 0.8831 - auc_roc: 0.9196 - val_loss: 0.2550 - val_accuracy: 0.8932 - val_auc_roc: 0.9200\n",
      "Epoch 41/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.2538 - accuracy: 0.8825 - auc_roc: 0.9204 - val_loss: 0.2537 - val_accuracy: 0.8896 - val_auc_roc: 0.9208\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 42/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.2278 - accuracy: 0.8955 - auc_roc: 0.9214 - val_loss: 0.2372 - val_accuracy: 0.8955 - val_auc_roc: 0.9219\n",
      "Epoch 43/100\n",
      "74880/74880 [==============================] - 4s 57us/step - loss: 0.2221 - accuracy: 0.8981 - auc_roc: 0.9225 - val_loss: 0.2366 - val_accuracy: 0.8970 - val_auc_roc: 0.9230\n",
      "Epoch 44/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.2188 - accuracy: 0.8979 - auc_roc: 0.9235 - val_loss: 0.2374 - val_accuracy: 0.8972 - val_auc_roc: 0.9241\n",
      "Epoch 45/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.2193 - accuracy: 0.8986 - auc_roc: 0.9246 - val_loss: 0.2356 - val_accuracy: 0.8965 - val_auc_roc: 0.9251\n",
      "Epoch 46/100\n",
      "74880/74880 [==============================] - 4s 60us/step - loss: 0.2161 - accuracy: 0.8984 - auc_roc: 0.9256 - val_loss: 0.2379 - val_accuracy: 0.8965 - val_auc_roc: 0.9261\n",
      "Epoch 47/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.2154 - accuracy: 0.8983 - auc_roc: 0.9266 - val_loss: 0.2364 - val_accuracy: 0.8988 - val_auc_roc: 0.9270\n",
      "Epoch 48/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.2138 - accuracy: 0.8996 - auc_roc: 0.9275 - val_loss: 0.2362 - val_accuracy: 0.8962 - val_auc_roc: 0.9279\n",
      "Epoch 49/100\n",
      "74880/74880 [==============================] - 5s 60us/step - loss: 0.2152 - accuracy: 0.8977 - auc_roc: 0.9284 - val_loss: 0.2354 - val_accuracy: 0.8998 - val_auc_roc: 0.9288\n",
      "Epoch 50/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.2119 - accuracy: 0.8997 - auc_roc: 0.9292 - val_loss: 0.2369 - val_accuracy: 0.8985 - val_auc_roc: 0.9297\n",
      "Epoch 51/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.2136 - accuracy: 0.8991 - auc_roc: 0.9301 - val_loss: 0.2329 - val_accuracy: 0.8970 - val_auc_roc: 0.9305\n",
      "Epoch 52/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.2118 - accuracy: 0.9001 - auc_roc: 0.9308 - val_loss: 0.2326 - val_accuracy: 0.8990 - val_auc_roc: 0.9312\n",
      "Epoch 53/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2120 - accuracy: 0.9007 - auc_roc: 0.9316 - val_loss: 0.2353 - val_accuracy: 0.8965 - val_auc_roc: 0.9320\n",
      "Epoch 54/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.2101 - accuracy: 0.9013 - auc_roc: 0.9323 - val_loss: 0.2338 - val_accuracy: 0.8998 - val_auc_roc: 0.9327\n",
      "Epoch 55/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.2075 - accuracy: 0.9018 - auc_roc: 0.9331 - val_loss: 0.2380 - val_accuracy: 0.8985 - val_auc_roc: 0.9334\n",
      "Epoch 56/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.2092 - accuracy: 0.9002 - auc_roc: 0.9338 - val_loss: 0.2333 - val_accuracy: 0.8977 - val_auc_roc: 0.9341\n",
      "Epoch 57/100\n",
      "74880/74880 [==============================] - 5s 61us/step - loss: 0.2085 - accuracy: 0.9010 - auc_roc: 0.9344 - val_loss: 0.2343 - val_accuracy: 0.8990 - val_auc_roc: 0.9348\n",
      "Epoch 58/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.2084 - accuracy: 0.9005 - auc_roc: 0.9351 - val_loss: 0.2362 - val_accuracy: 0.8972 - val_auc_roc: 0.9354\n",
      "Epoch 59/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.2079 - accuracy: 0.9016 - auc_roc: 0.9357 - val_loss: 0.2359 - val_accuracy: 0.8998 - val_auc_roc: 0.9360\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 60/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.2066 - accuracy: 0.9020 - auc_roc: 0.9363 - val_loss: 0.2341 - val_accuracy: 0.9018 - val_auc_roc: 0.9366\n",
      "Epoch 61/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.2041 - accuracy: 0.9029 - auc_roc: 0.9369 - val_loss: 0.2337 - val_accuracy: 0.9000 - val_auc_roc: 0.9372\n",
      "Epoch 62/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.2045 - accuracy: 0.9032 - auc_roc: 0.9375 - val_loss: 0.2339 - val_accuracy: 0.9010 - val_auc_roc: 0.9377\n",
      "Epoch 63/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.2033 - accuracy: 0.9029 - auc_roc: 0.9380 - val_loss: 0.2339 - val_accuracy: 0.9003 - val_auc_roc: 0.9383\n",
      "Epoch 64/100\n",
      "74880/74880 [==============================] - 4s 57us/step - loss: 0.2026 - accuracy: 0.9027 - auc_roc: 0.9386 - val_loss: 0.2345 - val_accuracy: 0.8995 - val_auc_roc: 0.9388\n",
      "Epoch 65/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.2033 - accuracy: 0.9038 - auc_roc: 0.9391 - val_loss: 0.2342 - val_accuracy: 0.8995 - val_auc_roc: 0.9394\n",
      "Epoch 66/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.2019 - accuracy: 0.9038 - auc_roc: 0.9396 - val_loss: 0.2344 - val_accuracy: 0.9003 - val_auc_roc: 0.9399\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 00066: early stopping\n",
      "\n",
      "Fold  3\n",
      "Model: \"sequential_62\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_182 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_181 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_121 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_183 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_182 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_122 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_184 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_183 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 74880 samples, validate on 3941 samples\n",
      "Epoch 1/100\n",
      "74880/74880 [==============================] - 7s 87us/step - loss: 0.4932 - accuracy: 0.7691 - auc_roc: 0.7319 - val_loss: 0.4353 - val_accuracy: 0.7947 - val_auc_roc: 0.7899\n",
      "Epoch 2/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.4442 - accuracy: 0.7984 - auc_roc: 0.8033 - val_loss: 0.4030 - val_accuracy: 0.8201 - val_auc_roc: 0.8142\n",
      "Epoch 3/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.4244 - accuracy: 0.8063 - auc_roc: 0.8214 - val_loss: 0.3846 - val_accuracy: 0.8282 - val_auc_roc: 0.8274\n",
      "Epoch 4/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.4086 - accuracy: 0.8138 - auc_roc: 0.8326 - val_loss: 0.3874 - val_accuracy: 0.8244 - val_auc_roc: 0.8372\n",
      "Epoch 5/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.3929 - accuracy: 0.8220 - auc_roc: 0.8411 - val_loss: 0.3714 - val_accuracy: 0.8404 - val_auc_roc: 0.8451\n",
      "Epoch 6/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.3841 - accuracy: 0.8252 - auc_roc: 0.8483 - val_loss: 0.3611 - val_accuracy: 0.8361 - val_auc_roc: 0.8512\n",
      "Epoch 7/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.3735 - accuracy: 0.8310 - auc_roc: 0.8540 - val_loss: 0.3426 - val_accuracy: 0.8493 - val_auc_roc: 0.8567\n",
      "Epoch 8/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.3659 - accuracy: 0.8356 - auc_roc: 0.8591 - val_loss: 0.3536 - val_accuracy: 0.8406 - val_auc_roc: 0.8614\n",
      "Epoch 9/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.3579 - accuracy: 0.8380 - auc_roc: 0.8635 - val_loss: 0.3265 - val_accuracy: 0.8554 - val_auc_roc: 0.8656\n",
      "Epoch 10/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.3499 - accuracy: 0.8406 - auc_roc: 0.8677 - val_loss: 0.3257 - val_accuracy: 0.8546 - val_auc_roc: 0.8697\n",
      "Epoch 11/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.3413 - accuracy: 0.8460 - auc_roc: 0.8715 - val_loss: 0.3402 - val_accuracy: 0.8541 - val_auc_roc: 0.8733\n",
      "Epoch 12/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.3376 - accuracy: 0.8478 - auc_roc: 0.8750 - val_loss: 0.3183 - val_accuracy: 0.8615 - val_auc_roc: 0.8765\n",
      "Epoch 13/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.3307 - accuracy: 0.8496 - auc_roc: 0.8780 - val_loss: 0.3083 - val_accuracy: 0.8640 - val_auc_roc: 0.8795\n",
      "Epoch 14/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.3285 - accuracy: 0.8509 - auc_roc: 0.8809 - val_loss: 0.3264 - val_accuracy: 0.8513 - val_auc_roc: 0.8822\n",
      "Epoch 15/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.3223 - accuracy: 0.8538 - auc_roc: 0.8835 - val_loss: 0.3072 - val_accuracy: 0.8630 - val_auc_roc: 0.8848\n",
      "Epoch 16/100\n",
      "74880/74880 [==============================] - 4s 57us/step - loss: 0.3212 - accuracy: 0.8537 - auc_roc: 0.8860 - val_loss: 0.3045 - val_accuracy: 0.8688 - val_auc_roc: 0.8871\n",
      "Epoch 17/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.3123 - accuracy: 0.8572 - auc_roc: 0.8883 - val_loss: 0.2985 - val_accuracy: 0.8683 - val_auc_roc: 0.8894\n",
      "Epoch 18/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.3121 - accuracy: 0.8579 - auc_roc: 0.8905 - val_loss: 0.3046 - val_accuracy: 0.8688 - val_auc_roc: 0.8914\n",
      "Epoch 19/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.3066 - accuracy: 0.8602 - auc_roc: 0.8924 - val_loss: 0.3046 - val_accuracy: 0.8660 - val_auc_roc: 0.8934\n",
      "Epoch 20/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.3049 - accuracy: 0.8614 - auc_roc: 0.8944 - val_loss: 0.2976 - val_accuracy: 0.8655 - val_auc_roc: 0.8952\n",
      "Epoch 21/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.3032 - accuracy: 0.8603 - auc_roc: 0.8961 - val_loss: 0.2967 - val_accuracy: 0.8683 - val_auc_roc: 0.8969\n",
      "Epoch 22/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.2972 - accuracy: 0.8632 - auc_roc: 0.8977 - val_loss: 0.2865 - val_accuracy: 0.8785 - val_auc_roc: 0.8986\n",
      "Epoch 23/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.2982 - accuracy: 0.8637 - auc_roc: 0.8993 - val_loss: 0.2896 - val_accuracy: 0.8741 - val_auc_roc: 0.9001\n",
      "Epoch 24/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2919 - accuracy: 0.8663 - auc_roc: 0.9008 - val_loss: 0.2971 - val_accuracy: 0.8721 - val_auc_roc: 0.9016\n",
      "Epoch 25/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.2902 - accuracy: 0.8662 - auc_roc: 0.9023 - val_loss: 0.3002 - val_accuracy: 0.8655 - val_auc_roc: 0.9030\n",
      "Epoch 26/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.2876 - accuracy: 0.8666 - auc_roc: 0.9036 - val_loss: 0.2876 - val_accuracy: 0.8744 - val_auc_roc: 0.9043\n",
      "Epoch 27/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.2844 - accuracy: 0.8700 - auc_roc: 0.9050 - val_loss: 0.2752 - val_accuracy: 0.8868 - val_auc_roc: 0.9056\n",
      "Epoch 28/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2800 - accuracy: 0.8715 - auc_roc: 0.9063 - val_loss: 0.2765 - val_accuracy: 0.8825 - val_auc_roc: 0.9069\n",
      "Epoch 29/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2792 - accuracy: 0.8717 - auc_roc: 0.9076 - val_loss: 0.2806 - val_accuracy: 0.8749 - val_auc_roc: 0.9081\n",
      "Epoch 30/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.2777 - accuracy: 0.8710 - auc_roc: 0.9087 - val_loss: 0.2771 - val_accuracy: 0.8805 - val_auc_roc: 0.9093\n",
      "Epoch 31/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.2744 - accuracy: 0.8727 - auc_roc: 0.9099 - val_loss: 0.2897 - val_accuracy: 0.8772 - val_auc_roc: 0.9104\n",
      "Epoch 32/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2733 - accuracy: 0.8752 - auc_roc: 0.9110 - val_loss: 0.2787 - val_accuracy: 0.8800 - val_auc_roc: 0.9115\n",
      "Epoch 33/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.2730 - accuracy: 0.8738 - auc_roc: 0.9120 - val_loss: 0.2907 - val_accuracy: 0.8724 - val_auc_roc: 0.9125\n",
      "Epoch 34/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.2703 - accuracy: 0.8748 - auc_roc: 0.9130 - val_loss: 0.2909 - val_accuracy: 0.8747 - val_auc_roc: 0.9135\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 35/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.2447 - accuracy: 0.8892 - auc_roc: 0.9141 - val_loss: 0.2623 - val_accuracy: 0.8957 - val_auc_roc: 0.9148\n",
      "Epoch 36/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.2378 - accuracy: 0.8917 - auc_roc: 0.9154 - val_loss: 0.2633 - val_accuracy: 0.8919 - val_auc_roc: 0.9161\n",
      "Epoch 37/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2346 - accuracy: 0.8941 - auc_roc: 0.9168 - val_loss: 0.2628 - val_accuracy: 0.8922 - val_auc_roc: 0.9174\n",
      "Epoch 38/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.2330 - accuracy: 0.8957 - auc_roc: 0.9180 - val_loss: 0.2629 - val_accuracy: 0.8939 - val_auc_roc: 0.9187\n",
      "Epoch 39/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.2320 - accuracy: 0.8954 - auc_roc: 0.9192 - val_loss: 0.2613 - val_accuracy: 0.8922 - val_auc_roc: 0.9198\n",
      "Epoch 40/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.2307 - accuracy: 0.8948 - auc_roc: 0.9204 - val_loss: 0.2618 - val_accuracy: 0.8944 - val_auc_roc: 0.9210\n",
      "Epoch 41/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.2319 - accuracy: 0.8939 - auc_roc: 0.9215 - val_loss: 0.2682 - val_accuracy: 0.8929 - val_auc_roc: 0.9220\n",
      "Epoch 42/100\n",
      "74880/74880 [==============================] - 4s 57us/step - loss: 0.2285 - accuracy: 0.8964 - auc_roc: 0.9225 - val_loss: 0.2632 - val_accuracy: 0.8927 - val_auc_roc: 0.9230\n",
      "Epoch 43/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.2270 - accuracy: 0.8977 - auc_roc: 0.9235 - val_loss: 0.2614 - val_accuracy: 0.8960 - val_auc_roc: 0.9240\n",
      "Epoch 44/100\n",
      "74880/74880 [==============================] - 4s 60us/step - loss: 0.2248 - accuracy: 0.8988 - auc_roc: 0.9245 - val_loss: 0.2651 - val_accuracy: 0.8924 - val_auc_roc: 0.9250\n",
      "Epoch 45/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.2278 - accuracy: 0.8963 - auc_roc: 0.9254 - val_loss: 0.2641 - val_accuracy: 0.8960 - val_auc_roc: 0.9259\n",
      "Epoch 46/100\n",
      "74880/74880 [==============================] - 4s 60us/step - loss: 0.2232 - accuracy: 0.8992 - auc_roc: 0.9263 - val_loss: 0.2614 - val_accuracy: 0.8957 - val_auc_roc: 0.9268\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 47/100\n",
      "74880/74880 [==============================] - 4s 60us/step - loss: 0.2236 - accuracy: 0.8987 - auc_roc: 0.9272 - val_loss: 0.2625 - val_accuracy: 0.8950 - val_auc_roc: 0.9276\n",
      "Epoch 48/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.2234 - accuracy: 0.8984 - auc_roc: 0.9280 - val_loss: 0.2611 - val_accuracy: 0.8955 - val_auc_roc: 0.9284\n",
      "Epoch 49/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.2219 - accuracy: 0.8998 - auc_roc: 0.9288 - val_loss: 0.2617 - val_accuracy: 0.8960 - val_auc_roc: 0.9292\n",
      "Epoch 50/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.2198 - accuracy: 0.9010 - auc_roc: 0.9296 - val_loss: 0.2618 - val_accuracy: 0.8947 - val_auc_roc: 0.9300\n",
      "Epoch 51/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.2208 - accuracy: 0.9005 - auc_roc: 0.9303 - val_loss: 0.2614 - val_accuracy: 0.8957 - val_auc_roc: 0.9307\n",
      "Epoch 52/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.2232 - accuracy: 0.8986 - auc_roc: 0.9310 - val_loss: 0.2622 - val_accuracy: 0.8960 - val_auc_roc: 0.9313\n",
      "Epoch 53/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.2210 - accuracy: 0.8996 - auc_roc: 0.9317 - val_loss: 0.2618 - val_accuracy: 0.8942 - val_auc_roc: 0.9320\n",
      "Epoch 54/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.2211 - accuracy: 0.8996 - auc_roc: 0.9323 - val_loss: 0.2620 - val_accuracy: 0.8950 - val_auc_roc: 0.9326\n",
      "Epoch 55/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.2205 - accuracy: 0.9006 - auc_roc: 0.9329 - val_loss: 0.2615 - val_accuracy: 0.8952 - val_auc_roc: 0.9332\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 56/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.2216 - accuracy: 0.8997 - auc_roc: 0.9335 - val_loss: 0.2615 - val_accuracy: 0.8955 - val_auc_roc: 0.9338\n",
      "Epoch 57/100\n",
      "74880/74880 [==============================] - 4s 57us/step - loss: 0.2221 - accuracy: 0.9004 - auc_roc: 0.9341 - val_loss: 0.2615 - val_accuracy: 0.8955 - val_auc_roc: 0.9344\n",
      "Epoch 58/100\n",
      "74880/74880 [==============================] - 4s 57us/step - loss: 0.2214 - accuracy: 0.9007 - auc_roc: 0.9346 - val_loss: 0.2616 - val_accuracy: 0.8950 - val_auc_roc: 0.9349\n",
      "Epoch 59/100\n",
      "74880/74880 [==============================] - 4s 57us/step - loss: 0.2226 - accuracy: 0.8992 - auc_roc: 0.9351 - val_loss: 0.2616 - val_accuracy: 0.8955 - val_auc_roc: 0.9354\n",
      "Epoch 60/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.2217 - accuracy: 0.9001 - auc_roc: 0.9356 - val_loss: 0.2618 - val_accuracy: 0.8950 - val_auc_roc: 0.9359\n",
      "Epoch 61/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.2191 - accuracy: 0.9015 - auc_roc: 0.9361 - val_loss: 0.2619 - val_accuracy: 0.8950 - val_auc_roc: 0.9364\n",
      "Epoch 62/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.2193 - accuracy: 0.9024 - auc_roc: 0.9366 - val_loss: 0.2618 - val_accuracy: 0.8950 - val_auc_roc: 0.9369\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 00062: early stopping\n",
      "\n",
      "Fold  4\n",
      "Model: \"sequential_63\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_185 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_184 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_123 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_186 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_185 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_124 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_187 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_186 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 74880 samples, validate on 3941 samples\n",
      "Epoch 1/100\n",
      "74880/74880 [==============================] - 7s 89us/step - loss: 0.4890 - accuracy: 0.7714 - auc_roc: 0.7454 - val_loss: 0.4308 - val_accuracy: 0.8028 - val_auc_roc: 0.7936\n",
      "Epoch 2/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.4397 - accuracy: 0.7986 - auc_roc: 0.8076 - val_loss: 0.4067 - val_accuracy: 0.8138 - val_auc_roc: 0.8178\n",
      "Epoch 3/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.4209 - accuracy: 0.8088 - auc_roc: 0.8245 - val_loss: 0.3914 - val_accuracy: 0.8244 - val_auc_roc: 0.8306\n",
      "Epoch 4/100\n",
      "74880/74880 [==============================] - 4s 60us/step - loss: 0.4051 - accuracy: 0.8161 - auc_roc: 0.8357 - val_loss: 0.3948 - val_accuracy: 0.8201 - val_auc_roc: 0.8396\n",
      "Epoch 5/100\n",
      "74880/74880 [==============================] - 4s 60us/step - loss: 0.3945 - accuracy: 0.8224 - auc_roc: 0.8431 - val_loss: 0.3634 - val_accuracy: 0.8300 - val_auc_roc: 0.8466\n",
      "Epoch 6/100\n",
      "74880/74880 [==============================] - 4s 60us/step - loss: 0.3809 - accuracy: 0.8298 - auc_roc: 0.8500 - val_loss: 0.3553 - val_accuracy: 0.8401 - val_auc_roc: 0.8530\n",
      "Epoch 7/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.3692 - accuracy: 0.8337 - auc_roc: 0.8559 - val_loss: 0.3677 - val_accuracy: 0.8371 - val_auc_roc: 0.8586\n",
      "Epoch 8/100\n",
      "74880/74880 [==============================] - 4s 60us/step - loss: 0.3607 - accuracy: 0.8367 - auc_roc: 0.8612 - val_loss: 0.3345 - val_accuracy: 0.8533 - val_auc_roc: 0.8635\n",
      "Epoch 9/100\n",
      "74880/74880 [==============================] - 5s 63us/step - loss: 0.3541 - accuracy: 0.8425 - auc_roc: 0.8658 - val_loss: 0.3308 - val_accuracy: 0.8556 - val_auc_roc: 0.8678\n",
      "Epoch 10/100\n",
      "74880/74880 [==============================] - 5s 62us/step - loss: 0.3501 - accuracy: 0.8436 - auc_roc: 0.8697 - val_loss: 0.3285 - val_accuracy: 0.8551 - val_auc_roc: 0.8714\n",
      "Epoch 11/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.3400 - accuracy: 0.8490 - auc_roc: 0.8733 - val_loss: 0.3174 - val_accuracy: 0.8620 - val_auc_roc: 0.8750\n",
      "Epoch 12/100\n",
      "74880/74880 [==============================] - 4s 57us/step - loss: 0.3325 - accuracy: 0.8506 - auc_roc: 0.8767 - val_loss: 0.3130 - val_accuracy: 0.8625 - val_auc_roc: 0.8783\n",
      "Epoch 13/100\n",
      "74880/74880 [==============================] - 4s 60us/step - loss: 0.3248 - accuracy: 0.8550 - auc_roc: 0.8799 - val_loss: 0.3053 - val_accuracy: 0.8665 - val_auc_roc: 0.8815\n",
      "Epoch 14/100\n",
      "74880/74880 [==============================] - 4s 57us/step - loss: 0.3215 - accuracy: 0.8565 - auc_roc: 0.8830 - val_loss: 0.3038 - val_accuracy: 0.8655 - val_auc_roc: 0.8844\n",
      "Epoch 15/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.3155 - accuracy: 0.8582 - auc_roc: 0.8858 - val_loss: 0.2910 - val_accuracy: 0.8716 - val_auc_roc: 0.8871\n",
      "Epoch 16/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.3120 - accuracy: 0.8615 - auc_roc: 0.8884 - val_loss: 0.2914 - val_accuracy: 0.8741 - val_auc_roc: 0.8896\n",
      "Epoch 17/100\n",
      "74880/74880 [==============================] - 4s 57us/step - loss: 0.3071 - accuracy: 0.8630 - auc_roc: 0.8908 - val_loss: 0.2919 - val_accuracy: 0.8678 - val_auc_roc: 0.8919\n",
      "Epoch 18/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.3061 - accuracy: 0.8628 - auc_roc: 0.8930 - val_loss: 0.2838 - val_accuracy: 0.8790 - val_auc_roc: 0.8940\n",
      "Epoch 19/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.2968 - accuracy: 0.8667 - auc_roc: 0.8950 - val_loss: 0.2820 - val_accuracy: 0.8741 - val_auc_roc: 0.8961\n",
      "Epoch 20/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.2956 - accuracy: 0.8676 - auc_roc: 0.8970 - val_loss: 0.2832 - val_accuracy: 0.8714 - val_auc_roc: 0.8980\n",
      "Epoch 21/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.2937 - accuracy: 0.8664 - auc_roc: 0.8989 - val_loss: 0.2790 - val_accuracy: 0.8790 - val_auc_roc: 0.8998\n",
      "Epoch 22/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.2892 - accuracy: 0.8699 - auc_roc: 0.9007 - val_loss: 0.2725 - val_accuracy: 0.8785 - val_auc_roc: 0.9015\n",
      "Epoch 23/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.2884 - accuracy: 0.8705 - auc_roc: 0.9023 - val_loss: 0.2750 - val_accuracy: 0.8795 - val_auc_roc: 0.9031\n",
      "Epoch 24/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.2856 - accuracy: 0.8729 - auc_roc: 0.9039 - val_loss: 0.2764 - val_accuracy: 0.8731 - val_auc_roc: 0.9046\n",
      "Epoch 25/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.2853 - accuracy: 0.8720 - auc_roc: 0.9053 - val_loss: 0.2768 - val_accuracy: 0.8792 - val_auc_roc: 0.9060\n",
      "Epoch 26/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.2804 - accuracy: 0.8727 - auc_roc: 0.9067 - val_loss: 0.2731 - val_accuracy: 0.8835 - val_auc_roc: 0.9073\n",
      "Epoch 27/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.2752 - accuracy: 0.8774 - auc_roc: 0.9080 - val_loss: 0.2641 - val_accuracy: 0.8835 - val_auc_roc: 0.9087\n",
      "Epoch 28/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.2729 - accuracy: 0.8774 - auc_roc: 0.9094 - val_loss: 0.2771 - val_accuracy: 0.8825 - val_auc_roc: 0.9100\n",
      "Epoch 29/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.2732 - accuracy: 0.8776 - auc_roc: 0.9106 - val_loss: 0.2678 - val_accuracy: 0.8790 - val_auc_roc: 0.9112\n",
      "Epoch 30/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.2696 - accuracy: 0.8782 - auc_roc: 0.9118 - val_loss: 0.2595 - val_accuracy: 0.8866 - val_auc_roc: 0.9124\n",
      "Epoch 31/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.2687 - accuracy: 0.8782 - auc_roc: 0.9129 - val_loss: 0.2740 - val_accuracy: 0.8810 - val_auc_roc: 0.9135\n",
      "Epoch 32/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.2695 - accuracy: 0.8767 - auc_roc: 0.9140 - val_loss: 0.2571 - val_accuracy: 0.8901 - val_auc_roc: 0.9145\n",
      "Epoch 33/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.2617 - accuracy: 0.8808 - auc_roc: 0.9150 - val_loss: 0.2519 - val_accuracy: 0.8939 - val_auc_roc: 0.9156\n",
      "Epoch 34/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.2620 - accuracy: 0.8810 - auc_roc: 0.9161 - val_loss: 0.2516 - val_accuracy: 0.8919 - val_auc_roc: 0.9166\n",
      "Epoch 35/100\n",
      "74880/74880 [==============================] - 4s 57us/step - loss: 0.2616 - accuracy: 0.8811 - auc_roc: 0.9171 - val_loss: 0.2475 - val_accuracy: 0.8896 - val_auc_roc: 0.9175\n",
      "Epoch 36/100\n",
      "74880/74880 [==============================] - 4s 57us/step - loss: 0.2580 - accuracy: 0.8826 - auc_roc: 0.9180 - val_loss: 0.2555 - val_accuracy: 0.8894 - val_auc_roc: 0.9185\n",
      "Epoch 37/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.2596 - accuracy: 0.8822 - auc_roc: 0.9189 - val_loss: 0.2506 - val_accuracy: 0.8927 - val_auc_roc: 0.9193\n",
      "Epoch 38/100\n",
      "74880/74880 [==============================] - 5s 61us/step - loss: 0.2545 - accuracy: 0.8850 - auc_roc: 0.9198 - val_loss: 0.2586 - val_accuracy: 0.8858 - val_auc_roc: 0.9202\n",
      "Epoch 39/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.2528 - accuracy: 0.8843 - auc_roc: 0.9206 - val_loss: 0.2530 - val_accuracy: 0.8929 - val_auc_roc: 0.9211\n",
      "Epoch 40/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.2513 - accuracy: 0.8840 - auc_roc: 0.9215 - val_loss: 0.2435 - val_accuracy: 0.8957 - val_auc_roc: 0.9219\n",
      "Epoch 41/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.2491 - accuracy: 0.8863 - auc_roc: 0.9223 - val_loss: 0.2426 - val_accuracy: 0.8957 - val_auc_roc: 0.9227\n",
      "Epoch 42/100\n",
      "74880/74880 [==============================] - 4s 60us/step - loss: 0.2476 - accuracy: 0.8870 - auc_roc: 0.9231 - val_loss: 0.2402 - val_accuracy: 0.8990 - val_auc_roc: 0.9235\n",
      "Epoch 43/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.2438 - accuracy: 0.8880 - auc_roc: 0.9239 - val_loss: 0.2466 - val_accuracy: 0.8934 - val_auc_roc: 0.9243\n",
      "Epoch 44/100\n",
      "74880/74880 [==============================] - 5s 61us/step - loss: 0.2430 - accuracy: 0.8888 - auc_roc: 0.9246 - val_loss: 0.2616 - val_accuracy: 0.8881 - val_auc_roc: 0.9250\n",
      "Epoch 45/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.2413 - accuracy: 0.8896 - auc_roc: 0.9254 - val_loss: 0.2495 - val_accuracy: 0.8914 - val_auc_roc: 0.9257\n",
      "Epoch 46/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.2405 - accuracy: 0.8889 - auc_roc: 0.9261 - val_loss: 0.2460 - val_accuracy: 0.8952 - val_auc_roc: 0.9264\n",
      "Epoch 47/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.2382 - accuracy: 0.8905 - auc_roc: 0.9268 - val_loss: 0.2343 - val_accuracy: 0.9005 - val_auc_roc: 0.9271\n",
      "Epoch 48/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.2374 - accuracy: 0.8913 - auc_roc: 0.9275 - val_loss: 0.2371 - val_accuracy: 0.8975 - val_auc_roc: 0.9278\n",
      "Epoch 49/100\n",
      "74880/74880 [==============================] - 4s 57us/step - loss: 0.2367 - accuracy: 0.8891 - auc_roc: 0.9282 - val_loss: 0.2459 - val_accuracy: 0.8932 - val_auc_roc: 0.9285\n",
      "Epoch 50/100\n",
      "74880/74880 [==============================] - 4s 57us/step - loss: 0.2347 - accuracy: 0.8910 - auc_roc: 0.9288 - val_loss: 0.2354 - val_accuracy: 0.8962 - val_auc_roc: 0.9291\n",
      "Epoch 51/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.2335 - accuracy: 0.8915 - auc_roc: 0.9294 - val_loss: 0.2393 - val_accuracy: 0.8998 - val_auc_roc: 0.9297\n",
      "Epoch 52/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.2313 - accuracy: 0.8935 - auc_roc: 0.9301 - val_loss: 0.2416 - val_accuracy: 0.8970 - val_auc_roc: 0.9304\n",
      "Epoch 53/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.2292 - accuracy: 0.8942 - auc_roc: 0.9307 - val_loss: 0.2349 - val_accuracy: 0.8995 - val_auc_roc: 0.9310\n",
      "Epoch 54/100\n",
      "74880/74880 [==============================] - 4s 57us/step - loss: 0.2335 - accuracy: 0.8916 - auc_roc: 0.9313 - val_loss: 0.2388 - val_accuracy: 0.8967 - val_auc_roc: 0.9315\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 55/100\n",
      "74880/74880 [==============================] - 4s 60us/step - loss: 0.2037 - accuracy: 0.9054 - auc_roc: 0.9319 - val_loss: 0.2266 - val_accuracy: 0.9079 - val_auc_roc: 0.9323\n",
      "Epoch 56/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.1967 - accuracy: 0.9069 - auc_roc: 0.9327 - val_loss: 0.2291 - val_accuracy: 0.9071 - val_auc_roc: 0.9331\n",
      "Epoch 57/100\n",
      "74880/74880 [==============================] - 4s 57us/step - loss: 0.1947 - accuracy: 0.9097 - auc_roc: 0.9335 - val_loss: 0.2268 - val_accuracy: 0.9102 - val_auc_roc: 0.9339\n",
      "Epoch 58/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.1926 - accuracy: 0.9089 - auc_roc: 0.9343 - val_loss: 0.2261 - val_accuracy: 0.9071 - val_auc_roc: 0.9347\n",
      "Epoch 59/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.1937 - accuracy: 0.9096 - auc_roc: 0.9350 - val_loss: 0.2277 - val_accuracy: 0.9081 - val_auc_roc: 0.9354\n",
      "Epoch 60/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.1923 - accuracy: 0.9099 - auc_roc: 0.9357 - val_loss: 0.2297 - val_accuracy: 0.9069 - val_auc_roc: 0.9361\n",
      "Epoch 61/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.1928 - accuracy: 0.9097 - auc_roc: 0.9364 - val_loss: 0.2253 - val_accuracy: 0.9120 - val_auc_roc: 0.9368\n",
      "Epoch 62/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.1891 - accuracy: 0.9117 - auc_roc: 0.9371 - val_loss: 0.2287 - val_accuracy: 0.9094 - val_auc_roc: 0.9375\n",
      "Epoch 63/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.1891 - accuracy: 0.9107 - auc_roc: 0.9378 - val_loss: 0.2274 - val_accuracy: 0.9109 - val_auc_roc: 0.9381\n",
      "Epoch 64/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.1884 - accuracy: 0.9103 - auc_roc: 0.9384 - val_loss: 0.2249 - val_accuracy: 0.9112 - val_auc_roc: 0.9388\n",
      "Epoch 65/100\n",
      "74880/74880 [==============================] - 5s 62us/step - loss: 0.1870 - accuracy: 0.9105 - auc_roc: 0.9391 - val_loss: 0.2245 - val_accuracy: 0.9099 - val_auc_roc: 0.9394\n",
      "Epoch 66/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.1868 - accuracy: 0.9103 - auc_roc: 0.9397 - val_loss: 0.2246 - val_accuracy: 0.9102 - val_auc_roc: 0.9400\n",
      "Epoch 67/100\n",
      "74880/74880 [==============================] - 4s 57us/step - loss: 0.1846 - accuracy: 0.9115 - auc_roc: 0.9403 - val_loss: 0.2276 - val_accuracy: 0.9125 - val_auc_roc: 0.9406\n",
      "Epoch 68/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.1873 - accuracy: 0.9112 - auc_roc: 0.9409 - val_loss: 0.2301 - val_accuracy: 0.9112 - val_auc_roc: 0.9411\n",
      "Epoch 69/100\n",
      "74880/74880 [==============================] - 4s 60us/step - loss: 0.1861 - accuracy: 0.9125 - auc_roc: 0.9414 - val_loss: 0.2264 - val_accuracy: 0.9117 - val_auc_roc: 0.9417\n",
      "Epoch 70/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.1834 - accuracy: 0.9140 - auc_roc: 0.9419 - val_loss: 0.2310 - val_accuracy: 0.9099 - val_auc_roc: 0.9422\n",
      "Epoch 71/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.1851 - accuracy: 0.9119 - auc_roc: 0.9425 - val_loss: 0.2297 - val_accuracy: 0.9092 - val_auc_roc: 0.9427\n",
      "Epoch 72/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.1854 - accuracy: 0.9104 - auc_roc: 0.9430 - val_loss: 0.2290 - val_accuracy: 0.9097 - val_auc_roc: 0.9432\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 73/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.1821 - accuracy: 0.9142 - auc_roc: 0.9435 - val_loss: 0.2288 - val_accuracy: 0.9107 - val_auc_roc: 0.9437\n",
      "Epoch 74/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.1834 - accuracy: 0.9134 - auc_roc: 0.9440 - val_loss: 0.2283 - val_accuracy: 0.9102 - val_auc_roc: 0.9442\n",
      "Epoch 75/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.1787 - accuracy: 0.9160 - auc_roc: 0.9444 - val_loss: 0.2279 - val_accuracy: 0.9117 - val_auc_roc: 0.9447\n",
      "Epoch 76/100\n",
      "74880/74880 [==============================] - 4s 57us/step - loss: 0.1799 - accuracy: 0.9141 - auc_roc: 0.9449 - val_loss: 0.2282 - val_accuracy: 0.9107 - val_auc_roc: 0.9451\n",
      "Epoch 77/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.1807 - accuracy: 0.9151 - auc_roc: 0.9454 - val_loss: 0.2289 - val_accuracy: 0.9112 - val_auc_roc: 0.9456\n",
      "Epoch 78/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.1815 - accuracy: 0.9134 - auc_roc: 0.9458 - val_loss: 0.2281 - val_accuracy: 0.9107 - val_auc_roc: 0.9460\n",
      "Epoch 79/100\n",
      "74880/74880 [==============================] - 4s 60us/step - loss: 0.1815 - accuracy: 0.9150 - auc_roc: 0.9462 - val_loss: 0.2278 - val_accuracy: 0.9104 - val_auc_roc: 0.9464\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 00079: early stopping\n",
      "\n",
      "Fold  5\n",
      "Model: \"sequential_64\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_188 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_187 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_125 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_189 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_188 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_126 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_190 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_189 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 74880 samples, validate on 3941 samples\n",
      "Epoch 1/100\n",
      "74880/74880 [==============================] - 6s 86us/step - loss: 0.4884 - accuracy: 0.7696 - auc_roc: 0.7430 - val_loss: 0.4373 - val_accuracy: 0.7983 - val_auc_roc: 0.7935\n",
      "Epoch 2/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.4393 - accuracy: 0.7990 - auc_roc: 0.8074 - val_loss: 0.4124 - val_accuracy: 0.8107 - val_auc_roc: 0.8178\n",
      "Epoch 3/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.4200 - accuracy: 0.8094 - auc_roc: 0.8252 - val_loss: 0.4127 - val_accuracy: 0.8028 - val_auc_roc: 0.8309\n",
      "Epoch 4/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.4033 - accuracy: 0.8174 - auc_roc: 0.8359 - val_loss: 0.3935 - val_accuracy: 0.8211 - val_auc_roc: 0.8404\n",
      "Epoch 5/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.3932 - accuracy: 0.8204 - auc_roc: 0.8440 - val_loss: 0.3608 - val_accuracy: 0.8414 - val_auc_roc: 0.8476\n",
      "Epoch 6/100\n",
      "74880/74880 [==============================] - 4s 57us/step - loss: 0.3836 - accuracy: 0.8267 - auc_roc: 0.8508 - val_loss: 0.3579 - val_accuracy: 0.8439 - val_auc_roc: 0.8536\n",
      "Epoch 7/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.3730 - accuracy: 0.8325 - auc_roc: 0.8565 - val_loss: 0.3460 - val_accuracy: 0.8478 - val_auc_roc: 0.8589\n",
      "Epoch 8/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.3619 - accuracy: 0.8381 - auc_roc: 0.8616 - val_loss: 0.3360 - val_accuracy: 0.8556 - val_auc_roc: 0.8639\n",
      "Epoch 9/100\n",
      "74880/74880 [==============================] - 4s 57us/step - loss: 0.3529 - accuracy: 0.8422 - auc_roc: 0.8663 - val_loss: 0.3331 - val_accuracy: 0.8541 - val_auc_roc: 0.8684\n",
      "Epoch 10/100\n",
      "74880/74880 [==============================] - 4s 57us/step - loss: 0.3467 - accuracy: 0.8442 - auc_roc: 0.8703 - val_loss: 0.3263 - val_accuracy: 0.8609 - val_auc_roc: 0.8723\n",
      "Epoch 11/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.3382 - accuracy: 0.8479 - auc_roc: 0.8742 - val_loss: 0.3061 - val_accuracy: 0.8673 - val_auc_roc: 0.8760\n",
      "Epoch 12/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.3303 - accuracy: 0.8518 - auc_roc: 0.8778 - val_loss: 0.3151 - val_accuracy: 0.8660 - val_auc_roc: 0.8794\n",
      "Epoch 13/100\n",
      "74880/74880 [==============================] - 4s 58us/step - loss: 0.3245 - accuracy: 0.8561 - auc_roc: 0.8810 - val_loss: 0.3059 - val_accuracy: 0.8655 - val_auc_roc: 0.8825\n",
      "Epoch 14/100\n",
      "74880/74880 [==============================] - 4s 57us/step - loss: 0.3217 - accuracy: 0.8560 - auc_roc: 0.8840 - val_loss: 0.2962 - val_accuracy: 0.8731 - val_auc_roc: 0.8853\n",
      "Epoch 15/100\n",
      "74880/74880 [==============================] - 4s 57us/step - loss: 0.3155 - accuracy: 0.8584 - auc_roc: 0.8867 - val_loss: 0.3000 - val_accuracy: 0.8701 - val_auc_roc: 0.8880\n",
      "Epoch 16/100\n",
      "74880/74880 [==============================] - 4s 57us/step - loss: 0.3109 - accuracy: 0.8596 - auc_roc: 0.8893 - val_loss: 0.2918 - val_accuracy: 0.8754 - val_auc_roc: 0.8904\n",
      "Epoch 17/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.3064 - accuracy: 0.8636 - auc_roc: 0.8916 - val_loss: 0.2874 - val_accuracy: 0.8807 - val_auc_roc: 0.8928\n",
      "Epoch 18/100\n",
      "74880/74880 [==============================] - 4s 57us/step - loss: 0.3023 - accuracy: 0.8650 - auc_roc: 0.8939 - val_loss: 0.2880 - val_accuracy: 0.8752 - val_auc_roc: 0.8949\n",
      "Epoch 19/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.2964 - accuracy: 0.8664 - auc_roc: 0.8960 - val_loss: 0.2875 - val_accuracy: 0.8752 - val_auc_roc: 0.8970\n",
      "Epoch 20/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2952 - accuracy: 0.8692 - auc_roc: 0.8980 - val_loss: 0.2820 - val_accuracy: 0.8744 - val_auc_roc: 0.8990\n",
      "Epoch 21/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.2901 - accuracy: 0.8692 - auc_roc: 0.8999 - val_loss: 0.2697 - val_accuracy: 0.8904 - val_auc_roc: 0.9008\n",
      "Epoch 22/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.2861 - accuracy: 0.8718 - auc_roc: 0.9017 - val_loss: 0.2738 - val_accuracy: 0.8810 - val_auc_roc: 0.9026\n",
      "Epoch 23/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.2882 - accuracy: 0.8701 - auc_roc: 0.9034 - val_loss: 0.2707 - val_accuracy: 0.8876 - val_auc_roc: 0.9041\n",
      "Epoch 24/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.2814 - accuracy: 0.8741 - auc_roc: 0.9049 - val_loss: 0.2665 - val_accuracy: 0.8840 - val_auc_roc: 0.9057\n",
      "Epoch 25/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2777 - accuracy: 0.8763 - auc_roc: 0.9065 - val_loss: 0.2604 - val_accuracy: 0.8881 - val_auc_roc: 0.9072\n",
      "Epoch 26/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.2755 - accuracy: 0.8769 - auc_roc: 0.9079 - val_loss: 0.2677 - val_accuracy: 0.8828 - val_auc_roc: 0.9087\n",
      "Epoch 27/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.2748 - accuracy: 0.8771 - auc_roc: 0.9093 - val_loss: 0.2688 - val_accuracy: 0.8896 - val_auc_roc: 0.9100\n",
      "Epoch 28/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.2705 - accuracy: 0.8778 - auc_roc: 0.9107 - val_loss: 0.2576 - val_accuracy: 0.8891 - val_auc_roc: 0.9113\n",
      "Epoch 29/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.2661 - accuracy: 0.8808 - auc_roc: 0.9120 - val_loss: 0.2500 - val_accuracy: 0.8906 - val_auc_roc: 0.9126\n",
      "Epoch 30/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.2647 - accuracy: 0.8821 - auc_roc: 0.9132 - val_loss: 0.2455 - val_accuracy: 0.8952 - val_auc_roc: 0.9138\n",
      "Epoch 31/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2613 - accuracy: 0.8823 - auc_roc: 0.9144 - val_loss: 0.2469 - val_accuracy: 0.8972 - val_auc_roc: 0.9150\n",
      "Epoch 32/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2603 - accuracy: 0.8830 - auc_roc: 0.9156 - val_loss: 0.2426 - val_accuracy: 0.8975 - val_auc_roc: 0.9162\n",
      "Epoch 33/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2583 - accuracy: 0.8827 - auc_roc: 0.9167 - val_loss: 0.2433 - val_accuracy: 0.8962 - val_auc_roc: 0.9173\n",
      "Epoch 34/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2585 - accuracy: 0.8836 - auc_roc: 0.9178 - val_loss: 0.2446 - val_accuracy: 0.8934 - val_auc_roc: 0.9183\n",
      "Epoch 35/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2513 - accuracy: 0.8858 - auc_roc: 0.9188 - val_loss: 0.2433 - val_accuracy: 0.8927 - val_auc_roc: 0.9193\n",
      "Epoch 36/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.2548 - accuracy: 0.8843 - auc_roc: 0.9198 - val_loss: 0.2496 - val_accuracy: 0.8944 - val_auc_roc: 0.9203\n",
      "Epoch 37/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.2497 - accuracy: 0.8873 - auc_roc: 0.9207 - val_loss: 0.2464 - val_accuracy: 0.8911 - val_auc_roc: 0.9212\n",
      "Epoch 38/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2479 - accuracy: 0.8869 - auc_roc: 0.9217 - val_loss: 0.2339 - val_accuracy: 0.9005 - val_auc_roc: 0.9221\n",
      "Epoch 39/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2456 - accuracy: 0.8903 - auc_roc: 0.9226 - val_loss: 0.2454 - val_accuracy: 0.8955 - val_auc_roc: 0.9230\n",
      "Epoch 40/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2498 - accuracy: 0.8865 - auc_roc: 0.9234 - val_loss: 0.2424 - val_accuracy: 0.8950 - val_auc_roc: 0.9238\n",
      "Epoch 41/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.2464 - accuracy: 0.8862 - auc_roc: 0.9242 - val_loss: 0.2400 - val_accuracy: 0.8962 - val_auc_roc: 0.9246\n",
      "Epoch 42/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.2412 - accuracy: 0.8901 - auc_roc: 0.9250 - val_loss: 0.2409 - val_accuracy: 0.8985 - val_auc_roc: 0.9254\n",
      "Epoch 43/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2414 - accuracy: 0.8916 - auc_roc: 0.9258 - val_loss: 0.2354 - val_accuracy: 0.9026 - val_auc_roc: 0.9262\n",
      "Epoch 44/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2383 - accuracy: 0.8907 - auc_roc: 0.9266 - val_loss: 0.2343 - val_accuracy: 0.9026 - val_auc_roc: 0.9270\n",
      "Epoch 45/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2391 - accuracy: 0.8911 - auc_roc: 0.9273 - val_loss: 0.2285 - val_accuracy: 0.9031 - val_auc_roc: 0.9277\n",
      "Epoch 46/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2347 - accuracy: 0.8921 - auc_roc: 0.9280 - val_loss: 0.2406 - val_accuracy: 0.8993 - val_auc_roc: 0.9284\n",
      "Epoch 47/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.2339 - accuracy: 0.8934 - auc_roc: 0.9288 - val_loss: 0.2301 - val_accuracy: 0.8970 - val_auc_roc: 0.9291\n",
      "Epoch 48/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.2317 - accuracy: 0.8945 - auc_roc: 0.9295 - val_loss: 0.2333 - val_accuracy: 0.8990 - val_auc_roc: 0.9298\n",
      "Epoch 49/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2317 - accuracy: 0.8929 - auc_roc: 0.9301 - val_loss: 0.2354 - val_accuracy: 0.8995 - val_auc_roc: 0.9304\n",
      "Epoch 50/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2309 - accuracy: 0.8942 - auc_roc: 0.9308 - val_loss: 0.2363 - val_accuracy: 0.8993 - val_auc_roc: 0.9311\n",
      "Epoch 51/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2294 - accuracy: 0.8935 - auc_roc: 0.9314 - val_loss: 0.2275 - val_accuracy: 0.9033 - val_auc_roc: 0.9317\n",
      "Epoch 52/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2278 - accuracy: 0.8945 - auc_roc: 0.9320 - val_loss: 0.2418 - val_accuracy: 0.8957 - val_auc_roc: 0.9323\n",
      "Epoch 53/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.2284 - accuracy: 0.8950 - auc_roc: 0.9326 - val_loss: 0.2270 - val_accuracy: 0.9023 - val_auc_roc: 0.9329\n",
      "Epoch 54/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2254 - accuracy: 0.8957 - auc_roc: 0.9332 - val_loss: 0.2179 - val_accuracy: 0.9061 - val_auc_roc: 0.9335\n",
      "Epoch 55/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.2224 - accuracy: 0.8957 - auc_roc: 0.9338 - val_loss: 0.2297 - val_accuracy: 0.9008 - val_auc_roc: 0.9341\n",
      "Epoch 56/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2225 - accuracy: 0.8976 - auc_roc: 0.9343 - val_loss: 0.2212 - val_accuracy: 0.9076 - val_auc_roc: 0.9346\n",
      "Epoch 57/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.2203 - accuracy: 0.8992 - auc_roc: 0.9349 - val_loss: 0.2244 - val_accuracy: 0.9076 - val_auc_roc: 0.9352\n",
      "Epoch 58/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.2172 - accuracy: 0.8981 - auc_roc: 0.9355 - val_loss: 0.2253 - val_accuracy: 0.9059 - val_auc_roc: 0.9357\n",
      "Epoch 59/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2187 - accuracy: 0.8984 - auc_roc: 0.9360 - val_loss: 0.2287 - val_accuracy: 0.9036 - val_auc_roc: 0.9362\n",
      "Epoch 60/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2146 - accuracy: 0.8992 - auc_roc: 0.9365 - val_loss: 0.2286 - val_accuracy: 0.9003 - val_auc_roc: 0.9368\n",
      "Epoch 61/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2177 - accuracy: 0.8982 - auc_roc: 0.9370 - val_loss: 0.2206 - val_accuracy: 0.9074 - val_auc_roc: 0.9373\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 62/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.1876 - accuracy: 0.9117 - auc_roc: 0.9376 - val_loss: 0.2103 - val_accuracy: 0.9102 - val_auc_roc: 0.9380\n",
      "Epoch 63/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.1819 - accuracy: 0.9141 - auc_roc: 0.9383 - val_loss: 0.2087 - val_accuracy: 0.9107 - val_auc_roc: 0.9387\n",
      "Epoch 64/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.1810 - accuracy: 0.9145 - auc_roc: 0.9390 - val_loss: 0.2087 - val_accuracy: 0.9109 - val_auc_roc: 0.9393\n",
      "Epoch 65/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.1796 - accuracy: 0.9147 - auc_roc: 0.9397 - val_loss: 0.2084 - val_accuracy: 0.9114 - val_auc_roc: 0.9400\n",
      "Epoch 66/100\n",
      "74880/74880 [==============================] - 4s 48us/step - loss: 0.1779 - accuracy: 0.9152 - auc_roc: 0.9403 - val_loss: 0.2093 - val_accuracy: 0.9104 - val_auc_roc: 0.9407\n",
      "Epoch 67/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.1786 - accuracy: 0.9160 - auc_roc: 0.9410 - val_loss: 0.2089 - val_accuracy: 0.9117 - val_auc_roc: 0.9413\n",
      "Epoch 68/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.1763 - accuracy: 0.9162 - auc_roc: 0.9416 - val_loss: 0.2114 - val_accuracy: 0.9094 - val_auc_roc: 0.9419\n",
      "Epoch 69/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.1754 - accuracy: 0.9155 - auc_roc: 0.9422 - val_loss: 0.2125 - val_accuracy: 0.9122 - val_auc_roc: 0.9425\n",
      "Epoch 70/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.1719 - accuracy: 0.9191 - auc_roc: 0.9428 - val_loss: 0.2090 - val_accuracy: 0.9125 - val_auc_roc: 0.9431\n",
      "Epoch 71/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.1709 - accuracy: 0.9188 - auc_roc: 0.9434 - val_loss: 0.2130 - val_accuracy: 0.9089 - val_auc_roc: 0.9437\n",
      "Epoch 72/100\n",
      "74880/74880 [==============================] - 4s 47us/step - loss: 0.1739 - accuracy: 0.9175 - auc_roc: 0.9440 - val_loss: 0.2103 - val_accuracy: 0.9094 - val_auc_roc: 0.9443\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 73/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.1724 - accuracy: 0.9175 - auc_roc: 0.9445 - val_loss: 0.2106 - val_accuracy: 0.9114 - val_auc_roc: 0.9448\n",
      "Epoch 74/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.1690 - accuracy: 0.9193 - auc_roc: 0.9451 - val_loss: 0.2111 - val_accuracy: 0.9104 - val_auc_roc: 0.9454\n",
      "Epoch 75/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.1690 - accuracy: 0.9187 - auc_roc: 0.9456 - val_loss: 0.2111 - val_accuracy: 0.9117 - val_auc_roc: 0.9459\n",
      "Epoch 76/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.1703 - accuracy: 0.9190 - auc_roc: 0.9461 - val_loss: 0.2113 - val_accuracy: 0.9120 - val_auc_roc: 0.9464\n",
      "Epoch 77/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.1695 - accuracy: 0.9190 - auc_roc: 0.9466 - val_loss: 0.2112 - val_accuracy: 0.9120 - val_auc_roc: 0.9469\n",
      "Epoch 78/100\n",
      "74880/74880 [==============================] - 3s 46us/step - loss: 0.1695 - accuracy: 0.9192 - auc_roc: 0.9471 - val_loss: 0.2112 - val_accuracy: 0.9122 - val_auc_roc: 0.9474\n",
      "Epoch 79/100\n",
      "74880/74880 [==============================] - 3s 47us/step - loss: 0.1690 - accuracy: 0.9189 - auc_roc: 0.9476 - val_loss: 0.2115 - val_accuracy: 0.9107 - val_auc_roc: 0.9478\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 00079: early stopping\n",
      "\n",
      "Fold  6\n",
      "Model: \"sequential_65\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_191 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_190 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_127 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_192 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_191 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_128 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_193 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_192 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 74880 samples, validate on 3941 samples\n",
      "Epoch 1/100\n",
      "74880/74880 [==============================] - 6s 78us/step - loss: 0.4895 - accuracy: 0.7713 - auc_roc: 0.7346 - val_loss: 0.4365 - val_accuracy: 0.7975 - val_auc_roc: 0.7929\n",
      "Epoch 2/100\n",
      "74880/74880 [==============================] - 4s 48us/step - loss: 0.4386 - accuracy: 0.7988 - auc_roc: 0.8070 - val_loss: 0.4104 - val_accuracy: 0.8135 - val_auc_roc: 0.8179\n",
      "Epoch 3/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.4175 - accuracy: 0.8107 - auc_roc: 0.8251 - val_loss: 0.3879 - val_accuracy: 0.8216 - val_auc_roc: 0.8315\n",
      "Epoch 4/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.4021 - accuracy: 0.8180 - auc_roc: 0.8369 - val_loss: 0.3795 - val_accuracy: 0.8308 - val_auc_roc: 0.8413\n",
      "Epoch 5/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.3907 - accuracy: 0.8221 - auc_roc: 0.8452 - val_loss: 0.3663 - val_accuracy: 0.8356 - val_auc_roc: 0.8487\n",
      "Epoch 6/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.3808 - accuracy: 0.8285 - auc_roc: 0.8518 - val_loss: 0.3572 - val_accuracy: 0.8419 - val_auc_roc: 0.8547\n",
      "Epoch 7/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.3718 - accuracy: 0.8338 - auc_roc: 0.8574 - val_loss: 0.3488 - val_accuracy: 0.8455 - val_auc_roc: 0.8598\n",
      "Epoch 8/100\n",
      "74880/74880 [==============================] - 3s 47us/step - loss: 0.3653 - accuracy: 0.8358 - auc_roc: 0.8622 - val_loss: 0.3491 - val_accuracy: 0.8406 - val_auc_roc: 0.8642\n",
      "Epoch 9/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.3567 - accuracy: 0.8380 - auc_roc: 0.8663 - val_loss: 0.3376 - val_accuracy: 0.8503 - val_auc_roc: 0.8682\n",
      "Epoch 10/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.3478 - accuracy: 0.8434 - auc_roc: 0.8703 - val_loss: 0.3231 - val_accuracy: 0.8516 - val_auc_roc: 0.8720\n",
      "Epoch 11/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.3436 - accuracy: 0.8448 - auc_roc: 0.8738 - val_loss: 0.3212 - val_accuracy: 0.8546 - val_auc_roc: 0.8754\n",
      "Epoch 12/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.3353 - accuracy: 0.8496 - auc_roc: 0.8769 - val_loss: 0.3079 - val_accuracy: 0.8612 - val_auc_roc: 0.8785\n",
      "Epoch 13/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.3310 - accuracy: 0.8505 - auc_roc: 0.8801 - val_loss: 0.3087 - val_accuracy: 0.8630 - val_auc_roc: 0.8815\n",
      "Epoch 14/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.3258 - accuracy: 0.8527 - auc_roc: 0.8829 - val_loss: 0.3052 - val_accuracy: 0.8655 - val_auc_roc: 0.8842\n",
      "Epoch 15/100\n",
      "74880/74880 [==============================] - 4s 48us/step - loss: 0.3210 - accuracy: 0.8543 - auc_roc: 0.8855 - val_loss: 0.2944 - val_accuracy: 0.8691 - val_auc_roc: 0.8867\n",
      "Epoch 16/100\n",
      "74880/74880 [==============================] - 4s 47us/step - loss: 0.3174 - accuracy: 0.8582 - auc_roc: 0.8879 - val_loss: 0.3131 - val_accuracy: 0.8577 - val_auc_roc: 0.8890\n",
      "Epoch 17/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.3098 - accuracy: 0.8587 - auc_roc: 0.8902 - val_loss: 0.2907 - val_accuracy: 0.8726 - val_auc_roc: 0.8913\n",
      "Epoch 18/100\n",
      "74880/74880 [==============================] - 4s 48us/step - loss: 0.3085 - accuracy: 0.8604 - auc_roc: 0.8923 - val_loss: 0.2833 - val_accuracy: 0.8785 - val_auc_roc: 0.8934\n",
      "Epoch 19/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.3027 - accuracy: 0.8624 - auc_roc: 0.8944 - val_loss: 0.2861 - val_accuracy: 0.8701 - val_auc_roc: 0.8954\n",
      "Epoch 20/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.3008 - accuracy: 0.8632 - auc_roc: 0.8963 - val_loss: 0.2822 - val_accuracy: 0.8724 - val_auc_roc: 0.8972\n",
      "Epoch 21/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.2947 - accuracy: 0.8670 - auc_roc: 0.8981 - val_loss: 0.2765 - val_accuracy: 0.8779 - val_auc_roc: 0.8990\n",
      "Epoch 22/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.2911 - accuracy: 0.8686 - auc_roc: 0.8999 - val_loss: 0.2751 - val_accuracy: 0.8810 - val_auc_roc: 0.9008\n",
      "Epoch 23/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.2903 - accuracy: 0.8685 - auc_roc: 0.9016 - val_loss: 0.2737 - val_accuracy: 0.8777 - val_auc_roc: 0.9024\n",
      "Epoch 24/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.2868 - accuracy: 0.8687 - auc_roc: 0.9032 - val_loss: 0.2733 - val_accuracy: 0.8802 - val_auc_roc: 0.9039\n",
      "Epoch 25/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.2832 - accuracy: 0.8699 - auc_roc: 0.9047 - val_loss: 0.2702 - val_accuracy: 0.8830 - val_auc_roc: 0.9054\n",
      "Epoch 26/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.2810 - accuracy: 0.8730 - auc_roc: 0.9061 - val_loss: 0.2685 - val_accuracy: 0.8802 - val_auc_roc: 0.9068\n",
      "Epoch 27/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.2819 - accuracy: 0.8724 - auc_roc: 0.9075 - val_loss: 0.2646 - val_accuracy: 0.8833 - val_auc_roc: 0.9081\n",
      "Epoch 28/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.2759 - accuracy: 0.8740 - auc_roc: 0.9088 - val_loss: 0.2624 - val_accuracy: 0.8843 - val_auc_roc: 0.9094\n",
      "Epoch 29/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.2740 - accuracy: 0.8759 - auc_roc: 0.9101 - val_loss: 0.2655 - val_accuracy: 0.8858 - val_auc_roc: 0.9107\n",
      "Epoch 30/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.2732 - accuracy: 0.8771 - auc_roc: 0.9113 - val_loss: 0.2661 - val_accuracy: 0.8845 - val_auc_roc: 0.9118\n",
      "Epoch 31/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.2703 - accuracy: 0.8774 - auc_roc: 0.9124 - val_loss: 0.2584 - val_accuracy: 0.8818 - val_auc_roc: 0.9130\n",
      "Epoch 32/100\n",
      "74880/74880 [==============================] - 4s 48us/step - loss: 0.2690 - accuracy: 0.8766 - auc_roc: 0.9135 - val_loss: 0.2645 - val_accuracy: 0.8795 - val_auc_roc: 0.9140\n",
      "Epoch 33/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.2630 - accuracy: 0.8794 - auc_roc: 0.9146 - val_loss: 0.2498 - val_accuracy: 0.8909 - val_auc_roc: 0.9151\n",
      "Epoch 34/100\n",
      "74880/74880 [==============================] - 4s 48us/step - loss: 0.2617 - accuracy: 0.8818 - auc_roc: 0.9156 - val_loss: 0.2521 - val_accuracy: 0.8901 - val_auc_roc: 0.9162\n",
      "Epoch 35/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.2602 - accuracy: 0.8818 - auc_roc: 0.9167 - val_loss: 0.2511 - val_accuracy: 0.8924 - val_auc_roc: 0.9172\n",
      "Epoch 36/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.2618 - accuracy: 0.8807 - auc_roc: 0.9176 - val_loss: 0.2622 - val_accuracy: 0.8835 - val_auc_roc: 0.9181\n",
      "Epoch 37/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.2589 - accuracy: 0.8821 - auc_roc: 0.9185 - val_loss: 0.2469 - val_accuracy: 0.8950 - val_auc_roc: 0.9190\n",
      "Epoch 38/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.2591 - accuracy: 0.8805 - auc_roc: 0.9194 - val_loss: 0.2621 - val_accuracy: 0.8835 - val_auc_roc: 0.9198\n",
      "Epoch 39/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.2558 - accuracy: 0.8828 - auc_roc: 0.9203 - val_loss: 0.2447 - val_accuracy: 0.8937 - val_auc_roc: 0.9207\n",
      "Epoch 40/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.2513 - accuracy: 0.8846 - auc_roc: 0.9211 - val_loss: 0.2420 - val_accuracy: 0.8932 - val_auc_roc: 0.9216\n",
      "Epoch 41/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2498 - accuracy: 0.8860 - auc_roc: 0.9220 - val_loss: 0.2494 - val_accuracy: 0.8896 - val_auc_roc: 0.9224\n",
      "Epoch 42/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.2514 - accuracy: 0.8852 - auc_roc: 0.9228 - val_loss: 0.2446 - val_accuracy: 0.8922 - val_auc_roc: 0.9232\n",
      "Epoch 43/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.2485 - accuracy: 0.8866 - auc_roc: 0.9236 - val_loss: 0.2529 - val_accuracy: 0.8911 - val_auc_roc: 0.9239\n",
      "Epoch 44/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.2482 - accuracy: 0.8865 - auc_roc: 0.9243 - val_loss: 0.2437 - val_accuracy: 0.8919 - val_auc_roc: 0.9246\n",
      "Epoch 45/100\n",
      "74880/74880 [==============================] - 4s 48us/step - loss: 0.2464 - accuracy: 0.8864 - auc_roc: 0.9250 - val_loss: 0.2548 - val_accuracy: 0.8934 - val_auc_roc: 0.9254\n",
      "Epoch 46/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.2453 - accuracy: 0.8867 - auc_roc: 0.9257 - val_loss: 0.2368 - val_accuracy: 0.9010 - val_auc_roc: 0.9260\n",
      "Epoch 47/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2430 - accuracy: 0.8885 - auc_roc: 0.9264 - val_loss: 0.2381 - val_accuracy: 0.8955 - val_auc_roc: 0.9267\n",
      "Epoch 48/100\n",
      "74880/74880 [==============================] - 4s 48us/step - loss: 0.2408 - accuracy: 0.8903 - auc_roc: 0.9271 - val_loss: 0.2407 - val_accuracy: 0.8985 - val_auc_roc: 0.9274\n",
      "Epoch 49/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2416 - accuracy: 0.8880 - auc_roc: 0.9277 - val_loss: 0.2385 - val_accuracy: 0.8995 - val_auc_roc: 0.9280\n",
      "Epoch 50/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.2433 - accuracy: 0.8891 - auc_roc: 0.9283 - val_loss: 0.2405 - val_accuracy: 0.8970 - val_auc_roc: 0.9286\n",
      "Epoch 51/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2362 - accuracy: 0.8918 - auc_roc: 0.9289 - val_loss: 0.2434 - val_accuracy: 0.8950 - val_auc_roc: 0.9292\n",
      "Epoch 52/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.2358 - accuracy: 0.8913 - auc_roc: 0.9295 - val_loss: 0.2408 - val_accuracy: 0.8957 - val_auc_roc: 0.9298\n",
      "Epoch 53/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.2356 - accuracy: 0.8911 - auc_roc: 0.9301 - val_loss: 0.2357 - val_accuracy: 0.9015 - val_auc_roc: 0.9304\n",
      "Epoch 54/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.2342 - accuracy: 0.8927 - auc_roc: 0.9307 - val_loss: 0.2342 - val_accuracy: 0.8965 - val_auc_roc: 0.9310\n",
      "Epoch 55/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2307 - accuracy: 0.8945 - auc_roc: 0.9313 - val_loss: 0.2488 - val_accuracy: 0.8962 - val_auc_roc: 0.9315\n",
      "Epoch 56/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2334 - accuracy: 0.8924 - auc_roc: 0.9318 - val_loss: 0.2308 - val_accuracy: 0.9041 - val_auc_roc: 0.9321\n",
      "Epoch 57/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2283 - accuracy: 0.8949 - auc_roc: 0.9324 - val_loss: 0.2406 - val_accuracy: 0.8950 - val_auc_roc: 0.9326\n",
      "Epoch 58/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2272 - accuracy: 0.8958 - auc_roc: 0.9329 - val_loss: 0.2326 - val_accuracy: 0.8993 - val_auc_roc: 0.9332\n",
      "Epoch 59/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.2298 - accuracy: 0.8930 - auc_roc: 0.9334 - val_loss: 0.2399 - val_accuracy: 0.8967 - val_auc_roc: 0.9337\n",
      "Epoch 60/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2290 - accuracy: 0.8935 - auc_roc: 0.9339 - val_loss: 0.2317 - val_accuracy: 0.9021 - val_auc_roc: 0.9341\n",
      "Epoch 61/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2276 - accuracy: 0.8958 - auc_roc: 0.9344 - val_loss: 0.2389 - val_accuracy: 0.8947 - val_auc_roc: 0.9346\n",
      "Epoch 62/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.2257 - accuracy: 0.8953 - auc_roc: 0.9349 - val_loss: 0.2322 - val_accuracy: 0.9021 - val_auc_roc: 0.9351\n",
      "Epoch 63/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.2271 - accuracy: 0.8960 - auc_roc: 0.9353 - val_loss: 0.2330 - val_accuracy: 0.9041 - val_auc_roc: 0.9355\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 64/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.1958 - accuracy: 0.9103 - auc_roc: 0.9359 - val_loss: 0.2235 - val_accuracy: 0.9087 - val_auc_roc: 0.9362\n",
      "Epoch 65/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.1924 - accuracy: 0.9129 - auc_roc: 0.9365 - val_loss: 0.2195 - val_accuracy: 0.9099 - val_auc_roc: 0.9368\n",
      "Epoch 66/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.1883 - accuracy: 0.9142 - auc_roc: 0.9372 - val_loss: 0.2211 - val_accuracy: 0.9092 - val_auc_roc: 0.9375\n",
      "Epoch 67/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.1887 - accuracy: 0.9140 - auc_roc: 0.9378 - val_loss: 0.2216 - val_accuracy: 0.9125 - val_auc_roc: 0.9381\n",
      "Epoch 68/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.1867 - accuracy: 0.9144 - auc_roc: 0.9384 - val_loss: 0.2200 - val_accuracy: 0.9109 - val_auc_roc: 0.9388\n",
      "Epoch 69/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.1844 - accuracy: 0.9168 - auc_roc: 0.9391 - val_loss: 0.2229 - val_accuracy: 0.9120 - val_auc_roc: 0.9394\n",
      "Epoch 70/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.1853 - accuracy: 0.9148 - auc_roc: 0.9397 - val_loss: 0.2219 - val_accuracy: 0.9104 - val_auc_roc: 0.9400\n",
      "Epoch 71/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.1852 - accuracy: 0.9164 - auc_roc: 0.9402 - val_loss: 0.2229 - val_accuracy: 0.9107 - val_auc_roc: 0.9405\n",
      "Epoch 72/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.1838 - accuracy: 0.9161 - auc_roc: 0.9408 - val_loss: 0.2233 - val_accuracy: 0.9132 - val_auc_roc: 0.9411\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 73/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.1798 - accuracy: 0.9189 - auc_roc: 0.9414 - val_loss: 0.2212 - val_accuracy: 0.9122 - val_auc_roc: 0.9416\n",
      "Epoch 74/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.1807 - accuracy: 0.9183 - auc_roc: 0.9419 - val_loss: 0.2214 - val_accuracy: 0.9127 - val_auc_roc: 0.9422\n",
      "Epoch 75/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.1776 - accuracy: 0.9194 - auc_roc: 0.9425 - val_loss: 0.2225 - val_accuracy: 0.9130 - val_auc_roc: 0.9427\n",
      "Epoch 76/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.1806 - accuracy: 0.9180 - auc_roc: 0.9430 - val_loss: 0.2221 - val_accuracy: 0.9114 - val_auc_roc: 0.9432\n",
      "Epoch 77/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.1797 - accuracy: 0.9195 - auc_roc: 0.9435 - val_loss: 0.2212 - val_accuracy: 0.9114 - val_auc_roc: 0.9437\n",
      "Epoch 78/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.1786 - accuracy: 0.9194 - auc_roc: 0.9440 - val_loss: 0.2222 - val_accuracy: 0.9127 - val_auc_roc: 0.9442\n",
      "Epoch 79/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.1806 - accuracy: 0.9192 - auc_roc: 0.9444 - val_loss: 0.2223 - val_accuracy: 0.9099 - val_auc_roc: 0.9447\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 00079: early stopping\n",
      "\n",
      "Fold  7\n",
      "Model: \"sequential_66\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_194 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_193 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_129 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_195 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_194 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_130 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_196 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_195 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 74880 samples, validate on 3941 samples\n",
      "Epoch 1/100\n",
      "74880/74880 [==============================] - 6s 83us/step - loss: 0.4877 - accuracy: 0.7723 - auc_roc: 0.7414 - val_loss: 0.4282 - val_accuracy: 0.8044 - val_auc_roc: 0.7951\n",
      "Epoch 2/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.4385 - accuracy: 0.8000 - auc_roc: 0.8092 - val_loss: 0.4044 - val_accuracy: 0.8150 - val_auc_roc: 0.8190\n",
      "Epoch 3/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.4195 - accuracy: 0.8092 - auc_roc: 0.8258 - val_loss: 0.3816 - val_accuracy: 0.8302 - val_auc_roc: 0.8317\n",
      "Epoch 4/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.4041 - accuracy: 0.8179 - auc_roc: 0.8366 - val_loss: 0.3738 - val_accuracy: 0.8310 - val_auc_roc: 0.8408\n",
      "Epoch 5/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.3914 - accuracy: 0.8245 - auc_roc: 0.8448 - val_loss: 0.3622 - val_accuracy: 0.8384 - val_auc_roc: 0.8481\n",
      "Epoch 6/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.3806 - accuracy: 0.8290 - auc_roc: 0.8512 - val_loss: 0.3573 - val_accuracy: 0.8417 - val_auc_roc: 0.8542\n",
      "Epoch 7/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.3707 - accuracy: 0.8338 - auc_roc: 0.8569 - val_loss: 0.3475 - val_accuracy: 0.8455 - val_auc_roc: 0.8594\n",
      "Epoch 8/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.3613 - accuracy: 0.8389 - auc_roc: 0.8619 - val_loss: 0.3366 - val_accuracy: 0.8559 - val_auc_roc: 0.8641\n",
      "Epoch 9/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.3533 - accuracy: 0.8424 - auc_roc: 0.8663 - val_loss: 0.3266 - val_accuracy: 0.8574 - val_auc_roc: 0.8684\n",
      "Epoch 10/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.3485 - accuracy: 0.8427 - auc_roc: 0.8702 - val_loss: 0.3340 - val_accuracy: 0.8538 - val_auc_roc: 0.8721\n",
      "Epoch 11/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.3411 - accuracy: 0.8474 - auc_roc: 0.8738 - val_loss: 0.3168 - val_accuracy: 0.8645 - val_auc_roc: 0.8755\n",
      "Epoch 12/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.3346 - accuracy: 0.8505 - auc_roc: 0.8772 - val_loss: 0.3175 - val_accuracy: 0.8620 - val_auc_roc: 0.8787\n",
      "Epoch 13/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.3277 - accuracy: 0.8535 - auc_roc: 0.8802 - val_loss: 0.3088 - val_accuracy: 0.8678 - val_auc_roc: 0.8817\n",
      "Epoch 14/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.3252 - accuracy: 0.8524 - auc_roc: 0.8831 - val_loss: 0.3267 - val_accuracy: 0.8597 - val_auc_roc: 0.8843\n",
      "Epoch 15/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.3214 - accuracy: 0.8562 - auc_roc: 0.8855 - val_loss: 0.3073 - val_accuracy: 0.8640 - val_auc_roc: 0.8867\n",
      "Epoch 16/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.3196 - accuracy: 0.8557 - auc_roc: 0.8879 - val_loss: 0.3114 - val_accuracy: 0.8673 - val_auc_roc: 0.8889\n",
      "Epoch 17/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.3120 - accuracy: 0.8597 - auc_roc: 0.8900 - val_loss: 0.3013 - val_accuracy: 0.8729 - val_auc_roc: 0.8911\n",
      "Epoch 18/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.3075 - accuracy: 0.8620 - auc_roc: 0.8921 - val_loss: 0.2892 - val_accuracy: 0.8772 - val_auc_roc: 0.8931\n",
      "Epoch 19/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.3059 - accuracy: 0.8618 - auc_roc: 0.8941 - val_loss: 0.2914 - val_accuracy: 0.8744 - val_auc_roc: 0.8950\n",
      "Epoch 20/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2992 - accuracy: 0.8652 - auc_roc: 0.8960 - val_loss: 0.2894 - val_accuracy: 0.8764 - val_auc_roc: 0.8969\n",
      "Epoch 21/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2943 - accuracy: 0.8678 - auc_roc: 0.8978 - val_loss: 0.2853 - val_accuracy: 0.8800 - val_auc_roc: 0.8987\n",
      "Epoch 22/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.2957 - accuracy: 0.8666 - auc_roc: 0.8995 - val_loss: 0.2911 - val_accuracy: 0.8757 - val_auc_roc: 0.9003\n",
      "Epoch 23/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.2919 - accuracy: 0.8675 - auc_roc: 0.9012 - val_loss: 0.2902 - val_accuracy: 0.8744 - val_auc_roc: 0.9019\n",
      "Epoch 24/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.2873 - accuracy: 0.8702 - auc_roc: 0.9027 - val_loss: 0.2760 - val_accuracy: 0.8848 - val_auc_roc: 0.9034\n",
      "Epoch 25/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.2862 - accuracy: 0.8707 - auc_roc: 0.9042 - val_loss: 0.2841 - val_accuracy: 0.8779 - val_auc_roc: 0.9048\n",
      "Epoch 26/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.2793 - accuracy: 0.8737 - auc_roc: 0.9056 - val_loss: 0.2763 - val_accuracy: 0.8914 - val_auc_roc: 0.9063\n",
      "Epoch 27/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.2844 - accuracy: 0.8712 - auc_roc: 0.9069 - val_loss: 0.2759 - val_accuracy: 0.8825 - val_auc_roc: 0.9075\n",
      "Epoch 28/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.2760 - accuracy: 0.8734 - auc_roc: 0.9082 - val_loss: 0.2670 - val_accuracy: 0.8866 - val_auc_roc: 0.9088\n",
      "Epoch 29/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2767 - accuracy: 0.8747 - auc_roc: 0.9094 - val_loss: 0.2699 - val_accuracy: 0.8894 - val_auc_roc: 0.9100\n",
      "Epoch 30/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2734 - accuracy: 0.8764 - auc_roc: 0.9106 - val_loss: 0.2693 - val_accuracy: 0.8917 - val_auc_roc: 0.9112\n",
      "Epoch 31/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2694 - accuracy: 0.8770 - auc_roc: 0.9117 - val_loss: 0.2672 - val_accuracy: 0.8919 - val_auc_roc: 0.9123\n",
      "Epoch 32/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2687 - accuracy: 0.8779 - auc_roc: 0.9129 - val_loss: 0.2661 - val_accuracy: 0.8937 - val_auc_roc: 0.9134\n",
      "Epoch 33/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.2691 - accuracy: 0.8762 - auc_roc: 0.9139 - val_loss: 0.2646 - val_accuracy: 0.8906 - val_auc_roc: 0.9144\n",
      "Epoch 34/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.2643 - accuracy: 0.8798 - auc_roc: 0.9149 - val_loss: 0.2663 - val_accuracy: 0.8932 - val_auc_roc: 0.9154\n",
      "Epoch 35/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.2621 - accuracy: 0.8794 - auc_roc: 0.9159 - val_loss: 0.2608 - val_accuracy: 0.8894 - val_auc_roc: 0.9164\n",
      "Epoch 36/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2626 - accuracy: 0.8800 - auc_roc: 0.9169 - val_loss: 0.2551 - val_accuracy: 0.8962 - val_auc_roc: 0.9173\n",
      "Epoch 37/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2605 - accuracy: 0.8799 - auc_roc: 0.9178 - val_loss: 0.2684 - val_accuracy: 0.8866 - val_auc_roc: 0.9182\n",
      "Epoch 38/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.2584 - accuracy: 0.8803 - auc_roc: 0.9186 - val_loss: 0.2585 - val_accuracy: 0.8944 - val_auc_roc: 0.9191\n",
      "Epoch 39/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2601 - accuracy: 0.8797 - auc_roc: 0.9195 - val_loss: 0.2617 - val_accuracy: 0.8876 - val_auc_roc: 0.9199\n",
      "Epoch 40/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2567 - accuracy: 0.8817 - auc_roc: 0.9203 - val_loss: 0.2605 - val_accuracy: 0.8922 - val_auc_roc: 0.9207\n",
      "Epoch 41/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.2538 - accuracy: 0.8817 - auc_roc: 0.9211 - val_loss: 0.2607 - val_accuracy: 0.8955 - val_auc_roc: 0.9214\n",
      "Epoch 42/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2515 - accuracy: 0.8831 - auc_roc: 0.9218 - val_loss: 0.2626 - val_accuracy: 0.8929 - val_auc_roc: 0.9222\n",
      "Epoch 43/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2493 - accuracy: 0.8860 - auc_roc: 0.9226 - val_loss: 0.2559 - val_accuracy: 0.8952 - val_auc_roc: 0.9230\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 44/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2234 - accuracy: 0.8963 - auc_roc: 0.9234 - val_loss: 0.2461 - val_accuracy: 0.9005 - val_auc_roc: 0.9240\n",
      "Epoch 45/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2183 - accuracy: 0.8998 - auc_roc: 0.9245 - val_loss: 0.2468 - val_accuracy: 0.9021 - val_auc_roc: 0.9250\n",
      "Epoch 46/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2154 - accuracy: 0.9000 - auc_roc: 0.9255 - val_loss: 0.2465 - val_accuracy: 0.8982 - val_auc_roc: 0.9260\n",
      "Epoch 47/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2116 - accuracy: 0.9018 - auc_roc: 0.9265 - val_loss: 0.2477 - val_accuracy: 0.8972 - val_auc_roc: 0.9270\n",
      "Epoch 48/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2106 - accuracy: 0.9021 - auc_roc: 0.9275 - val_loss: 0.2491 - val_accuracy: 0.9023 - val_auc_roc: 0.9279\n",
      "Epoch 49/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2107 - accuracy: 0.9024 - auc_roc: 0.9284 - val_loss: 0.2458 - val_accuracy: 0.9005 - val_auc_roc: 0.9288\n",
      "Epoch 50/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2114 - accuracy: 0.9020 - auc_roc: 0.9293 - val_loss: 0.2489 - val_accuracy: 0.8982 - val_auc_roc: 0.9297\n",
      "Epoch 51/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2085 - accuracy: 0.9020 - auc_roc: 0.9301 - val_loss: 0.2460 - val_accuracy: 0.9036 - val_auc_roc: 0.9305\n",
      "Epoch 52/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.2084 - accuracy: 0.9039 - auc_roc: 0.9309 - val_loss: 0.2476 - val_accuracy: 0.9005 - val_auc_roc: 0.9313\n",
      "Epoch 53/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.2076 - accuracy: 0.9046 - auc_roc: 0.9317 - val_loss: 0.2474 - val_accuracy: 0.9038 - val_auc_roc: 0.9321\n",
      "Epoch 54/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.2076 - accuracy: 0.9034 - auc_roc: 0.9325 - val_loss: 0.2497 - val_accuracy: 0.9036 - val_auc_roc: 0.9328\n",
      "Epoch 55/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2050 - accuracy: 0.9046 - auc_roc: 0.9332 - val_loss: 0.2470 - val_accuracy: 0.9031 - val_auc_roc: 0.9336\n",
      "Epoch 56/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.2058 - accuracy: 0.9030 - auc_roc: 0.9339 - val_loss: 0.2479 - val_accuracy: 0.9013 - val_auc_roc: 0.9343\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 57/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2025 - accuracy: 0.9048 - auc_roc: 0.9346 - val_loss: 0.2451 - val_accuracy: 0.9041 - val_auc_roc: 0.9350\n",
      "Epoch 58/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2017 - accuracy: 0.9073 - auc_roc: 0.9353 - val_loss: 0.2447 - val_accuracy: 0.9033 - val_auc_roc: 0.9356\n",
      "Epoch 59/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.2007 - accuracy: 0.9085 - auc_roc: 0.9360 - val_loss: 0.2458 - val_accuracy: 0.9028 - val_auc_roc: 0.9363\n",
      "Epoch 60/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.1997 - accuracy: 0.9079 - auc_roc: 0.9366 - val_loss: 0.2462 - val_accuracy: 0.9031 - val_auc_roc: 0.9369\n",
      "Epoch 61/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.2000 - accuracy: 0.9078 - auc_roc: 0.9372 - val_loss: 0.2458 - val_accuracy: 0.9031 - val_auc_roc: 0.9375\n",
      "Epoch 62/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2002 - accuracy: 0.9086 - auc_roc: 0.9378 - val_loss: 0.2457 - val_accuracy: 0.9023 - val_auc_roc: 0.9381\n",
      "Epoch 63/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.2009 - accuracy: 0.9089 - auc_roc: 0.9384 - val_loss: 0.2447 - val_accuracy: 0.9041 - val_auc_roc: 0.9387\n",
      "Epoch 64/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.2018 - accuracy: 0.9069 - auc_roc: 0.9389 - val_loss: 0.2458 - val_accuracy: 0.9036 - val_auc_roc: 0.9392\n",
      "Epoch 65/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.2007 - accuracy: 0.9066 - auc_roc: 0.9395 - val_loss: 0.2455 - val_accuracy: 0.9041 - val_auc_roc: 0.9397\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 66/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.2000 - accuracy: 0.9069 - auc_roc: 0.9400 - val_loss: 0.2456 - val_accuracy: 0.9036 - val_auc_roc: 0.9402\n",
      "Epoch 67/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.1995 - accuracy: 0.9069 - auc_roc: 0.9405 - val_loss: 0.2456 - val_accuracy: 0.9033 - val_auc_roc: 0.9407\n",
      "Epoch 68/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.2023 - accuracy: 0.9077 - auc_roc: 0.9410 - val_loss: 0.2457 - val_accuracy: 0.9033 - val_auc_roc: 0.9412\n",
      "Epoch 69/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.1993 - accuracy: 0.9083 - auc_roc: 0.9414 - val_loss: 0.2457 - val_accuracy: 0.9033 - val_auc_roc: 0.9417\n",
      "Epoch 70/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.2004 - accuracy: 0.9069 - auc_roc: 0.9419 - val_loss: 0.2456 - val_accuracy: 0.9033 - val_auc_roc: 0.9421\n",
      "Epoch 71/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.2006 - accuracy: 0.9072 - auc_roc: 0.9423 - val_loss: 0.2456 - val_accuracy: 0.9033 - val_auc_roc: 0.9425\n",
      "Epoch 72/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2005 - accuracy: 0.9068 - auc_roc: 0.9427 - val_loss: 0.2458 - val_accuracy: 0.9033 - val_auc_roc: 0.9429\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 00072: early stopping\n",
      "\n",
      "Fold  8\n",
      "Model: \"sequential_67\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_197 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_196 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_131 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_198 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_197 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_132 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_199 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_198 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 74880 samples, validate on 3941 samples\n",
      "Epoch 1/100\n",
      "74880/74880 [==============================] - 6s 79us/step - loss: 0.4883 - accuracy: 0.7723 - auc_roc: 0.7419 - val_loss: 0.4399 - val_accuracy: 0.7942 - val_auc_roc: 0.7933\n",
      "Epoch 2/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.4406 - accuracy: 0.7982 - auc_roc: 0.8067 - val_loss: 0.4124 - val_accuracy: 0.8115 - val_auc_roc: 0.8172\n",
      "Epoch 3/100\n",
      "74880/74880 [==============================] - 4s 47us/step - loss: 0.4194 - accuracy: 0.8088 - auc_roc: 0.8243 - val_loss: 0.3988 - val_accuracy: 0.8122 - val_auc_roc: 0.8306\n",
      "Epoch 4/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.4019 - accuracy: 0.8184 - auc_roc: 0.8358 - val_loss: 0.3794 - val_accuracy: 0.8277 - val_auc_roc: 0.8407\n",
      "Epoch 5/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.3890 - accuracy: 0.8250 - auc_roc: 0.8447 - val_loss: 0.3618 - val_accuracy: 0.8401 - val_auc_roc: 0.8485\n",
      "Epoch 6/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.3789 - accuracy: 0.8295 - auc_roc: 0.8519 - val_loss: 0.3557 - val_accuracy: 0.8379 - val_auc_roc: 0.8549\n",
      "Epoch 7/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.3660 - accuracy: 0.8359 - auc_roc: 0.8580 - val_loss: 0.3413 - val_accuracy: 0.8475 - val_auc_roc: 0.8608\n",
      "Epoch 8/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.3619 - accuracy: 0.8380 - auc_roc: 0.8632 - val_loss: 0.3373 - val_accuracy: 0.8533 - val_auc_roc: 0.8654\n",
      "Epoch 9/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.3534 - accuracy: 0.8406 - auc_roc: 0.8675 - val_loss: 0.3301 - val_accuracy: 0.8523 - val_auc_roc: 0.8696\n",
      "Epoch 10/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.3488 - accuracy: 0.8429 - auc_roc: 0.8716 - val_loss: 0.3252 - val_accuracy: 0.8577 - val_auc_roc: 0.8733\n",
      "Epoch 11/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.3389 - accuracy: 0.8490 - auc_roc: 0.8750 - val_loss: 0.3141 - val_accuracy: 0.8625 - val_auc_roc: 0.8768\n",
      "Epoch 12/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.3318 - accuracy: 0.8514 - auc_roc: 0.8785 - val_loss: 0.3099 - val_accuracy: 0.8670 - val_auc_roc: 0.8801\n",
      "Epoch 13/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.3266 - accuracy: 0.8548 - auc_roc: 0.8816 - val_loss: 0.2994 - val_accuracy: 0.8724 - val_auc_roc: 0.8831\n",
      "Epoch 14/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.3208 - accuracy: 0.8560 - auc_roc: 0.8845 - val_loss: 0.2987 - val_accuracy: 0.8663 - val_auc_roc: 0.8859\n",
      "Epoch 15/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.3187 - accuracy: 0.8568 - auc_roc: 0.8872 - val_loss: 0.2956 - val_accuracy: 0.8708 - val_auc_roc: 0.8884\n",
      "Epoch 16/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.3141 - accuracy: 0.8578 - auc_roc: 0.8896 - val_loss: 0.2911 - val_accuracy: 0.8741 - val_auc_roc: 0.8907\n",
      "Epoch 17/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.3094 - accuracy: 0.8604 - auc_roc: 0.8918 - val_loss: 0.2874 - val_accuracy: 0.8800 - val_auc_roc: 0.8929\n",
      "Epoch 18/100\n",
      "74880/74880 [==============================] - 4s 49us/step - loss: 0.3046 - accuracy: 0.8632 - auc_roc: 0.8940 - val_loss: 0.2829 - val_accuracy: 0.8769 - val_auc_roc: 0.8951\n",
      "Epoch 19/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2996 - accuracy: 0.8650 - auc_roc: 0.8961 - val_loss: 0.2778 - val_accuracy: 0.8810 - val_auc_roc: 0.8971\n",
      "Epoch 20/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.2952 - accuracy: 0.8668 - auc_roc: 0.8980 - val_loss: 0.2752 - val_accuracy: 0.8818 - val_auc_roc: 0.8990\n",
      "Epoch 21/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2947 - accuracy: 0.8670 - auc_roc: 0.8999 - val_loss: 0.2819 - val_accuracy: 0.8716 - val_auc_roc: 0.9008\n",
      "Epoch 22/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2933 - accuracy: 0.8675 - auc_roc: 0.9016 - val_loss: 0.2745 - val_accuracy: 0.8790 - val_auc_roc: 0.9024\n",
      "Epoch 23/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.2895 - accuracy: 0.8678 - auc_roc: 0.9032 - val_loss: 0.2710 - val_accuracy: 0.8823 - val_auc_roc: 0.9040\n",
      "Epoch 24/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2817 - accuracy: 0.8725 - auc_roc: 0.9048 - val_loss: 0.2703 - val_accuracy: 0.8823 - val_auc_roc: 0.9055\n",
      "Epoch 25/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.2800 - accuracy: 0.8730 - auc_roc: 0.9063 - val_loss: 0.2650 - val_accuracy: 0.8878 - val_auc_roc: 0.9070\n",
      "Epoch 26/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.2766 - accuracy: 0.8749 - auc_roc: 0.9078 - val_loss: 0.2671 - val_accuracy: 0.8876 - val_auc_roc: 0.9085\n",
      "Epoch 27/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.2722 - accuracy: 0.8766 - auc_roc: 0.9092 - val_loss: 0.2645 - val_accuracy: 0.8853 - val_auc_roc: 0.9099\n",
      "Epoch 28/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2731 - accuracy: 0.8760 - auc_roc: 0.9105 - val_loss: 0.2609 - val_accuracy: 0.8873 - val_auc_roc: 0.9112\n",
      "Epoch 29/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2728 - accuracy: 0.8752 - auc_roc: 0.9118 - val_loss: 0.2585 - val_accuracy: 0.8858 - val_auc_roc: 0.9124\n",
      "Epoch 30/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.2686 - accuracy: 0.8776 - auc_roc: 0.9130 - val_loss: 0.2604 - val_accuracy: 0.8884 - val_auc_roc: 0.9135\n",
      "Epoch 31/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.2666 - accuracy: 0.8769 - auc_roc: 0.9141 - val_loss: 0.2517 - val_accuracy: 0.8909 - val_auc_roc: 0.9147\n",
      "Epoch 32/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2642 - accuracy: 0.8783 - auc_roc: 0.9152 - val_loss: 0.2536 - val_accuracy: 0.8955 - val_auc_roc: 0.9158\n",
      "Epoch 33/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2638 - accuracy: 0.8786 - auc_roc: 0.9163 - val_loss: 0.2591 - val_accuracy: 0.8889 - val_auc_roc: 0.9168\n",
      "Epoch 34/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2585 - accuracy: 0.8798 - auc_roc: 0.9173 - val_loss: 0.2473 - val_accuracy: 0.8960 - val_auc_roc: 0.9178\n",
      "Epoch 35/100\n",
      "74880/74880 [==============================] - 4s 50us/step - loss: 0.2586 - accuracy: 0.8810 - auc_roc: 0.9183 - val_loss: 0.2483 - val_accuracy: 0.8906 - val_auc_roc: 0.9188\n",
      "Epoch 36/100\n",
      "74880/74880 [==============================] - 4s 51us/step - loss: 0.2554 - accuracy: 0.8834 - auc_roc: 0.9193 - val_loss: 0.2456 - val_accuracy: 0.8909 - val_auc_roc: 0.9198\n",
      "Epoch 37/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.2519 - accuracy: 0.8830 - auc_roc: 0.9203 - val_loss: 0.2425 - val_accuracy: 0.9048 - val_auc_roc: 0.9207\n",
      "Epoch 38/100\n",
      "74880/74880 [==============================] - 5s 69us/step - loss: 0.2525 - accuracy: 0.8842 - auc_roc: 0.9212 - val_loss: 0.2506 - val_accuracy: 0.8917 - val_auc_roc: 0.9216\n",
      "Epoch 39/100\n",
      "74880/74880 [==============================] - 5s 68us/step - loss: 0.2517 - accuracy: 0.8827 - auc_roc: 0.9220 - val_loss: 0.2446 - val_accuracy: 0.8937 - val_auc_roc: 0.9225\n",
      "Epoch 40/100\n",
      "74880/74880 [==============================] - 5s 69us/step - loss: 0.2487 - accuracy: 0.8862 - auc_roc: 0.9229 - val_loss: 0.2474 - val_accuracy: 0.8970 - val_auc_roc: 0.9233\n",
      "Epoch 41/100\n",
      "74880/74880 [==============================] - 5s 72us/step - loss: 0.2468 - accuracy: 0.8867 - auc_roc: 0.9237 - val_loss: 0.2378 - val_accuracy: 0.9008 - val_auc_roc: 0.9241\n",
      "Epoch 42/100\n",
      "74880/74880 [==============================] - 5s 71us/step - loss: 0.2454 - accuracy: 0.8864 - auc_roc: 0.9245 - val_loss: 0.2408 - val_accuracy: 0.9010 - val_auc_roc: 0.9249\n",
      "Epoch 43/100\n",
      "74880/74880 [==============================] - 5s 70us/step - loss: 0.2429 - accuracy: 0.8883 - auc_roc: 0.9253 - val_loss: 0.2349 - val_accuracy: 0.9003 - val_auc_roc: 0.9257\n",
      "Epoch 44/100\n",
      "74880/74880 [==============================] - 5s 70us/step - loss: 0.2433 - accuracy: 0.8873 - auc_roc: 0.9261 - val_loss: 0.2421 - val_accuracy: 0.8965 - val_auc_roc: 0.9264\n",
      "Epoch 45/100\n",
      "74880/74880 [==============================] - 5s 70us/step - loss: 0.2422 - accuracy: 0.8894 - auc_roc: 0.9268 - val_loss: 0.2363 - val_accuracy: 0.9003 - val_auc_roc: 0.9271\n",
      "Epoch 46/100\n",
      "74880/74880 [==============================] - 5s 69us/step - loss: 0.2411 - accuracy: 0.8891 - auc_roc: 0.9275 - val_loss: 0.2369 - val_accuracy: 0.8985 - val_auc_roc: 0.9278\n",
      "Epoch 47/100\n",
      "74880/74880 [==============================] - 5s 71us/step - loss: 0.2358 - accuracy: 0.8907 - auc_roc: 0.9282 - val_loss: 0.2300 - val_accuracy: 0.8980 - val_auc_roc: 0.9285\n",
      "Epoch 48/100\n",
      "74880/74880 [==============================] - 5s 70us/step - loss: 0.2373 - accuracy: 0.8900 - auc_roc: 0.9289 - val_loss: 0.2368 - val_accuracy: 0.8967 - val_auc_roc: 0.9292\n",
      "Epoch 49/100\n",
      "74880/74880 [==============================] - 5s 69us/step - loss: 0.2342 - accuracy: 0.8926 - auc_roc: 0.9295 - val_loss: 0.2372 - val_accuracy: 0.8993 - val_auc_roc: 0.9299\n",
      "Epoch 50/100\n",
      "74880/74880 [==============================] - 5s 70us/step - loss: 0.2323 - accuracy: 0.8930 - auc_roc: 0.9302 - val_loss: 0.2309 - val_accuracy: 0.9021 - val_auc_roc: 0.9305\n",
      "Epoch 51/100\n",
      "74880/74880 [==============================] - 5s 70us/step - loss: 0.2313 - accuracy: 0.8909 - auc_roc: 0.9308 - val_loss: 0.2428 - val_accuracy: 0.8957 - val_auc_roc: 0.9311\n",
      "Epoch 52/100\n",
      "74880/74880 [==============================] - 5s 70us/step - loss: 0.2321 - accuracy: 0.8910 - auc_roc: 0.9314 - val_loss: 0.2298 - val_accuracy: 0.9046 - val_auc_roc: 0.9317\n",
      "Epoch 53/100\n",
      "74880/74880 [==============================] - 5s 71us/step - loss: 0.2316 - accuracy: 0.8914 - auc_roc: 0.9320 - val_loss: 0.2305 - val_accuracy: 0.9021 - val_auc_roc: 0.9323\n",
      "Epoch 54/100\n",
      "74880/74880 [==============================] - 5s 69us/step - loss: 0.2296 - accuracy: 0.8933 - auc_roc: 0.9326 - val_loss: 0.2337 - val_accuracy: 0.9023 - val_auc_roc: 0.9329\n",
      "Epoch 55/100\n",
      "74880/74880 [==============================] - 5s 70us/step - loss: 0.2282 - accuracy: 0.8925 - auc_roc: 0.9331 - val_loss: 0.2323 - val_accuracy: 0.9064 - val_auc_roc: 0.9334\n",
      "Epoch 56/100\n",
      "74880/74880 [==============================] - 5s 71us/step - loss: 0.2254 - accuracy: 0.8957 - auc_roc: 0.9337 - val_loss: 0.2322 - val_accuracy: 0.9018 - val_auc_roc: 0.9340\n",
      "Epoch 57/100\n",
      "74880/74880 [==============================] - 5s 68us/step - loss: 0.2251 - accuracy: 0.8949 - auc_roc: 0.9342 - val_loss: 0.2335 - val_accuracy: 0.9018 - val_auc_roc: 0.9345\n",
      "Epoch 58/100\n",
      "74880/74880 [==============================] - 5s 67us/step - loss: 0.2244 - accuracy: 0.8969 - auc_roc: 0.9348 - val_loss: 0.2276 - val_accuracy: 0.9038 - val_auc_roc: 0.9350\n",
      "Epoch 59/100\n",
      "74880/74880 [==============================] - 5s 67us/step - loss: 0.2265 - accuracy: 0.8941 - auc_roc: 0.9353 - val_loss: 0.2245 - val_accuracy: 0.9084 - val_auc_roc: 0.9355\n",
      "Epoch 60/100\n",
      "74880/74880 [==============================] - 5s 68us/step - loss: 0.2218 - accuracy: 0.8968 - auc_roc: 0.9358 - val_loss: 0.2326 - val_accuracy: 0.9013 - val_auc_roc: 0.9360\n",
      "Epoch 61/100\n",
      "74880/74880 [==============================] - 5s 68us/step - loss: 0.2221 - accuracy: 0.8942 - auc_roc: 0.9363 - val_loss: 0.2279 - val_accuracy: 0.9051 - val_auc_roc: 0.9365\n",
      "Epoch 62/100\n",
      "74880/74880 [==============================] - 5s 68us/step - loss: 0.2202 - accuracy: 0.8977 - auc_roc: 0.9367 - val_loss: 0.2292 - val_accuracy: 0.9054 - val_auc_roc: 0.9370\n",
      "Epoch 63/100\n",
      "74880/74880 [==============================] - 5s 68us/step - loss: 0.2166 - accuracy: 0.8989 - auc_roc: 0.9372 - val_loss: 0.2291 - val_accuracy: 0.9048 - val_auc_roc: 0.9375\n",
      "Epoch 64/100\n",
      "74880/74880 [==============================] - 5s 70us/step - loss: 0.2175 - accuracy: 0.8981 - auc_roc: 0.9377 - val_loss: 0.2234 - val_accuracy: 0.9089 - val_auc_roc: 0.9379\n",
      "Epoch 65/100\n",
      "74880/74880 [==============================] - 5s 71us/step - loss: 0.2161 - accuracy: 0.8988 - auc_roc: 0.9382 - val_loss: 0.2317 - val_accuracy: 0.9054 - val_auc_roc: 0.9384\n",
      "Epoch 66/100\n",
      "74880/74880 [==============================] - 5s 67us/step - loss: 0.2162 - accuracy: 0.8981 - auc_roc: 0.9386 - val_loss: 0.2319 - val_accuracy: 0.9051 - val_auc_roc: 0.9388\n",
      "Epoch 67/100\n",
      "74880/74880 [==============================] - ETA: 0s - loss: 0.2136 - accuracy: 0.9009 - auc_roc: 0.93 - 5s 70us/step - loss: 0.2137 - accuracy: 0.9009 - auc_roc: 0.9390 - val_loss: 0.2345 - val_accuracy: 0.9018 - val_auc_roc: 0.9393\n",
      "Epoch 68/100\n",
      "74880/74880 [==============================] - 5s 69us/step - loss: 0.2133 - accuracy: 0.9008 - auc_roc: 0.9395 - val_loss: 0.2259 - val_accuracy: 0.9087 - val_auc_roc: 0.9397\n",
      "Epoch 69/100\n",
      "74880/74880 [==============================] - 5s 69us/step - loss: 0.2139 - accuracy: 0.9006 - auc_roc: 0.9399 - val_loss: 0.2255 - val_accuracy: 0.9028 - val_auc_roc: 0.9401\n",
      "Epoch 70/100\n",
      "74880/74880 [==============================] - 5s 71us/step - loss: 0.2137 - accuracy: 0.8980 - auc_roc: 0.9403 - val_loss: 0.2265 - val_accuracy: 0.9084 - val_auc_roc: 0.9405\n",
      "Epoch 71/100\n",
      "74880/74880 [==============================] - 5s 69us/step - loss: 0.2143 - accuracy: 0.8991 - auc_roc: 0.9407 - val_loss: 0.2293 - val_accuracy: 0.9056 - val_auc_roc: 0.9409\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 72/100\n",
      "74880/74880 [==============================] - 5s 69us/step - loss: 0.1884 - accuracy: 0.9119 - auc_roc: 0.9411 - val_loss: 0.2243 - val_accuracy: 0.9107 - val_auc_roc: 0.9414\n",
      "Epoch 73/100\n",
      "74880/74880 [==============================] - 5s 69us/step - loss: 0.1798 - accuracy: 0.9155 - auc_roc: 0.9417 - val_loss: 0.2236 - val_accuracy: 0.9076 - val_auc_roc: 0.9419\n",
      "Epoch 74/100\n",
      "74880/74880 [==============================] - 5s 68us/step - loss: 0.1773 - accuracy: 0.9163 - auc_roc: 0.9422 - val_loss: 0.2239 - val_accuracy: 0.9109 - val_auc_roc: 0.9425\n",
      "Epoch 75/100\n",
      "74880/74880 [==============================] - 5s 68us/step - loss: 0.1769 - accuracy: 0.9174 - auc_roc: 0.9428 - val_loss: 0.2277 - val_accuracy: 0.9097 - val_auc_roc: 0.9430\n",
      "Epoch 76/100\n",
      "74880/74880 [==============================] - 5s 70us/step - loss: 0.1762 - accuracy: 0.9166 - auc_roc: 0.9433 - val_loss: 0.2279 - val_accuracy: 0.9081 - val_auc_roc: 0.9436\n",
      "Epoch 77/100\n",
      "74880/74880 [==============================] - 5s 70us/step - loss: 0.1728 - accuracy: 0.9193 - auc_roc: 0.9438 - val_loss: 0.2284 - val_accuracy: 0.9097 - val_auc_roc: 0.9441\n",
      "Epoch 78/100\n",
      "74880/74880 [==============================] - 5s 69us/step - loss: 0.1707 - accuracy: 0.9205 - auc_roc: 0.9443 - val_loss: 0.2259 - val_accuracy: 0.9112 - val_auc_roc: 0.9446\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 00078: early stopping\n",
      "\n",
      "Fold  9\n",
      "Model: \"sequential_68\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_200 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_199 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_133 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_201 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_200 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_134 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_202 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_201 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 74880 samples, validate on 3941 samples\n",
      "Epoch 1/100\n",
      "74880/74880 [==============================] - 8s 104us/step - loss: 0.4857 - accuracy: 0.7734 - auc_roc: 0.7471 - val_loss: 0.4501 - val_accuracy: 0.7881 - val_auc_roc: 0.7971\n",
      "Epoch 2/100\n",
      "74880/74880 [==============================] - 5s 69us/step - loss: 0.4391 - accuracy: 0.7990 - auc_roc: 0.8102 - val_loss: 0.4041 - val_accuracy: 0.8148 - val_auc_roc: 0.8196\n",
      "Epoch 3/100\n",
      "74880/74880 [==============================] - 5s 71us/step - loss: 0.4193 - accuracy: 0.8096 - auc_roc: 0.8265 - val_loss: 0.3811 - val_accuracy: 0.8292 - val_auc_roc: 0.8326\n",
      "Epoch 4/100\n",
      "74880/74880 [==============================] - 5s 71us/step - loss: 0.4059 - accuracy: 0.8146 - auc_roc: 0.8372 - val_loss: 0.3681 - val_accuracy: 0.8330 - val_auc_roc: 0.8414\n",
      "Epoch 5/100\n",
      "74880/74880 [==============================] - 5s 70us/step - loss: 0.3914 - accuracy: 0.8220 - auc_roc: 0.8452 - val_loss: 0.3539 - val_accuracy: 0.8391 - val_auc_roc: 0.8488\n",
      "Epoch 6/100\n",
      "74880/74880 [==============================] - 5s 70us/step - loss: 0.3793 - accuracy: 0.8286 - auc_roc: 0.8521 - val_loss: 0.3496 - val_accuracy: 0.8434 - val_auc_roc: 0.8551\n",
      "Epoch 7/100\n",
      "74880/74880 [==============================] - 5s 70us/step - loss: 0.3711 - accuracy: 0.8314 - auc_roc: 0.8578 - val_loss: 0.3351 - val_accuracy: 0.8541 - val_auc_roc: 0.8604\n",
      "Epoch 8/100\n",
      "74880/74880 [==============================] - 5s 71us/step - loss: 0.3659 - accuracy: 0.8348 - auc_roc: 0.8626 - val_loss: 0.3298 - val_accuracy: 0.8594 - val_auc_roc: 0.8647\n",
      "Epoch 9/100\n",
      "74880/74880 [==============================] - 5s 71us/step - loss: 0.3546 - accuracy: 0.8412 - auc_roc: 0.8669 - val_loss: 0.3244 - val_accuracy: 0.8571 - val_auc_roc: 0.8689\n",
      "Epoch 10/100\n",
      "74880/74880 [==============================] - 5s 72us/step - loss: 0.3486 - accuracy: 0.8436 - auc_roc: 0.8708 - val_loss: 0.3260 - val_accuracy: 0.8554 - val_auc_roc: 0.8726\n",
      "Epoch 11/100\n",
      "74880/74880 [==============================] - 5s 69us/step - loss: 0.3409 - accuracy: 0.8482 - auc_roc: 0.8743 - val_loss: 0.3152 - val_accuracy: 0.8582 - val_auc_roc: 0.8760\n",
      "Epoch 12/100\n",
      "74880/74880 [==============================] - 5s 70us/step - loss: 0.3364 - accuracy: 0.8491 - auc_roc: 0.8776 - val_loss: 0.3130 - val_accuracy: 0.8630 - val_auc_roc: 0.8791\n",
      "Epoch 13/100\n",
      "74880/74880 [==============================] - 5s 70us/step - loss: 0.3305 - accuracy: 0.8529 - auc_roc: 0.8806 - val_loss: 0.2965 - val_accuracy: 0.8759 - val_auc_roc: 0.8820\n",
      "Epoch 14/100\n",
      "74880/74880 [==============================] - 5s 69us/step - loss: 0.3262 - accuracy: 0.8546 - auc_roc: 0.8834 - val_loss: 0.3001 - val_accuracy: 0.8724 - val_auc_roc: 0.8846\n",
      "Epoch 15/100\n",
      "74880/74880 [==============================] - 5s 71us/step - loss: 0.3230 - accuracy: 0.8564 - auc_roc: 0.8859 - val_loss: 0.2979 - val_accuracy: 0.8693 - val_auc_roc: 0.8870\n",
      "Epoch 16/100\n",
      "74880/74880 [==============================] - 5s 73us/step - loss: 0.3164 - accuracy: 0.8580 - auc_roc: 0.8882 - val_loss: 0.3109 - val_accuracy: 0.8630 - val_auc_roc: 0.8893\n",
      "Epoch 17/100\n",
      "74880/74880 [==============================] - 5s 72us/step - loss: 0.3138 - accuracy: 0.8610 - auc_roc: 0.8903 - val_loss: 0.2827 - val_accuracy: 0.8782 - val_auc_roc: 0.8914\n",
      "Epoch 18/100\n",
      "74880/74880 [==============================] - 5s 72us/step - loss: 0.3091 - accuracy: 0.8618 - auc_roc: 0.8924 - val_loss: 0.2868 - val_accuracy: 0.8779 - val_auc_roc: 0.8934\n",
      "Epoch 19/100\n",
      "74880/74880 [==============================] - 5s 72us/step - loss: 0.3040 - accuracy: 0.8648 - auc_roc: 0.8944 - val_loss: 0.2829 - val_accuracy: 0.8782 - val_auc_roc: 0.8954\n",
      "Epoch 20/100\n",
      "74880/74880 [==============================] - 5s 72us/step - loss: 0.3022 - accuracy: 0.8650 - auc_roc: 0.8962 - val_loss: 0.2847 - val_accuracy: 0.8818 - val_auc_roc: 0.8972\n",
      "Epoch 21/100\n",
      "74880/74880 [==============================] - 5s 73us/step - loss: 0.2982 - accuracy: 0.8662 - auc_roc: 0.8980 - val_loss: 0.2687 - val_accuracy: 0.8848 - val_auc_roc: 0.8989\n",
      "Epoch 22/100\n",
      "74880/74880 [==============================] - 5s 72us/step - loss: 0.2971 - accuracy: 0.8663 - auc_roc: 0.8997 - val_loss: 0.2752 - val_accuracy: 0.8838 - val_auc_roc: 0.9005\n",
      "Epoch 23/100\n",
      "74880/74880 [==============================] - 5s 73us/step - loss: 0.2943 - accuracy: 0.8679 - auc_roc: 0.9013 - val_loss: 0.2643 - val_accuracy: 0.8927 - val_auc_roc: 0.9020\n",
      "Epoch 24/100\n",
      "74880/74880 [==============================] - 6s 75us/step - loss: 0.2894 - accuracy: 0.8713 - auc_roc: 0.9028 - val_loss: 0.2719 - val_accuracy: 0.8828 - val_auc_roc: 0.9035\n",
      "Epoch 25/100\n",
      "74880/74880 [==============================] - 5s 72us/step - loss: 0.2855 - accuracy: 0.8703 - auc_roc: 0.9042 - val_loss: 0.2644 - val_accuracy: 0.8853 - val_auc_roc: 0.9049\n",
      "Epoch 26/100\n",
      "74880/74880 [==============================] - 5s 73us/step - loss: 0.2831 - accuracy: 0.8727 - auc_roc: 0.9056 - val_loss: 0.2620 - val_accuracy: 0.8919 - val_auc_roc: 0.9063\n",
      "Epoch 27/100\n",
      "74880/74880 [==============================] - 6s 74us/step - loss: 0.2815 - accuracy: 0.8748 - auc_roc: 0.9069 - val_loss: 0.2659 - val_accuracy: 0.8873 - val_auc_roc: 0.9076\n",
      "Epoch 28/100\n",
      "74880/74880 [==============================] - 5s 72us/step - loss: 0.2798 - accuracy: 0.8744 - auc_roc: 0.9082 - val_loss: 0.2538 - val_accuracy: 0.8932 - val_auc_roc: 0.9088\n",
      "Epoch 29/100\n",
      "74880/74880 [==============================] - 5s 73us/step - loss: 0.2783 - accuracy: 0.8755 - auc_roc: 0.9094 - val_loss: 0.2504 - val_accuracy: 0.8950 - val_auc_roc: 0.9100\n",
      "Epoch 30/100\n",
      "74880/74880 [==============================] - 5s 72us/step - loss: 0.2755 - accuracy: 0.8761 - auc_roc: 0.9106 - val_loss: 0.2538 - val_accuracy: 0.8934 - val_auc_roc: 0.9111\n",
      "Epoch 31/100\n",
      "74880/74880 [==============================] - 5s 72us/step - loss: 0.2722 - accuracy: 0.8774 - auc_roc: 0.9117 - val_loss: 0.2567 - val_accuracy: 0.8932 - val_auc_roc: 0.9122\n",
      "Epoch 32/100\n",
      "74880/74880 [==============================] - 6s 74us/step - loss: 0.2714 - accuracy: 0.8773 - auc_roc: 0.9128 - val_loss: 0.2510 - val_accuracy: 0.8988 - val_auc_roc: 0.9133\n",
      "Epoch 33/100\n",
      "74880/74880 [==============================] - 6s 75us/step - loss: 0.2708 - accuracy: 0.8764 - auc_roc: 0.9138 - val_loss: 0.2502 - val_accuracy: 0.8960 - val_auc_roc: 0.9143\n",
      "Epoch 34/100\n",
      "74880/74880 [==============================] - 5s 72us/step - loss: 0.2634 - accuracy: 0.8809 - auc_roc: 0.9148 - val_loss: 0.2508 - val_accuracy: 0.8950 - val_auc_roc: 0.9153\n",
      "Epoch 35/100\n",
      "74880/74880 [==============================] - 5s 72us/step - loss: 0.2630 - accuracy: 0.8819 - auc_roc: 0.9158 - val_loss: 0.2451 - val_accuracy: 0.8965 - val_auc_roc: 0.9163\n",
      "Epoch 36/100\n",
      "74880/74880 [==============================] - 5s 72us/step - loss: 0.2616 - accuracy: 0.8810 - auc_roc: 0.9167 - val_loss: 0.2463 - val_accuracy: 0.8962 - val_auc_roc: 0.9172\n",
      "Epoch 37/100\n",
      "74880/74880 [==============================] - 5s 73us/step - loss: 0.2616 - accuracy: 0.8817 - auc_roc: 0.9176 - val_loss: 0.2428 - val_accuracy: 0.8977 - val_auc_roc: 0.9181\n",
      "Epoch 38/100\n",
      "74880/74880 [==============================] - 6s 74us/step - loss: 0.2599 - accuracy: 0.8832 - auc_roc: 0.9185 - val_loss: 0.2459 - val_accuracy: 0.8939 - val_auc_roc: 0.9189\n",
      "Epoch 39/100\n",
      "74880/74880 [==============================] - 5s 73us/step - loss: 0.2548 - accuracy: 0.8848 - auc_roc: 0.9194 - val_loss: 0.2518 - val_accuracy: 0.8929 - val_auc_roc: 0.9198\n",
      "Epoch 40/100\n",
      "74880/74880 [==============================] - 5s 73us/step - loss: 0.2536 - accuracy: 0.8849 - auc_roc: 0.9202 - val_loss: 0.2403 - val_accuracy: 0.9028 - val_auc_roc: 0.9206\n",
      "Epoch 41/100\n",
      "74880/74880 [==============================] - 5s 72us/step - loss: 0.2538 - accuracy: 0.8848 - auc_roc: 0.9210 - val_loss: 0.2411 - val_accuracy: 0.8950 - val_auc_roc: 0.9214\n",
      "Epoch 42/100\n",
      "74880/74880 [==============================] - 5s 73us/step - loss: 0.2523 - accuracy: 0.8848 - auc_roc: 0.9218 - val_loss: 0.2390 - val_accuracy: 0.9000 - val_auc_roc: 0.9222\n",
      "Epoch 43/100\n",
      "74880/74880 [==============================] - 5s 72us/step - loss: 0.2508 - accuracy: 0.8836 - auc_roc: 0.9226 - val_loss: 0.2374 - val_accuracy: 0.8977 - val_auc_roc: 0.9229\n",
      "Epoch 44/100\n",
      "74880/74880 [==============================] - 5s 72us/step - loss: 0.2475 - accuracy: 0.8856 - auc_roc: 0.9233 - val_loss: 0.2321 - val_accuracy: 0.9036 - val_auc_roc: 0.9237\n",
      "Epoch 45/100\n",
      "74880/74880 [==============================] - 5s 72us/step - loss: 0.2459 - accuracy: 0.8873 - auc_roc: 0.9241 - val_loss: 0.2319 - val_accuracy: 0.9021 - val_auc_roc: 0.9244\n",
      "Epoch 46/100\n",
      "74880/74880 [==============================] - 5s 72us/step - loss: 0.2467 - accuracy: 0.8865 - auc_roc: 0.9248 - val_loss: 0.2321 - val_accuracy: 0.8993 - val_auc_roc: 0.9251\n",
      "Epoch 47/100\n",
      "74880/74880 [==============================] - 5s 72us/step - loss: 0.2454 - accuracy: 0.8869 - auc_roc: 0.9255 - val_loss: 0.2322 - val_accuracy: 0.8995 - val_auc_roc: 0.9258\n",
      "Epoch 48/100\n",
      "74880/74880 [==============================] - 5s 71us/step - loss: 0.2436 - accuracy: 0.8881 - auc_roc: 0.9261 - val_loss: 0.2336 - val_accuracy: 0.9038 - val_auc_roc: 0.9264\n",
      "Epoch 49/100\n",
      "74880/74880 [==============================] - 5s 71us/step - loss: 0.2424 - accuracy: 0.8884 - auc_roc: 0.9267 - val_loss: 0.2267 - val_accuracy: 0.9041 - val_auc_roc: 0.9270\n",
      "Epoch 50/100\n",
      "74880/74880 [==============================] - 5s 73us/step - loss: 0.2441 - accuracy: 0.8880 - auc_roc: 0.9273 - val_loss: 0.2417 - val_accuracy: 0.8972 - val_auc_roc: 0.9276\n",
      "Epoch 51/100\n",
      "74880/74880 [==============================] - 5s 71us/step - loss: 0.2387 - accuracy: 0.8903 - auc_roc: 0.9279 - val_loss: 0.2346 - val_accuracy: 0.9064 - val_auc_roc: 0.9282\n",
      "Epoch 52/100\n",
      "74880/74880 [==============================] - 5s 72us/step - loss: 0.2355 - accuracy: 0.8909 - auc_roc: 0.9286 - val_loss: 0.2343 - val_accuracy: 0.9054 - val_auc_roc: 0.9289\n",
      "Epoch 53/100\n",
      "74880/74880 [==============================] - 5s 73us/step - loss: 0.2365 - accuracy: 0.8911 - auc_roc: 0.9292 - val_loss: 0.2304 - val_accuracy: 0.9054 - val_auc_roc: 0.9294\n",
      "Epoch 54/100\n",
      "74880/74880 [==============================] - 5s 73us/step - loss: 0.2325 - accuracy: 0.8924 - auc_roc: 0.9297 - val_loss: 0.2348 - val_accuracy: 0.9059 - val_auc_roc: 0.9300\n",
      "Epoch 55/100\n",
      "74880/74880 [==============================] - 6s 74us/step - loss: 0.2337 - accuracy: 0.8926 - auc_roc: 0.9303 - val_loss: 0.2285 - val_accuracy: 0.9043 - val_auc_roc: 0.9306\n",
      "Epoch 56/100\n",
      "74880/74880 [==============================] - 5s 73us/step - loss: 0.2345 - accuracy: 0.8900 - auc_roc: 0.9309 - val_loss: 0.2399 - val_accuracy: 0.9000 - val_auc_roc: 0.9311\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 57/100\n",
      "74880/74880 [==============================] - 6s 74us/step - loss: 0.2071 - accuracy: 0.9042 - auc_roc: 0.9315 - val_loss: 0.2191 - val_accuracy: 0.9137 - val_auc_roc: 0.9319\n",
      "Epoch 58/100\n",
      "74880/74880 [==============================] - 5s 72us/step - loss: 0.1978 - accuracy: 0.9067 - auc_roc: 0.9323 - val_loss: 0.2189 - val_accuracy: 0.9114 - val_auc_roc: 0.9326\n",
      "Epoch 59/100\n",
      "74880/74880 [==============================] - 5s 73us/step - loss: 0.1981 - accuracy: 0.9062 - auc_roc: 0.9330 - val_loss: 0.2190 - val_accuracy: 0.9112 - val_auc_roc: 0.9334\n",
      "Epoch 60/100\n",
      "74880/74880 [==============================] - 6s 75us/step - loss: 0.1943 - accuracy: 0.9074 - auc_roc: 0.9338 - val_loss: 0.2202 - val_accuracy: 0.9089 - val_auc_roc: 0.9341\n",
      "Epoch 61/100\n",
      "74880/74880 [==============================] - 5s 72us/step - loss: 0.1954 - accuracy: 0.9076 - auc_roc: 0.9345 - val_loss: 0.2188 - val_accuracy: 0.9132 - val_auc_roc: 0.9348\n",
      "Epoch 62/100\n",
      "74880/74880 [==============================] - 6s 74us/step - loss: 0.1950 - accuracy: 0.9062 - auc_roc: 0.9352 - val_loss: 0.2215 - val_accuracy: 0.9109 - val_auc_roc: 0.9355\n",
      "Epoch 63/100\n",
      "74880/74880 [==============================] - 5s 72us/step - loss: 0.1929 - accuracy: 0.9074 - auc_roc: 0.9359 - val_loss: 0.2204 - val_accuracy: 0.9109 - val_auc_roc: 0.9362\n",
      "Epoch 64/100\n",
      "74880/74880 [==============================] - 5s 67us/step - loss: 0.1926 - accuracy: 0.9072 - auc_roc: 0.9365 - val_loss: 0.2193 - val_accuracy: 0.9130 - val_auc_roc: 0.9368\n",
      "Epoch 65/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.1915 - accuracy: 0.9082 - auc_roc: 0.9372 - val_loss: 0.2226 - val_accuracy: 0.9142 - val_auc_roc: 0.9375\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 66/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.1880 - accuracy: 0.9103 - auc_roc: 0.9378 - val_loss: 0.2215 - val_accuracy: 0.9127 - val_auc_roc: 0.9381\n",
      "Epoch 67/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.1881 - accuracy: 0.9107 - auc_roc: 0.9384 - val_loss: 0.2214 - val_accuracy: 0.9117 - val_auc_roc: 0.9387\n",
      "Epoch 68/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.1887 - accuracy: 0.9081 - auc_roc: 0.9390 - val_loss: 0.2213 - val_accuracy: 0.9120 - val_auc_roc: 0.9393\n",
      "Epoch 69/100\n",
      "74880/74880 [==============================] - 4s 53us/step - loss: 0.1887 - accuracy: 0.9093 - auc_roc: 0.9396 - val_loss: 0.2210 - val_accuracy: 0.9125 - val_auc_roc: 0.9399\n",
      "Epoch 70/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.1871 - accuracy: 0.9093 - auc_roc: 0.9401 - val_loss: 0.2212 - val_accuracy: 0.9125 - val_auc_roc: 0.9404\n",
      "Epoch 71/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.1871 - accuracy: 0.9106 - auc_roc: 0.9407 - val_loss: 0.2209 - val_accuracy: 0.9125 - val_auc_roc: 0.9410\n",
      "Epoch 72/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.1858 - accuracy: 0.9109 - auc_roc: 0.9412 - val_loss: 0.2211 - val_accuracy: 0.9137 - val_auc_roc: 0.9415\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 73/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.1871 - accuracy: 0.9100 - auc_roc: 0.9417 - val_loss: 0.2211 - val_accuracy: 0.9135 - val_auc_roc: 0.9420\n",
      "Epoch 74/100\n",
      "74880/74880 [==============================] - 4s 55us/step - loss: 0.1870 - accuracy: 0.9102 - auc_roc: 0.9422 - val_loss: 0.2211 - val_accuracy: 0.9132 - val_auc_roc: 0.9425\n",
      "Epoch 75/100\n",
      "74880/74880 [==============================] - 4s 52us/step - loss: 0.1874 - accuracy: 0.9085 - auc_roc: 0.9427 - val_loss: 0.2210 - val_accuracy: 0.9130 - val_auc_roc: 0.9430\n",
      "Epoch 00075: early stopping\n",
      "\n",
      "Fold  10\n",
      "Model: \"sequential_69\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_203 (Dense)            (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "activation_202 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_135 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_204 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_203 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_136 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_205 (Dense)            (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_204 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 106,753\n",
      "Trainable params: 106,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 74880 samples, validate on 3941 samples\n",
      "Epoch 1/100\n",
      "74880/74880 [==============================] - 6s 85us/step - loss: 0.4942 - accuracy: 0.7688 - auc_roc: 0.7337 - val_loss: 0.4330 - val_accuracy: 0.7993 - val_auc_roc: 0.7880\n",
      "Epoch 2/100\n",
      "74880/74880 [==============================] - 4s 56us/step - loss: 0.4447 - accuracy: 0.7965 - auc_roc: 0.8033 - val_loss: 0.4094 - val_accuracy: 0.8122 - val_auc_roc: 0.8133\n",
      "Epoch 3/100\n",
      "74880/74880 [==============================] - 4s 59us/step - loss: 0.4244 - accuracy: 0.8062 - auc_roc: 0.8208 - val_loss: 0.3872 - val_accuracy: 0.8275 - val_auc_roc: 0.8268\n",
      "Epoch 4/100\n",
      "74880/74880 [==============================] - 4s 54us/step - loss: 0.4066 - accuracy: 0.8153 - auc_roc: 0.8319 - val_loss: 0.3704 - val_accuracy: 0.8396 - val_auc_roc: 0.8369\n",
      "Epoch 5/100\n",
      "63232/74880 [========================>.....] - ETA: 0s - loss: 0.3944 - accuracy: 0.8215 - auc_roc: 0.8405"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-fa2106e9958c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m               callbacks=my_callbacks)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3275\u001b[0m         \u001b[0mtensor_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3276\u001b[0m         array_vals.append(np.asarray(value,\n\u001b[0;32m-> 3277\u001b[0;31m                                      dtype=tensor_type.as_numpy_dtype))\n\u001b[0m\u001b[1;32m   3278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3279\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "folds = list(StratifiedKFold(n_splits=20, shuffle=True, random_state=1).split(Xtrain_react, Ytrain_react))\n",
    "\n",
    "for j, (train_idx, val_idx) in enumerate(folds):\n",
    "    print('\\nFold ',j+1)\n",
    "    X_train_cv = np.array(Xtrain_react)[train_idx]\n",
    "    y_train_cv = np.array(Ytrain_react)[train_idx]\n",
    "    X_valid_cv = np.array(Xtrain_react)[val_idx]\n",
    "    y_valid_cv= np.array(Ytrain_react)[val_idx]\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train_cv), y_train_cv)\n",
    "    my_callbacks = get_callbacks('MLP_react')\n",
    "    model = get_model()\n",
    "    model.fit(X_train_cv, y_train_cv,\n",
    "              validation_data=(X_valid_cv, y_valid_cv),\n",
    "              shuffle=True,\n",
    "              batch_size=128,\n",
    "              epochs=100,\n",
    "              class_weight=class_weights,\n",
    "              callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLP original]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor       0.77      0.96      0.86       755\n",
      "       rumor       0.87      0.48      0.61       406\n",
      "\n",
      "    accuracy                           0.79      1161\n",
      "   macro avg       0.82      0.72      0.74      1161\n",
      "weighted avg       0.81      0.79      0.77      1161\n",
      "\n",
      "\n",
      "\n",
      "[MLP with reactions]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor       0.83      0.91      0.87       755\n",
      "       rumor       0.80      0.66      0.72       406\n",
      "\n",
      "    accuracy                           0.82      1161\n",
      "   macro avg       0.82      0.79      0.80      1161\n",
      "weighted avg       0.82      0.82      0.82      1161\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "def with_reaction_mlp_model_predict(model, X, thresh=0.5): #return predict, prob\n",
    "    this_y_prob = []\n",
    "    this_y = []\n",
    "    for x in X:\n",
    "        _y = model.predict_classes(x)\n",
    "        if sum(_y) > len(x)*thresh:\n",
    "            this_y.append(1)\n",
    "        else:\n",
    "            this_y.append(0)\n",
    "        _y = model.predict_proba(x)\n",
    "        _y_prob = sum(_y)/len(x)\n",
    "        this_y_prob.append(_y_prob)\n",
    "    return this_y, this_y_prob\n",
    "\n",
    "lr1 = joblib.load('./200g/trained_models/LogisticRegressionCV_org.pkl')\n",
    "lr2 = joblib.load('./200g/trained_models/LogisticRegressionCV_react.pkl')\n",
    "best_mlp = os.listdir('./200g/trained_models/MLP_org/')[-1]\n",
    "mlp1 = load_model('./200g/trained_models/MLP_org/%s'%best_mlp, custom_objects={'auc_roc': auc_roc})\n",
    "best_mlp = os.listdir('./200g/trained_models/MLP_react/')[-1]\n",
    "mlp2 = load_model('./200g/trained_models/MLP_react/%s'%best_mlp, custom_objects={'auc_roc': auc_roc})\n",
    "\n",
    "lr1_y_prob = lr1.predict_proba(Xtest_org)[:,1]\n",
    "lr1_y = lr1.predict(Xtest_org)\n",
    "lr2_y, lr2_y_prob = with_reaction_lr_model_predict(lr2, Xtest_react)\n",
    "mlp1_y, mlp1_y_prob = with_reaction_mlp_model_predict(mlp1, Xtest_react)\n",
    "mlp2_y, mlp2_y_prob = with_reaction_mlp_model_predict(mlp2, Xtest_react)\n",
    "\n",
    "draw_ROC([\"Original\",\"with Reactions\",\"MLP Original\",\"MLP with Reactions\"], \n",
    "         Ytest_org, [lr1_y_prob,lr2_y_prob,mlp1_y_prob,mlp2_y_prob], \n",
    "         'ROC curves - Original v.s with Reactions')\n",
    "\n",
    "print(\"[MLP original]\")\n",
    "print(classification_report(Ytest_react, mlp1_y, target_names=['non-rumor','rumor']))\n",
    "print(\"\\n\\n[MLP with reactions]\")\n",
    "print(classification_report(Ytest_react, mlp2_y, target_names=['non-rumor','rumor']))\n",
    "\n",
    "del lr1\n",
    "del lr2\n",
    "del mlp1\n",
    "del mlp2\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **由上我們可得**：\n",
    "- **Logistic Regression中，「沒有加上Reactions」的ROC-AUC較高，而F1-score是所有模型中最高的。**\n",
    "    - 可能是因為 Logistic Regression 擅長短語句任務\n",
    "- **Multi-Layer Perceptron中，「加上Reactions」的ROC-AUC較高，是所有模型中最高的。**\n",
    "    - 如果更認真尋找超參數，MLP應該是最有潛力的模型\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
